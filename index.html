<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>AZ-204</title>
  <link rel="stylesheet" href="https://stackedit.io/style.css" />
</head>

<body class="stackedit">
  <div class="stackedit__left">
    <div class="stackedit__toc">
      
<ul>
<li><a href="#create-azure-app-service-web-apps">Create Azure App Service web apps</a>
<ul>
<li><a href="#explore-azure-app-service">Explore Azure App Service</a></li>
<li><a href="#scale-apps-in-azure-app-service">Scale apps in Azure App Service</a></li>
<li><a href="#explore-azure-app-service-deployment-slots">Explore Azure App Service deployment slots</a></li>
</ul>
</li>
<li><a href="#az-204-implement-azure-functions">AZ-204: Implement Azure Functions</a>
<ul>
<li><a href="#explore-azure-functions">Explore Azure Functions</a>
<ul>
<li></li>
<li><a href="#compare-azure-functions-hosting-options">Compare Azure Functions hosting options</a></li>
<li><a href="#scale-azure-functions">Scale Azure Functions</a></li>
<li><a href="#develop-azure-functions">Develop Azure Functions</a></li>
<li><a href="#create-triggers-and-bindings">Create triggers and bindings</a></li>
<li><a href="#javascript-example">JavaScript example</a></li>
<li><a href="#class-library-example">Class library example</a></li>
<li><a href="#connect-functions-to-azure-services">Connect functions to Azure services</a></li>
</ul>
</li>
<li><a href="#implement-durable-functions">Implement Durable Functions</a></li>
</ul>
</li>
<li><a href="#az-204-develop-solutions-that-use-blob-storage">AZ-204: Develop solutions that use Blob storage</a>
<ul>
<li><a href="#explore-azure-blob-storage">Explore Azure Blob storage</a></li>
<li><a href="#manage-the-azure-blob-storage-lifecycle">Manage the Azure Blob storage lifecycle</a>
<ul>
<li></li>
<li><a href="#discover-blob-storage-lifecycle-policies">Discover Blob storage lifecycle policies</a></li>
<li><a href="#implement-blob-storage-lifecycle-policies">Implement Blob storage lifecycle policies</a></li>
<li><a href="#rehydrate-blob-data-from-the-archive-tier">Rehydrate blob data from the archive tier</a></li>
</ul>
</li>
<li><a href="#work-with-azure-blob-storage">Work with Azure Blob storage</a></li>
</ul>
</li>
<li><a href="#az-204-develop-solutions-that-use-azure-cosmos-db">AZ-204: Develop solutions that use Azure Cosmos DB</a>
<ul>
<li><a href="#explore-azure-cosmos-db">Explore Azure Cosmos DB</a></li>
<li><a href="#implement-partitioning-in-azure-cosmos-db">Implement partitioning in Azure Cosmos DB</a></li>
<li><a href="#work-with-azure-cosmos-db">Work with Azure Cosmos DB</a></li>
<li><a href="#database-examples">Database examples</a></li>
<li><a href="#container-examples">Container examples</a></li>
<li><a href="#item-examples">Item examples</a></li>
</ul>
</li>
<li><a href="#az-204-implement-infrastructure-as-a-service-solutions">AZ-204: Implement infrastructure as a service solutions</a>
<ul>
<li><a href="#provision-virtual-machines-in-azure">Provision virtual machines in Azure</a></li>
</ul>
</li>
</ul>

    </div>
  </div>
  <div class="stackedit__right">
    <div class="stackedit__html">
      <h1 id="create-azure-app-service-web-apps">Create Azure App Service web apps</h1>
<h2 id="explore-azure-app-service">Explore Azure App Service</h2>
<h3 id="examine-azure-app-service">Examine Azure App Service</h3>
<p>Azure App Service is an HTTP-based service for hosting web applications, REST APIs, and mobile back ends.<br>
Supported programming languages:</p>
<ul>
<li>.NET,</li>
<li>.NET Core,</li>
<li>Java,</li>
<li>Ruby,</li>
<li>Node.js,</li>
<li>PHP,</li>
<li>Python.<br>
Applications run and scale with ease on both Windows and Linux-based environments.</li>
</ul>
<h4 id="features">Features:</h4>
<ul>
<li>Built-in auto scale support: Baked into Azure App Service is the ability to scale up/down or scale out/in.</li>
<li>Continuous integration/deployment support: The Azure portal provides out-of-the-box continuous integration and deployment with Azure DevOps, GitHub, Bitbucket, FTP, or a local Git repository on your development machine.</li>
<li>Deployment slots: When you deploy your web app, web app on Linux, mobile back end, or API app to Azure App Service, you can use a separate deployment slot instead of the default production slot when you’re running in the Standard, Premium, or Isolated App Service plan tier.</li>
<li>App Service on Linux: App Service can also host web apps natively on Linux for supported application stacks.</li>
</ul>
<p>App Service on Linux does have some limitations:</p>
<ul>
<li>App Service on Linux is not supported on Shared pricing tier.</li>
<li>You can’t mix Windows and Linux apps in the same App Service plan.</li>
</ul>
<h3 id="examine-azure-app-service-plans">Examine Azure App Service plans</h3>
<p>In App Service, an app (Web Apps, API Apps, or Mobile Apps) always runs in an <em>App Service plan</em>. An App Service plan defines a set of compute resources for a web app to run. One or more apps can be configured to run on the same computing resources (or in the same App Service plan).</p>
<p>Each App Service plan defines:</p>
<ul>
<li>Region (West US, East US, etc.)</li>
<li>Number of VM instances</li>
<li>Size of VM instances (Small, Medium, Large)</li>
<li>Pricing tier (Free, Shared, Basic, Standard, Premium, PremiumV2, PremiumV3, Isolated)</li>
</ul>
<p>The <em>pricing tier</em> of an App Service plan determines what App Service features you get and how much you pay for the plan. There are a few categories of pricing tiers:</p>
<ul>
<li><strong>Shared compute</strong>: Both <strong>Free</strong> and <strong>Shared</strong> share the resource pools of your apps with the apps of other customers. These tiers allocate CPU quotas to each app that runs on the shared resources, and the resources can’t scale out.</li>
<li><strong>Dedicated compute</strong>: The <strong>Basic</strong>, <strong>Standard</strong>, <strong>Premium</strong>, <strong>PremiumV2</strong>, and <strong>PremiumV3</strong> tiers run apps on dedicated Azure VMs. Only apps in the same App Service plan share the same compute resources. The higher the tier, the more VM instances are available to you for scale-out.</li>
<li><strong>Isolated</strong>: This tier runs dedicated Azure VMs on dedicated Azure Virtual Networks. It provides network isolation on top of compute isolation to your apps. It provides the maximum scale-out capabilities.</li>
<li><strong>Consumption:</strong> This tier is only available to <em>function apps</em>. It scales the functions dynamically depending on workload.</li>
</ul>
<h4 id="how-does-my-app-run-and-scale">How does my app run and scale?</h4>
<p>The App Service plan is the <strong>scale unit</strong> of the App Service apps:</p>
<ul>
<li>An app runs on all the VM instances configured in the App Service plan.</li>
<li>If multiple apps are in the same App Service plan, they all share the same VM instances.</li>
<li>If you have multiple deployment slots for an app, all deployment slots also run on the same VM instances.</li>
<li>If you enable diagnostic logs, perform backups, or run WebJobs, they also use CPU cycles and memory on these VM instances.</li>
</ul>
<h4 id="what-if-my-app-needs-more-capabilities-or-features">What if my app needs more capabilities or features?</h4>
<p>Isolate your app into a new App Service plan when:</p>
<ul>
<li>The app is resource-intensive.</li>
<li>You want to scale the app independently from the other apps in the existing plan.</li>
<li>The app needs resource in a different geographical region.</li>
</ul>
<h3 id="deploy-to-app-service">Deploy to App Service</h3>
<h4 id="automated-deployment">Automated deployment</h4>
<p>Azure supports automated deployment directly from several sources. The following options are available:</p>
<ul>
<li>Azure DevOps</li>
<li>GitHub</li>
<li>Bitbucket</li>
</ul>
<h4 id="manual-deployment">Manual deployment</h4>
<p>There are a few options that you can use to manually push your code to Azure:</p>
<ul>
<li><strong>Git</strong>: App Service web apps feature a Git URL that you can add as a remote repository.</li>
<li><strong>CLI</strong>: <code>webapp up</code> is a feature of the <code>az</code> command-line interface that packages your app and deploys it.</li>
<li><strong>Zip deploy</strong>: Use <code>curl</code> or a similar HTTP utility to send a ZIP of your application files to App Service.</li>
<li><strong>FTP/S</strong>: FTP or FTPS is a traditional way of pushing your code to many hosting environments, including App Service.</li>
</ul>
<h4 id="use-deployment-slots">Use deployment slots</h4>
<p>Whenever possible, use deployment slots when deploying a new production build. When using a Standard App Service Plan tier or better, you can deploy your app to a staging environment and then swap your staging and production slots. The swap operation warms up the necessary worker instances to match your production scale, thus eliminating downtime.</p>
<h3 id="explore-authentication-and-authorization-in-app-service">Explore authentication and authorization in App Service</h3>
<p>Azure App Service provides built-in authentication and authorization support by writing minimal or no code.</p>
<h4 id="why-use-the-built-in-authentication">Why use the built-in authentication?</h4>
<p>The built-in authentication feature for App Service and Azure Functions can save you time and effort by providing out-of-the-box authentication with federated identity providers.</p>
<h4 id="identity-providers">Identity providers</h4>
<p>App Service uses federated identity, in which a third-party identity provider manages the user identities and authentication flow for you. The following identity providers are available by default:</p>
<ul>
<li>Microsoft Identity Platform</li>
<li>Facebook</li>
<li>Google</li>
<li>Twitter</li>
<li>Any OpenID Connect provider</li>
</ul>
<h5 id="how-it-works">How it works</h5>
<p>The authentication and authorization module runs in the same sandbox as your application code. When it’s enabled, every incoming HTTP request passes through it before being handled by your application code. This module handles several things for your app:</p>
<ul>
<li>Authenticates users with the specified provider</li>
<li>Validates, stores, and refreshes tokens</li>
<li>Manages the authenticated session</li>
<li>Injects identity information into request headers</li>
</ul>
<p>The module runs separately from your application code and is configured using app settings.</p>
<h5 id="authentication-flow">Authentication flow</h5>
<ul>
<li>Without provider SDK: The application delegates federated sign-in to App Service. This is typically the case with browser apps, which can present the provider’s login page to the user. The server code manages the sign-in process, so it is also called <em>server-directed flow</em> or <em>server flow</em>.</li>
<li>With provider SDK: The application signs users in to the provider manually and then submits the authentication token to App Service for validation. This is typically the case with browser-less apps, which can’t present the provider’s sign-in page to the user. The application code manages the sign-in process, so it is also called <em>client-directed flow</em> or <em>client flow</em>. This applies to REST APIs, Azure Functions, JavaScript browser clients, and native mobile apps that sign users in using the provider’s SDK.</li>
</ul>

<table>
<thead>
<tr>
<th>Step</th>
<th>Without provider SDK</th>
<th>With provider SDK</th>
</tr>
</thead>
<tbody>
<tr>
<td>Sign user in</td>
<td>Redirects client to <code>/.auth/login/&lt;provider&gt;</code>.</td>
<td>Client code signs user in directly with provider’s SDK and receives an authentication token. For information, see the provider’s documentation.</td>
</tr>
<tr>
<td>Post-authentication</td>
<td>Provider redirects client to <code>/.auth/login/&lt;provider&gt;/callback</code>.</td>
<td>Client code posts token from provider to <code>/.auth/login/&lt;provider&gt;</code> for validation.</td>
</tr>
<tr>
<td>Establish authenticated session</td>
<td>App Service adds authenticated cookie to response.</td>
<td>App Service returns its own authentication token to client code.</td>
</tr>
<tr>
<td>Serve authenticated content</td>
<td>Client includes authentication cookie in subsequent requests (automatically handled by browser).</td>
<td>Client code presents authentication token in <code>X-ZUMO-AUTH</code> header (automatically handled by Mobile Apps client SDKs).</td>
</tr>
</tbody>
</table><h5 id="authorization-behavior">Authorization behavior</h5>
<ul>
<li>Allow unauthenticated requests: This option defers authorization of unauthenticated traffic to your application code.</li>
<li>Require authentication: This option will reject any unauthenticated traffic to your application. This rejection can be a redirect action to one of the configured identity providers.</li>
</ul>
<h3 id="discover-app-service-networking-features">Discover App Service networking features</h3>
<p>By default, apps hosted in App Service are accessible directly through the internet and can reach only internet-hosted endpoints. But for many applications, you need to control the inbound and outbound network traffic.</p>
<p>Deployment-Types:</p>
<ul>
<li>Multi-Tenant pricing SKUs: Free, Shared, Basic, Standard, Premium, PremiumV2, and PremiumV3</li>
<li>Single-Tenant pricing SKU:  Isolated</li>
</ul>
<h4 id="multi-tenant-app-service-networking-features">Multi-tenant App Service networking features</h4>
<p>All the roles in an App Service deployment exist in a multi-tenant network. The roles that handle incoming HTTP or HTTPS requests are called <em>front ends</em>. The roles that host the customer workload are called <em>workers</em>.</p>
<p>Because there are many different customers in the same App Service scale unit, you can’t connect the App Service network directly to your network. Instead of connecting the networks, you need features to handle the various aspects of application communication.</p>

<table>
<thead>
<tr>
<th>Inbound features</th>
<th>Outbound features</th>
</tr>
</thead>
<tbody>
<tr>
<td>App-assigned address</td>
<td>Hybrid Connections</td>
</tr>
<tr>
<td>Access restrictions</td>
<td>Gateway-required virtual network integration</td>
</tr>
<tr>
<td>Service endpoints</td>
<td>Virtual network integration</td>
</tr>
<tr>
<td>Private endpoints</td>
<td></td>
</tr>
</tbody>
</table><p>The following inbound use cases are examples of how to use App Service networking features to control traffic inbound to your app.</p>

<table>
<thead>
<tr>
<th>Inbound use case</th>
<th>Feature</th>
</tr>
</thead>
<tbody>
<tr>
<td>Support IP-based SSL needs for your app</td>
<td>App-assigned address</td>
</tr>
<tr>
<td>Support unshared dedicated inbound address for your app</td>
<td>App-assigned address</td>
</tr>
<tr>
<td>Restrict access to your app from a set of well-defined addresses</td>
<td>Access restrictions</td>
</tr>
</tbody>
</table><h4 id="default-networking-behavior">Default networking behavior</h4>
<p>The Free and Shared SKU plans host customer workloads on multitenant workers. The Basic and higher plans host customer workloads that are dedicated to only one App Service plan.</p>
<h5 id="outbound-addresses">Outbound addresses</h5>
<p>The worker VMs are broken down in large part by the App Service plans.</p>
<ul>
<li>The Free, Shared, Basic, Standard, and Premium plans all use the same worker VM type.</li>
<li>The PremiumV2 plan uses another VM type.</li>
<li>PremiumV3 uses yet another VM type.<br>
When you change the VM family, you get a different set of outbound addresses.</li>
</ul>
<p>The outbound addresses used by your app for making outbound calls are listed in the properties for your app. These addresses are shared by all the apps running on the same worker VM family in the App Service deployment.</p>
<h5 id="find-outbound-ips">Find outbound IPs</h5>
<p>To find the outbound IP addresses currently used by your app in the Azure portal, click <strong>Properties</strong> in your app’s left-hand navigation. If you want to see all the addresses that your app might use in a scale unit, there’s a property called <code>possibleOutboundAddresses</code> that will list them.</p>
<h3 id="configure-web-app-settings">Configure web app settings</h3>
<h4 id="configure-application-settings">Configure application settings</h4>
<p>In App Service, app settings are variables passed as environment variables to the application code.</p>
<p><img src="https://docs.microsoft.com/en-us/learn/wwl-azure/configure-web-app-settings/media/configure-app-settings.png" alt="Navigating to Configuration > Application settings"><br>
For <a href="http://ASP.NET">ASP.NET</a> and <a href="http://ASP.NET">ASP.NET</a> Core developers, setting app settings in App Service are like setting them in <code>&lt;appSettings&gt;</code> in <em>Web.config</em> or <em>appsettings.json</em>, but the values in App Service override the ones in <em>Web.config</em> or <em>appsettings.json</em>.</p>
<p>App settings are always encrypted when stored (encrypted-at-rest).</p>
<h4 id="adding-and-editing-settings">Adding and editing settings</h4>
<p><img src="https://docs.microsoft.com/en-us/learn/wwl-azure/configure-web-app-settings/media/app-configure-slotsetting.png" alt="Selecting deployment slot setting to stick the setting to the current slot.">If you are using deployment slots you can specify if your setting is swappable or not. In the dialog, you can stick the setting to the current slot.</p>
<h5 id="editing-application-settings-in-bulk">Editing application settings in bulk</h5>
<p>To add or edit app settings in bulk, click the <strong>Advanced</strong> edit button.</p>
<pre><code>[
  {
    "name": "&lt;key-1&gt;",
    "value": "&lt;value-1&gt;",
    "slotSetting": false
  },
  {
    "name": "&lt;key-2&gt;",
    "value": "&lt;value-2&gt;",
    "slotSetting": false
  },
  ...
]
</code></pre>
<h3 id="configure-general-settings">Configure general settings</h3>
<ul>
<li><strong>Stack settings</strong>: The software stack to run the app, including the language and SDK versions.<br>
<img src="https://docs.microsoft.com/en-us/learn/wwl-azure/configure-web-app-settings/media/open-general-linux.png" alt="Establishing the stack settings which includes the programming language.">- <strong>Platform settings</strong>: Lets you configure settings for the hosting platform</li>
<li><strong>Debugging</strong>: Enable remote debugging for <a href="http://ASP.NET">ASP.NET</a>, <a href="http://ASP.NET">ASP.NET</a> Core, or Node.js apps.</li>
<li><strong>Incoming client certificates</strong>: require client certificates in mutual authentication.</li>
</ul>
<h3 id="configure-path-mappings">Configure path mappings</h3>
<p>In the <strong>Configuration &gt; Path mappings</strong> section you can configure handler mappings, and virtual application and directory mappings. The <strong>Path mappings</strong> page will display different options based on the OS type.</p>
<h4 id="windows-apps-uncontainerized">Windows apps (uncontainerized)</h4>
<p>For Windows apps, you can customize the IIS handler mappings and virtual applications and directories. Handler mappings let you add custom script processors to handle requests for specific file extensions.</p>
<ul>
<li><strong>Extension</strong>: The file extension you want to handle, such as *<em>.php</em> or <em>handler.fcgi</em>.</li>
<li><strong>Script processor</strong>: The absolute path of the script processor.</li>
<li><strong>Arguments</strong>: Optional command-line arguments for the script processor.</li>
</ul>
<h4 id="linux-and-containerized-apps">Linux and containerized apps</h4>
<p>You can add custom storage for your containerized app.</p>
<h3 id="enable-diagnostic-logging">Enable diagnostic logging</h3>
<p>| Type | Platform | Location | Description |<br>
| Application logging | Windows, Linux | App Service file system and/or Azure Storage blobs | Logs messages generated by your application code. Each message is assigned one of the following categories: <strong>Critical</strong>, <strong>Error</strong>, <strong>Warning</strong>, <strong>Info</strong>, <strong>Debug</strong>, and <strong>Trace</strong>. |<br>
| Web server logging | Windows | App Service file system or Azure Storage blobs | Raw HTTP request data in the W3C extended log file format.  |<br>
| Detailed error logging | Windows | App Service file system | Copies of the <em>.html</em> error pages that would have been sent to the client browser. |<br>
| Failed request tracing | Windows | App Service file system | Detailed tracing information on failed requests  |<br>
| Deployment logging | Windows, Linux | App Service file system | Helps determine why a deployment failed. Deployment logging happens automatically and there are no configurable settings for deployment logging. |</p>
<h4 id="enable-application-logging-windows">Enable application logging (Windows)</h4>
<p>To enable application logging for Windows apps in the Azure portal, navigate to your app and select <strong>App Service logs</strong>. Available Options:</p>
<ul>
<li>Application Logging (Filesystem)</li>
<li>Application Logging (Blob),</li>
</ul>
<h4 id="enable-application-logging-linuxcontainer">Enable application logging (Linux/Container)</h4>
<p>Available Options:</p>
<ul>
<li>File System</li>
</ul>
<h4 id="enable-web-server-logging">Enable web server logging</h4>
<p>Available Options:</p>
<ul>
<li>Filesystem)</li>
<li>Blob</li>
</ul>
<h4 id="add-log-messages-in-code">Add log messages in code</h4>
<p>In your application code, you use the usual logging facilities to send log messages to the application logs.</p>
<h4 id="stream-logs">Stream logs</h4>
<p>Before you stream logs in real time, enable the log type that you want. Any information written to files ending in .txt, .log, or .htm that are stored in the <code>/LogFiles</code> directory (<code>d:/home/logfiles</code>) is streamed by App Service. Places, where logs can be streamed:</p>
<ul>
<li>Azure portal</li>
<li>Azure CLI</li>
<li>Local console</li>
</ul>
<h4 id="access-log-files">Access log files</h4>
<p>If you configure the Azure Storage blobs option for a log type, you need a client tool that works with Azure Storage.</p>
<p>For logs stored in the App Service file system, the easiest way is to download the ZIP file in the browser</p>
<h3 id="configure-security-certificates">Configure security certificates</h3>
<p>A certificate uploaded into an app is stored in a deployment unit that is bound to the app service plan’s resource group and region combination (internally called a <em>webspace</em>). This makes the certificate accessible to other apps in the same resource group and region combination.</p>
<ul>
<li>Create a free App Service managed certificate</li>
<li>Purchase an App Service certificate</li>
<li>Import a certificate from Key Vault</li>
<li>Upload a private certificate</li>
<li>Upload a public certificate: Public certificates are not used to secure custom domains, but you can load them into your code if you need them to access remote resources.</li>
</ul>
<h4 id="private-certificate-requirements">Private certificate requirements</h4>
<p>The free <strong>App Service managed certificate</strong> and the <strong>App Service certificate</strong> already satisfy the requirements of App Service. If you want to use a private certificate in App Service, your certificate must meet the following requirements:</p>
<ul>
<li>Exported as a password-protected PFX file, encrypted using triple DES.</li>
<li>Contains private key at least 2048 bits long</li>
<li>Contains all intermediate certificates in the certificate chain</li>
</ul>
<p>To secure a custom domain in a TLS binding, the certificate has additional requirements:</p>
<ul>
<li>Contains an Extended Key Usage for server authentication (OID = 1.3.6.1.5.5.7.3.1)</li>
<li>Signed by a trusted certificate authority</li>
</ul>
<h4 id="creating-a-free-managed-certificate">Creating a free managed certificate</h4>
<p>To create custom TLS/SSL bindings or enable client certificates for your App Service app, your App Service plan must be in the <strong>Basic</strong>, <strong>Standard</strong>, <strong>Premium</strong>, or <strong>Isolated</strong> tier.</p>
<p>It’s a TLS/SSL server certificate that’s fully managed by App Service and renewed continuously. You create the certificate and bind it to a custom domain, and let App Service do the rest.</p>
<h4 id="import-an-app-service-certificate">Import an App Service Certificate</h4>
<p>If you purchase an App Service Certificate from Azure, Azure manages the following tasks:</p>
<ul>
<li>Takes care of the purchase process from GoDaddy.</li>
<li>Performs domain verification of the certificate.</li>
<li>Maintains the certificate in Azure Key Vault.</li>
<li>Manages certificate renewal.</li>
<li>Synchronize the certificate automatically with the imported copies in App Service apps.</li>
</ul>
<h4 id="upload-a-private-certificate">Upload a private certificate</h4>
<p>If your certificate authority gives you multiple certificates in the certificate chain, you need to merge the certificates in order. Then you can Export your merged TLS/SSL certificate with the private key that your certificate request was generated with.</p>
<h4 id="enforce-https">Enforce HTTPS</h4>
<p>By default, anyone can still access your app using HTTP. You can redirect all HTTP requests to the HTTPS port.<br>
<img src="https://docs.microsoft.com/en-us/learn/wwl-azure/configure-web-app-settings/media/enforce-https.png" alt="Enabling HTTPS Only in your web app."></p>
<h3 id="manage-app-features">Manage app features</h3>
<p>Feature management is a modern software-development practice that decouples feature release from code deployment and enables quick changes to feature availability on demand. It uses a technique called feature flags (also known as feature toggles, feature switches, and so on) to dynamically administer a feature’s lifecycle.</p>
<h4 id="basic-concepts">Basic concepts</h4>
<ul>
<li><strong>Feature flag</strong>: The state of the feature flag triggers whether the code block runs or not.</li>
<li><strong>Feature manager</strong>: A feature manager is an application package that handles the lifecycle of all the feature flags in an application.</li>
<li><strong>Filter</strong>: A filter is a rule for evaluating the state of a feature flag.</li>
</ul>
<p>An effective implementation of feature management consists of at least two components working in concert:</p>
<ul>
<li>An application that makes use of feature flags.</li>
<li>A separate repository that stores the feature flags and their current states.</li>
</ul>
<h4 id="feature-flag-usage-in-code">Feature flag usage in code</h4>
<pre><code>if (featureFlag) {
    // Run the following code
}
</code></pre>
<h4 id="feature-flag-declaration">Feature flag declaration</h4>
<p>Each feature flag has two parts: a name and a list of one or more filters that are used to evaluate if a feature’s state is <em>on</em> (that is, when its value is <code>True</code>). A filter defines a use case for when a feature should be turned on.</p>
<p>The feature manager supports <em>appsettings.json</em> as a configuration source for feature flags.</p>
<h4 id="feature-flag-repository">Feature flag repository</h4>
<p>To use feature flags effectively, you need to externalize all the feature flags used in an application. This approach allows you to change feature flag states without modifying and redeploying the application itself. Azure App Configuration is designed to be a centralized repository for feature flags.</p>
<h2 id="scale-apps-in-azure-app-service">Scale apps in Azure App Service</h2>
<h3 id="examine-autoscale-factors">Examine autoscale factors</h3>
<h4 id="what-is-autoscaling">What is autoscaling?</h4>
<p>Autoscaling is a cloud system or process that adjusts available resources based on the current demand. Autoscaling performs scaling <em>in and out</em>, as opposed to scaling <em>up and down</em>. Autoscaling can be triggered according to a schedule, or by assessing whether the system is running short on resources.</p>
<h4 id="azure-app-service-autoscaling">Azure App Service Autoscaling</h4>
<p>Autoscaling in Azure App Service monitors the resource metrics of a web app as it runs. It detects situations where additional resources are required to handle an increasing workload, and ensures those resources are available before the system becomes overloaded.</p>
<h5 id="autoscaling-rules">Autoscaling rules</h5>
<p>Autoscaling makes its decisions based on rules that you define. A rule specifies the threshold for a metric, and triggers an autoscale event when this threshold is crossed. Autoscaling can also deallocate resources when the workload has diminished.</p>
<h4 id="when-should-you-consider-autoscaling">When should you consider autoscaling?</h4>
<p>Autoscaling provides elasticity for your services. It’s a suitable solution when hosting any application when you can’t easily predict the workload in advance, or when the workload is likely to vary by date or time.</p>
<p>Autoscaling improves availability and fault tolerance.</p>
<p>Autoscaling works by adding or removing web servers. If your web apps perform resource-intensive processing as part of each request, then autoscaling might not be an effective approach. In these situations, manually scaling up may be necessary.</p>
<p>Autoscaling isn’t the best approach to handling long-term growth. Autoscaling has an overhead associated with monitoring resources and determining whether to trigger a scaling event. In this scenario, if you can anticipate the rate of growth, manually scaling the system over time may be a more cost effective approach.</p>
<p>The number of instances of a service is also a factor. The fewer the number of instances initially, the less capacity you have to handle an increasing workload while autoscaling spins up additional instances.</p>
<h3 id="identify-autoscale-factors">Identify autoscale factors</h3>
<p>Autoscaling enables you to specify the conditions under which a web app should be scaled out, and back in again:</p>
<ul>
<li>Scale based on a metric, such as the length of the disk queue, or the number of HTTP requests awaiting processing.</li>
<li>Scale to a specific instance count according to a schedule. For example, you can arrange to scale out at a particular time of day, or on a specific date or day of the week. You also specify an end date, and the system will scale back in at this time.</li>
</ul>
<p>When the web app scales out, Azure starts new instances of the hardware defined by the App Service Plan to the app.<br>
To prevent runaway autoscaling, an App Service Plan has an instance limit.</p>
<h4 id="metrics-for-autoscale-rules">Metrics for autoscale rules</h4>
<ul>
<li><strong>CPU Percentage</strong></li>
<li><strong>Memory Percentage</strong></li>
<li><strong>Disk Queue Length</strong></li>
<li><strong>Http Queue Length</strong></li>
<li><strong>Data In</strong>. This metric is the number of bytes received across all instances.</li>
<li><strong>Data Out</strong>. This metric is the number of bytes sent by all instances.</li>
</ul>
<h4 id="how-an-autoscale-rule-analyzes-metrics">How an autoscale rule analyzes metrics</h4>
<p>Autoscaling works by analyzing trends in metric values over time across all instances.</p>
<p>In the first step, an autoscale rule aggregates the values retrieved for a metric for all instances across a period of time known as the <em>time grain</em>.Each metric has its own intrinsic time grain, but in most cases this period is 1 minute. The aggregated value is known as the <em>time aggregation</em>. The options available are <em>Average</em>, <em>Minimum</em>, <em>Maximum</em>, <em>Total</em>, <em>Last</em>, and <em>Count</em>.</p>
<p>autoscale rule performs a second step that performs a further aggregation of the value calculated by the <em>time aggregation</em> over a longer, user-specified period, known as the <em>Duration</em>. The minimum <em>Duration</em> is 5 minutes.</p>
<p>The aggregation calculation for the <em>Duration</em> can be different from that of the <em>time grain</em>.</p>
<h4 id="autoscale-actions">Autoscale actions</h4>
<p>When an autoscale rule detects that a metric has crossed a threshold, it can perform an autoscale action. An autoscale action can be <em>scale-out</em> or <em>scale-in</em>.</p>
<p>An autoscale action has a <em>cool down</em> period, specified in minutes. During this interval, the scale rule won’t be triggered again. Remember that it takes time to start up or shut down instances, and so any metrics gathered might not show any significant changes for several minutes. The minimum cool down period is five minutes.</p>
<h4 id="pairing-autoscale-rules">Pairing autoscale rules</h4>
<p>You should plan for scaling-in when a workload decreases. Consider defining autoscale rules in pairs in the same autoscale condition.</p>
<h4 id="combining-autoscale-rules">Combining autoscale rules</h4>
<p>A single autoscale condition can contain several autoscale rules (for example, a scale-out rule and the corresponding scale-in rule). However, the autoscale rules in an autoscale condition don’t have to be directly related. When determining whether to scale out, the autoscale action will be performed if <strong>any</strong> of the scale-out rules are met. When scaling in, the autoscale action will run <strong>only if all</strong> of the scale-in rules are met.</p>
<h3 id="enable-autoscale-in-app-service">Enable autoscale in App Service</h3>
<h4 id="enable-autoscaling">Enable autoscaling</h4>
<p>To get started with autoscaling navigate to your App Service plan in the Azure portal and select <strong>Scale out (App Service plan)</strong> in the <strong>Settings</strong> group in the left navigation pane. Selecting <strong>Custom autoscale</strong> reveals condition groups you can use to manage your scale settings.</p>
<p><img src="https://docs.microsoft.com/en-us/learn/wwl-azure/scale-apps-app-service/media/enable-autoscale.png" alt="Enabling autoscale"></p>
<h4 id="add-scale-conditions">Add scale conditions</h4>
<p>Once you enable autoscaling, you can edit the automatically created default scale condition, and you can add your own custom scale conditions.<br>
<img src="https://docs.microsoft.com/en-us/learn/wwl-azure/scale-apps-app-service/media/autoscale-conditions.png" alt="The condition page for an App Service Plan showing the default scale condition."></p>
<h4 id="create-scale-rules">Create scale rules</h4>
<p><img src="https://docs.microsoft.com/en-us/learn/wwl-azure/scale-apps-app-service/media/autoscale-rules.png" alt="Create scale rules"></p>
<h4 id="monitor-autoscaling-activity">Monitor autoscaling activity</h4>
<p>The Azure portal enables you to track when autoscaling has occurred through the <strong>Run history</strong> chart.<br>
<img src="https://docs.microsoft.com/en-us/learn/wwl-azure/scale-apps-app-service/media/autoscale-run-history.png" alt=""></p>
<p>You can use the <strong>Run history</strong> chart in conjunction with the metrics shown on the <strong>Overview</strong> page to correlate the autoscaling events with resource utilization.</p>
<p><img src="https://docs.microsoft.com/en-us/learn/wwl-azure/scale-apps-app-service/media/service-plan-metrics.png" alt="The metrics shown on the App Service Plan overview page."></p>
<h3 id="explore-autoscale-best-practices">Explore autoscale best practices</h3>
<ul>
<li><strong>Ensure the maximum and minimum values are different and have an adequate margin between them</strong></li>
<li><strong>Choose the appropriate statistic for your diagnostics metric</strong></li>
<li><strong>Choose the thresholds carefully for all metric types</strong>: Don’t define rules like this, because they will lead to a constant scaling out and scaling back in: Increase instances by one count when Thread Count &gt;= 600, Decrease instances by one count when Thread Count &lt;= 600</li>
<li><strong>Considerations for scaling when multiple rules are configured in a profile</strong>: On  <em>scale out</em>, autoscale runs if any rule is met. On  <em>scale-in</em>, autoscale require all rules to be met.</li>
<li><strong>Always select a safe default instance count</strong></li>
<li><strong>Configure autoscale notifications</strong>: Autoscale will post to the Activity Log if any of the following conditions occur:
<ul>
<li>Autoscale issues a scale operation</li>
<li>Autoscale service successfully completes a scale action</li>
<li>Autoscale service fails to take a scale action.</li>
<li>Metrics aren’t available for autoscale service to make a scale decision.</li>
<li>Metrics are available (recovery) again to make a scale decision.<br>
In addition to using activity log alerts, you can also configure email or webhook notifications to get notified.</li>
</ul>
</li>
</ul>
<h2 id="explore-azure-app-service-deployment-slots">Explore Azure App Service deployment slots</h2>
<h3 id="explore-staging-environments">Explore staging environments</h3>
<p>You can use a separate deployment slot instead of the default production slot when you’re running in the <strong>Standard</strong>, <strong>Premium</strong>, or <strong>Isolated</strong> App Service plan tier. Deployment slots are live apps with their own host names.</p>
<p>Deploying your application to a non-production slot has the following benefits:</p>
<ul>
<li>You can validate app changes in a staging deployment slot before swapping it with the production slot.</li>
<li>Deploying an app to a slot first and swapping it into production makes sure that all instances of the slot are warmed up before being swapped into production.</li>
<li>After a swap, the slot with previously staged app now has the previous production app. If the changes swapped into the production slot aren’t as you expect, you can perform the same swap immediately to get your “last known good site” back.</li>
</ul>
<p>Each App Service plan tier supports a different number of deployment slots. There’s no additional charge for using deployment slots.</p>
<h3 id="examine-slot-swapping">Examine slot swapping</h3>
<p>When you swap slots (for example, from a staging slot to the production slot), App Service does the following to ensure that the target slot doesn’t experience downtime:</p>
<ol>
<li>
<p>Apply the following settings from the target slot (for example, the production slot) to all instances of the source slot:</p>
<ul>
<li>Slot-specific app settings and connection strings, if applicable.</li>
<li>Continuous deployment settings, if enabled.</li>
<li>App Service authentication settings, if enabled.</li>
</ul>
<p>Any of these cases trigger all instances in the source slot to restart. During  <strong>swap with preview</strong>, this marks the end of the first phase. The swap operation is paused, and you can validate that the source slot works correctly with the target slot’s settings.</p>
</li>
<li>
<p>Wait for every instance in the source slot to complete its restart.</p>
</li>
<li>
<p>If local cache is enabled, trigger local cache initialization by making an HTTP request to the application root ("/") on each instance of the source slot.</p>
</li>
<li>
<p>If auto swap is enabled with custom warm-up, trigger Application Initiation by making an HTTP request to the application root ("/") on each instance of the source slot.</p>
</li>
<li>
<p>If all instances on the source slot are warmed up successfully, swap the two slots by switching the routing rules for the two slots.</p>
</li>
<li>
<p>Now that the source slot has the pre-swap app previously in the target slot, perform the same operation by applying all settings and restarting the instances.</p>
</li>
</ol>

<table>
<thead>
<tr>
<th>Settings that are swapped</th>
<th>Settings that aren’t swapped</th>
</tr>
</thead>
<tbody>
<tr>
<td>General settings, such as framework version, 32/64-bit, web sockets</td>
<td>Publishing endpoints</td>
</tr>
<tr>
<td>App settings (can be configured to stick to a slot)</td>
<td>Custom domain names</td>
</tr>
<tr>
<td>Connection strings (can be configured to stick to a slot)</td>
<td>Non-public certificates and TLS/SSL settings</td>
</tr>
<tr>
<td>Handler mappings</td>
<td>Scale settings</td>
</tr>
<tr>
<td>Public certificates</td>
<td>WebJobs schedulers</td>
</tr>
<tr>
<td>WebJobs content</td>
<td>IP restrictions</td>
</tr>
<tr>
<td>Hybrid connections</td>
<td>Always On</td>
</tr>
<tr>
<td>Virtual network integration</td>
<td>Diagnostic log settings</td>
</tr>
<tr>
<td>Service endpoints</td>
<td>Cross-origin resource sharing (CORS)</td>
</tr>
<tr>
<td>Azure Content Delivery Network</td>
<td></td>
</tr>
</tbody>
</table><p>Settings can be made swappable or be prevented from swapping via configuration.</p>
<h3 id="swap-deployment-slots">Swap deployment slots</h3>
<h4 id="manually-swapping-deployment-slots">Manually swapping deployment slots</h4>
<ol>
<li>Go to your app’s  <strong>Deployment slots</strong>  page and select  <strong>Swap</strong>.</li>
<li>Select the desired <strong>Source</strong> and <strong>Target</strong> slots. Usually, the target is the production slot. Also, select the <strong>Source Changes</strong> and <strong>Target Changes</strong> tabs and verify that the configuration changes are expected.</li>
</ol>
<h5 id="swap-with-preview-multi-phase-swap">Swap with preview (multi-phase swap)</h5>
<p>Before you swap into production as the target slot, validate that the app runs with the swapped settings.</p>
<ol>
<li>Follow the steps above in Swap deployment slots but select  <strong>Perform swap with preview</strong>. The dialog box shows you how the configuration in the source slot changes in phase 1, and how the source and target slot change in phase 2.</li>
<li>When you’re ready to start the swap, select <strong>Start Swap</strong>. When phase 1 finishes, you’re notified in the dialog box. Preview the swap in the source slot by going to <code>https://&lt;app_name&gt;-&lt;source-slot-name&gt;.azurewebsites.net</code>.</li>
<li>When you’re ready to complete the pending swap, select <strong>Complete Swap</strong> in <strong>Swap action</strong> and select <strong>Complete Swap</strong>.</li>
</ol>
<h4 id="configure-auto-swap">Configure auto swap</h4>
<p>Auto swap streamlines Azure DevOps scenarios where you want to deploy your app continuously with zero cold starts and zero downtime for customers of the app.</p>
<h4 id="specify-custom-warm-up">Specify custom warm-up</h4>
<p>Some apps might require custom warm-up actions before the swap. The <code>applicationInitialization</code> configuration element in web.config lets you specify custom initialization actions.</p>
<pre><code>&lt;system.webServer&gt;
 &lt;applicationInitialization&gt;
   &lt;add initializationPage="/" hostName="[app hostname]" /&gt;
   &lt;add initializationPage="/Home/About" hostName="[app hostname]" /&gt;
 &lt;/applicationInitialization&gt; 
&lt;/system.webServer&gt;
</code></pre>
<h4 id="roll-back-and-monitor-a-swap">Roll back and monitor a swap</h4>
<p>If any errors occur in the target slot (for example, the production slot) after a slot swap, restore the slots to their pre-swap states by swapping the same two slots immediately.</p>
<h3 id="route-traffic-in-app-service">Route traffic in App Service</h3>
<p>By default, all client requests to the app’s production URL (<code>http://&lt;app_name&gt;.azurewebsites.net</code>) are routed to the production slot. You can route a portion of the traffic to another slot. This feature is useful if you need user feedback for a new update, but you’re not ready to release it to production.</p>
<h4 id="route-production-traffic-automatically">Route production traffic automatically</h4>
<p>In the  <strong>Traffic %</strong>  column of the slot you want to route to, specify a percentage (between 0 and 100) to represent the amount of total traffic you want to route. Select  <strong>Save</strong>.</p>
<p>After a client is automatically routed to a specific slot, it’s “pinned” to that slot for the life of that client session. On the client browser, you can see which slot your session is pinned to by looking at the <code>x-ms-routing-name</code> cookie in your HTTP headers. A request that’s routed to the “staging” slot has the cookie <code>x-ms-routing-name=staging</code>. A request that’s routed to the production slot has the cookie <code>x-ms-routing-name=self</code>.</p>
<h4 id="route-production-traffic-manually">Route production traffic manually</h4>
<p>In addition to automatic traffic routing, App Service can route requests to a specific slot. This is useful when you want your users to be able to opt in to or opt out of your beta app. To route production traffic manually, you use the <code>x-ms-routing-name</code> query parameter.</p>
<p>To let users opt out of your beta app, for example, you can put this link on your webpage:</p>
<pre><code>&lt;a href="&lt;webappname&gt;.azurewebsites.net/?x-ms-routing-name=self"&gt;Go back to production app&lt;/a&gt;
</code></pre>
<p>By default, new slots are given a routing rule of <code>0%</code>, a default value is displayed in grey. When you explicitly set this value to <code>0%</code> it is displayed in black, your users can access the staging slot manually by using the <code>x-ms-routing-name</code> query parameter.</p>
<h1 id="az-204-implement-azure-functions">AZ-204: Implement Azure Functions</h1>
<h2 id="explore-azure-functions">Explore Azure Functions</h2>
<h4 id="discover-azure-functions">Discover Azure Functions</h4>
<p>Azure Functions are a great solution for processing data, integrating systems, working with the internet-of-things (IoT), and building simple APIs and microservices. Consider Functions for tasks like image or order processing, file maintenance, or for any tasks that you want to run on a schedule. Functions provides templates to get you started with key scenarios.</p>
<p>Azure Functions supports <em>triggers</em>, which are ways to start execution of your code, and <em>bindings</em>, which are ways to simplify coding for input and output data.</p>
<h4 id="compare-azure-functions-and-azure-logic-apps">Compare Azure Functions and Azure Logic Apps</h4>
<p>Both Functions and Logic Apps enable serverless workloads. Azure Functions is a serverless compute service, whereas Azure Logic Apps provides serverless workflows. Both can create complex orchestrations. An orchestration is a collection of functions or steps, called actions in Logic Apps, that are executed to accomplish a complex task.</p>
<p>For Azure Functions, you develop orchestrations by writing code and using the Durable Functions extension.<br>
For Logic Apps, you create orchestrations by using a GUI or editing configuration files.</p>

<table>
<thead>
<tr>
<th></th>
<th>Azure Functions</th>
<th>Logic Apps</th>
</tr>
</thead>
<tbody>
<tr>
<td>Development</td>
<td>Code-first (imperative)</td>
<td>Designer-first (declarative)</td>
</tr>
<tr>
<td>Connectivity</td>
<td>About a dozen built-in binding types, write code for custom bindings</td>
<td>Large collection of connectors,</td>
</tr>
<tr>
<td>Actions</td>
<td>Each activity is an Azure function; write code for activity functions</td>
<td>Large collection of ready-made actions</td>
</tr>
<tr>
<td>Monitoring</td>
<td>Azure Application Insights</td>
<td>Azure portal, Azure Monitor logs</td>
</tr>
<tr>
<td>Management</td>
<td>REST API, Visual Studio</td>
<td>Azure portal, REST API, PowerShell, Visual Studio</td>
</tr>
</tbody>
</table><h4 id="compare-functions-and-webjobs">Compare Functions and WebJobs</h4>
<p>Like Azure Functions, Azure App Service WebJobs with the WebJobs SDK is a code-first integration service that is designed for developers.</p>
<p>Azure Functions is built on the WebJobs SDK, so it shares many of the same event triggers and connections to other Azure services.</p>
<p>| | Functions | WebJobs with WebJobs SDK |<br>
| Serverless app model with automatic scaling | Yes | No |<br>
| Develop and test in browser | Yes | No<br>
| Pay-per-use pricing | Yes | No<br>
| Integration with Logic Apps | Yes | No<br>
| Trigger events | Timer<br>
Azure Storage queues and blobs<br>
Azure Service Bus queues and topics<br>
Azure Cosmos DB<br>
Azure Event Hubs<br>
HTTP/WebHook (GitHub  Slack)<br>
Azure Event Grid | Timer<br>
Azure Storage queues and blobs<br>
Azure Service Bus queues and topics<br>
Azure Cosmos DB<br>
Azure Event Hubs<br>
File system |</p>
<p>For most scenarios, Azure Functions is the best choice.</p>
<h3 id="compare-azure-functions-hosting-options">Compare Azure Functions hosting options</h3>
<p>There are three basic hosting plans available for Azure Functions: Consumption plan, Functions Premium plan, and App service (Dedicated) plan.</p>
<p>The hosting plan you choose dictates the following behaviors:</p>
<ul>
<li>How your function app is scaled.</li>
<li>The resources available to each function app instance.</li>
<li>Support for advanced functionality, such as Azure Virtual Network connectivity.</li>
</ul>

<table>
<thead>
<tr>
<th>Plan</th>
<th>Benefits</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Consumption plan</strong></td>
<td>This is the default hosting plan. It scales automatically and you only pay for compute resources when your functions are running. Instances of the Functions host are dynamically added and removed based on the number of incoming events.</td>
</tr>
<tr>
<td><strong>Premium plan</strong></td>
<td>Automatically scales based on demand using pre-warmed workers, which run applications with no delay after being idle, runs on more powerful instances, and connects to virtual networks.</td>
</tr>
<tr>
<td><strong>Dedicated plan</strong></td>
<td>Run your functions within an App Service plan at regular App Service plan rates. Best for long-running scenarios where Durable Functions can’t be used.</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>Hosting option</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ASE</strong></td>
<td>App Service Environment (ASE) is an App Service feature that provides a fully isolated and dedicated environment for securely running App Service apps at high scale.</td>
</tr>
<tr>
<td><strong>Kubernetes</strong></td>
<td>Kubernetes provides a fully isolated and dedicated environment running on top of the Kubernetes platform.</td>
</tr>
</tbody>
</table><h4 id="always-on">Always on</h4>
<p>If you run on a Dedicated plan, you should enable the  <strong>Always on</strong>  setting so that your function app runs correctly. On an App Service plan, the functions runtime goes idle after a few minutes of inactivity, so only HTTP triggers will “wake up” your functions. Always on is available only on an App Service plan. On a Consumption plan, the platform activates function apps automatically.</p>
<h4 id="storage-account-requirements">Storage account requirements</h4>
<p>On any plan, a function app requires a general Azure Storage account, which supports Azure Blob, Queue, Files, and Table storage. This is because Functions rely on Azure Storage for operations such as managing triggers and logging function executions, but some storage accounts don’t support queues and tables.</p>
<h3 id="scale-azure-functions">Scale Azure Functions</h3>
<p>In the Consumption and Premium plans, Azure Functions scales CPU and memory resources by adding additional instances of the Functions host. The number of instances is determined on the number of events that trigger a function.</p>
<p>Each instance of the Functions host in the Consumption plan is limited to 1.5 GB of memory and one CPU.<br>
In the Premium plan, the plan size determines the available memory and CPU for all apps in that plan on that instance.</p>
<h4 id="runtime-scaling">Runtime scaling</h4>
<p>Azure Functions uses a component called the <em>scale controller</em> to monitor the rate of events and determine whether to scale out or scale in. The scale controller uses heuristics for each trigger type.</p>
<p>The unit of scale for Azure Functions is the function app. When the function app is scaled out, additional resources are allocated to run multiple instances of the Azure Functions host.</p>
<p><img src="https://docs.microsoft.com/en-us/learn/wwl-azure/explore-azure-functions/media/central-listener.png" alt="Scale controller monitoring events and creating instances"></p>
<h4 id="scaling-behaviors">Scaling behaviors</h4>
<ul>
<li>
<p><strong>Maximum instances:</strong>  A single function app only scales out to a maximum of 200 instances. A single instance may process more than one message or request at a time though, so there isn’t a set limit on number of concurrent executions.</p>
</li>
<li>
<p><strong>New instance rate:</strong>  For HTTP triggers, new instances are allocated, at most, once per second. For non-HTTP triggers, new instances are allocated, at most, once every 30 seconds.</p>
</li>
</ul>
<h4 id="limit-scale-out">Limit scale out</h4>
<p>You may wish to restrict the maximum number of instances an app used to scale out. This is most common for cases where a downstream component like a database has limited throughput. By default, Consumption plan functions scale out to as many as 200 instances, and Premium plan functions will scale out to as many as 100 instances.</p>
<h4 id="azure-functions-scaling-in-an-app-service-plan">Azure Functions scaling in an App service plan</h4>
<p>Using an App Service plan, you can manually scale out by adding more VM instances.</p>
<h3 id="develop-azure-functions">Develop Azure Functions</h3>
<p>A function contains two important pieces - your code, which can be written in a variety of languages, and some config, the <em>function.json</em> file. For compiled languages, this config file is generated automatically from annotations in your code. For scripting languages, you must provide the config file yourself.</p>
<p>The  <em>function.json</em>  file defines the function’s trigger, bindings, and other configuration settings. Every function has one and only one trigger. The runtime uses this config file to determine the events to monitor and how to pass data into and return data from a function execution. The following is an example  <em>function.json</em>  file.</p>
<p>JSON</p>
<pre><code>{
    "disabled":false,
    "bindings":[
        // ... bindings here
        {
            "type": "bindingType",
            "direction": "in",
            "name": "myParamName",
            // ... more depending on binding
        }
    ]
}
</code></pre>
<p>The  <code>bindings</code>  property is where you configure both triggers and bindings. Each binding shares a few common settings and some settings which are specific to a particular type of binding. Every binding requires the following settings:</p>

<table>
<thead>
<tr>
<th>Property</th>
<th>Types</th>
<th>Comments</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>type</code></td>
<td>string</td>
<td>Name of binding. For example,  <code>queueTrigger</code>.</td>
</tr>
<tr>
<td><code>direction</code></td>
<td>string</td>
<td>Indicates whether the binding is for receiving data into the function or sending data from the function. For example,  <code>in</code>  or  <code>out</code>.</td>
</tr>
<tr>
<td><code>name</code></td>
<td>string</td>
<td>The name that is used for the bound data in the function. For example,  <code>myQueue</code>.</td>
</tr>
</tbody>
</table><h4 id="function-app">Function app</h4>
<p>A function app provides an execution context in Azure in which your functions run. As such, it is the unit of deployment and management for your functions. A function app is comprised of one or more individual functions that are managed, deployed, and scaled together.</p>
<h4 id="folder-structure">Folder structure</h4>
<p>The code for all the functions in a specific function app is located in a root project folder that contains a host configuration file. The host.json file contains runtime-specific configurations and is in the root folder of the function app. A <em>bin</em> folder contains packages and other library files that the function app requires. Specific folder structures required by the function app depend on language.</p>
<h4 id="local-development-environments">Local development environments</h4>
<p>Your local functions can connect to live Azure services, and you can debug them on your local computer using the full Functions runtime.</p>
<p>Do not mix local development with portal development in the same function app. When you create and publish functions from a local project, you should not try to maintain or modify project code in the portal.</p>
<h3 id="create-triggers-and-bindings">Create triggers and bindings</h3>
<p>Triggers are what cause a function to run. A trigger defines how a function is invoked and a function must have exactly one trigger. Triggers have associated data, which is often provided as the payload of the function.</p>
<p>Binding to a function is a way of declaratively connecting another resource to the function; bindings may be connected as  <em>input bindings</em>,  <em>output bindings</em>, or both. Data from bindings is provided to the function as parameters. Bindings are optional and a function might have one or multiple input and/or output bindings.</p>
<h4 id="trigger-and-binding-definitions">Trigger and binding definitions</h4>

<table>
<thead>
<tr>
<th>Language</th>
<th>Triggers and bindings are configured by…</th>
</tr>
</thead>
<tbody>
<tr>
<td>C# class library</td>
<td>decorating methods and parameters with C# attributes</td>
</tr>
<tr>
<td>Java</td>
<td>decorating methods and parameters with Java annotations</td>
</tr>
<tr>
<td>JavaScript/PowerShell/Python/TypeScript</td>
<td>updating  <em>function.json</em>  schema</td>
</tr>
</tbody>
</table><h4 id="binding-direction">Binding direction</h4>
<ul>
<li>For triggers, the direction is always  <code>in</code></li>
<li>Input and output bindings use  <code>in</code>  and  <code>out</code></li>
<li>Some bindings support a special direction  <code>inout</code>. If you use  <code>inout</code>, only the  <strong>Advanced editor</strong>  is available via the  <strong>Integrate</strong>  tab in the portal.</li>
</ul>
<h4 id="azure-functions-trigger-and-binding-example">Azure Functions trigger and binding example</h4>
<p>Suppose you want to write a new row to Azure Table storage whenever a new message appears in Azure Queue storage. This scenario can be implemented using an Azure Queue storage trigger and an Azure Table storage output binding.</p>
<p>Here’s a  <em>function.json</em>  file for this scenario.</p>
<p>JSON</p>
<pre><code>{
  "bindings": [
    {
      "type": "queueTrigger",
      "direction": "in",
      "name": "order",
      "queueName": "myqueue-items",
      "connection": "MY_STORAGE_ACCT_APP_SETTING"
    },
    {
      "type": "table",
      "direction": "out",
      "name": "$return",
      "tableName": "outTable",
      "connection": "MY_TABLE_STORAGE_ACCT_APP_SETTING"
    }
  ]
}
</code></pre>
<h5 id="c-script-example">C# script example</h5>
<pre><code>#r "Newtonsoft.Json"

using Microsoft.Extensions.Logging;
using Newtonsoft.Json.Linq;

// From an incoming queue message that is a JSON object, add fields and write to Table storage
// The method return value creates a new row in Table Storage
public static Person Run(JObject order, ILogger log)
{
    return new Person() { 
            PartitionKey = "Orders", 
            RowKey = Guid.NewGuid().ToString(),  
            Name = order["Name"].ToString(),
            MobileNumber = order["MobileNumber"].ToString() };  
}

public class Person
{
    public string PartitionKey { get; set; }
    public string RowKey { get; set; }
    public string Name { get; set; }
    public string MobileNumber { get; set; }
}

</code></pre>
<h3 id="javascript-example">JavaScript example</h3>
<pre><code>// From an incoming queue message that is a JSON object, add fields and write to Table Storage
module.exports = async function (context, order) {
    order.PartitionKey = "Orders";
    order.RowKey = generateRandomId(); 

    context.bindings.order = order;
};

function generateRandomId() {
    return Math.random().toString(36).substring(2, 15) +
        Math.random().toString(36).substring(2, 15);
}

</code></pre>
<h3 id="class-library-example">Class library example</h3>
<pre><code>public static class QueueTriggerTableOutput
{
    [FunctionName("QueueTriggerTableOutput")]
    [return: Table("outTable", Connection = "MY_TABLE_STORAGE_ACCT_APP_SETTING")]
    public static Person Run(
        [QueueTrigger("myqueue-items", Connection = "MY_STORAGE_ACCT_APP_SETTING")]JObject order,
        ILogger log)
    {
        return new Person() {
                PartitionKey = "Orders",
                RowKey = Guid.NewGuid().ToString(),
                Name = order["Name"].ToString(),
                MobileNumber = order["MobileNumber"].ToString() };
    }
}

public class Person
{
    public string PartitionKey { get; set; }
    public string RowKey { get; set; }
    public string Name { get; set; }
    public string MobileNumber { get; set; }
</code></pre>
<h3 id="connect-functions-to-azure-services">Connect functions to Azure services</h3>
<p>Your function project does not directly accept the connection details. It references connection information by name from its configuration provider, allowing them to be changed across environments.</p>
<p>The default configuration provider uses environment variables. These might be set by Application Settings when running in the Azure Functions service, or from the local settings filewhen developing locally.</p>
<h4 id="connection-values">Connection values</h4>
<p>When the connection name resolves to a single exact value, the runtime identifies the value as a <em>connection string</em>, which typically includes a secret.</p>
<p>However, a connection name can also refer to a collection of multiple configuration items. Environment variables can be treated as a collection by using a shared prefix that ends in double underscores <code>__</code>. The group can then be referenced by setting the connection name to this prefix.</p>
<p>For example, the <code>connection</code> property for a Azure Blob trigger definition might be <code>Storage1</code>. As long as there is no single string value configured with <code>Storage1</code> as its name, <code>Storage1__serviceUri</code> would be used for the <code>serviceUri</code> property of the connection.</p>
<h4 id="configure-an-identity-based-connection">Configure an identity-based connection</h4>
<p>Some connections in Azure Functions are configured to use an identity instead of a secret. Support depends on the extension using the connection. In some cases, a connection string may still be required in Functions even though the service to which you are connecting supports identity-based connections.</p>
<p>When hosted in the Azure Functions service, identity-based connections use a managed identity.</p>
<h4 id="grant-permission-to-the-identity">Grant permission to the identity</h4>
<p>Whatever identity is being used must have permissions to perform the intended actions. This is typically done by assigning a role in Azure RBAC or specifying the identity in an access policy, depending on the service to which you are connecting.</p>
<h2 id="implement-durable-functions">Implement Durable Functions</h2>
<h3 id="explore-durable-functions-app-patterns">Explore Durable Functions app patterns</h3>
<p><em>Durable Functions</em>  is an extension of Azure Functions that lets you write stateful functions in a serverless compute environment. It lets you define stateful workflows by writing <em>orchestrator functions</em> and stateful entities by writing <em>entity functions</em>.</p>
<h4 id="supported-languages">Supported languages</h4>
<ul>
<li>C#</li>
<li>JavaScript</li>
<li>Python</li>
<li>F#</li>
<li>PowerShell</li>
</ul>
<h4 id="application-patterns">Application patterns</h4>
<p>The primary use case for Durable Functions is simplifying complex, stateful coordination requirements in serverless applications.</p>
<h5 id="function-chaining">Function chaining</h5>
<p>In the function chaining pattern, a sequence of functions executes in a specific order. In this pattern, the output of one function is applied to the input of another function.</p>
<p><img src="https://docs.microsoft.com/en-us/learn/wwl-azure/implement-durable-functions/media/function-chaining.png" alt="Four functions executing in a specific order"></p>
<pre><code>[FunctionName("Chaining")]
public static async Task&lt;object&gt; Run(
    [OrchestrationTrigger] IDurableOrchestrationContext context)
{
    try
    {
        var x = await context.CallActivityAsync&lt;object&gt;("F1", null);
        var y = await context.CallActivityAsync&lt;object&gt;("F2", x);
        var z = await context.CallActivityAsync&lt;object&gt;("F3", y);
        return  await context.CallActivityAsync&lt;object&gt;("F4", z);
    }
    catch (Exception)
    {
        // Error handling or compensation goes here.
    }
}
</code></pre>
<h5 id="fan-outfan-in">Fan out/fan in</h5>
<p>In the fan out/fan in pattern, you execute multiple functions in parallel and then wait for all functions to finish. Often, some aggregation work is done on the results that are returned from the functions.</p>
<p><img src="https://docs.microsoft.com/en-us/learn/wwl-azure/implement-durable-functions/media/fan-out-fan-in.png" alt="A single function that fans out to three functions that execute in parallel and, after operation of all three ends, sending the result to another function."><br>
With normal functions, you can fan out by having the function send multiple messages to a queue. To fan in you write code to track when the queue-triggered functions end, and then store function outputs.</p>
<pre><code>[FunctionName("FanOutFanIn")]
public static async Task Run(
    [OrchestrationTrigger] IDurableOrchestrationContext context)
{
    var parallelTasks = new List&lt;Task&lt;int&gt;&gt;();

    // Get a list of N work items to process in parallel.
    object[] workBatch = await context.CallActivityAsync&lt;object[]&gt;("F1", null);
    for (int i = 0; i &lt; workBatch.Length; i++)
    {
        Task&lt;int&gt; task = context.CallActivityAsync&lt;int&gt;("F2", workBatch[i]);
        parallelTasks.Add(task);
    }

    await Task.WhenAll(parallelTasks);

    // Aggregate all N outputs and send the result to F3.
    int sum = parallelTasks.Sum(t =&gt; t.Result);
    await context.CallActivityAsync("F3", sum);
}
</code></pre>
<h5 id="async-http-apis">Async HTTP APIs</h5>
<p>The async HTTP API pattern addresses the problem of coordinating the state of long-running operations with external clients. A common way to implement this pattern is by having an HTTP endpoint trigger the long-running action. Then, redirect the client to a status endpoint that the client polls to learn when the operation is finished.<br>
<img src="https://docs.microsoft.com/en-us/learn/wwl-azure/implement-durable-functions/media/async-http-api.png" alt="An HTTP endpoint triggers the long running action with the client polling for completion status."></p>
<p>Durable Functions provides <strong>built-in support</strong> for this pattern, simplifying or even removing the code you need to write to interact with long-running function executions.</p>
<p>The following example shows REST commands that start an orchestrator and query its status. For clarity, some protocol details are omitted from the example.</p>
<pre><code>&gt; curl -X POST https://myfunc.azurewebsites.net/orchestrators/DoWork -H "Content-Length: 0" -i
HTTP/1.1 202 Accepted
Content-Type: application/json
Location: https://myfunc.azurewebsites.net/runtime/webhooks/durabletask/b79baf67f717453ca9e86c5da21e03ec

{"id":"b79baf67f717453ca9e86c5da21e03ec", ...}

&gt; curl https://myfunc.azurewebsites.net/runtime/webhooks/durabletask/b79baf67f717453ca9e86c5da21e03ec -i
HTTP/1.1 202 Accepted
Content-Type: application/json
Location: https://myfunc.azurewebsites.net/runtime/webhooks/durabletask/b79baf67f717453ca9e86c5da21e03ec

{"runtimeStatus":"Running","lastUpdatedTime":"2019-03-16T21:20:47Z", ...}

&gt; curl https://myfunc.azurewebsites.net/runtime/webhooks/durabletask/b79baf67f717453ca9e86c5da21e03ec -i
HTTP/1.1 200 OK
Content-Length: 175
Content-Type: application/json

{"runtimeStatus":"Completed","lastUpdatedTime":"2019-03-16T21:20:57Z", ...}
</code></pre>
<p>The Durable Functions extension exposes built-in HTTP APIs that manage long-running orchestrations. You can alternatively implement this pattern yourself by using your own function triggers (such as HTTP, a queue, or Azure Event Hubs) and the orchestration client binding.</p>
<pre><code>public static class HttpStart
{
    [FunctionName("HttpStart")]
    public static async Task&lt;HttpResponseMessage&gt; Run(
        [HttpTrigger(AuthorizationLevel.Function, methods: "post", Route = "orchestrators/{functionName}")] HttpRequestMessage req,
        [DurableClient] IDurableClient starter,
        string functionName,
        ILogger log)
    {
        // Function input comes from the request content.
        object eventData = await req.Content.ReadAsAsync&lt;object&gt;();
        string instanceId = await starter.StartNewAsync(functionName, eventData);

        log.LogInformation($"Started orchestration with ID = '{instanceId}'.");

        return starter.CreateCheckStatusResponse(req, instanceId);
    }
}
</code></pre>
<h5 id="monitor">Monitor</h5>
<p>The monitor pattern refers to a flexible, recurring process in a workflow. An example is polling until specific conditions are met.<br>
<img src="https://docs.microsoft.com/en-us/learn/wwl-azure/implement-durable-functions/media/monitor.png" alt="A function running until a specific condition is met."></p>
<pre><code>[FunctionName("MonitorJobStatus")]
public static async Task Run(
    [OrchestrationTrigger] IDurableOrchestrationContext context)
{
    int jobId = context.GetInput&lt;int&gt;();
    int pollingInterval = GetPollingInterval();
    DateTime expiryTime = GetExpiryTime();

    while (context.CurrentUtcDateTime &lt; expiryTime)
    {
        var jobStatus = await context.CallActivityAsync&lt;string&gt;("GetJobStatus", jobId);
        if (jobStatus == "Completed")
        {
            // Perform an action when a condition is met.
            await context.CallActivityAsync("SendAlert", machineId);
            break;
        }

        // Orchestration sleeps until this time.
        var nextCheck = context.CurrentUtcDateTime.AddSeconds(pollingInterval);
        await context.CreateTimer(nextCheck, CancellationToken.None);
    }

    // Perform more work here, or let the orchestration end.
}
</code></pre>
<h5 id="human-interaction">Human interaction</h5>
<p>Many automated processes involve some kind of human interaction. Involving humans in an automated process is tricky because people aren’t as highly available and as responsive as cloud services. An automated process might allow for this interaction by using timeouts and compensation logic.</p>
<p><img src="https://docs.microsoft.com/en-us/learn/wwl-azure/implement-durable-functions/media/human-interaction-pattern.png" alt="A process awaiting human interaction before continuing."></p>
<p>You can implement the pattern in this example by using an orchestrator function. The orchestrator uses a durable timer to request approval. The orchestrator escalates if timeout occurs. The orchestrator waits for an external event, such as a notification that’s generated by a human interaction.</p>
<pre><code>[FunctionName("ApprovalWorkflow")]
public static async Task Run(
    [OrchestrationTrigger] IDurableOrchestrationContext context)
{
    await context.CallActivityAsync("RequestApproval", null);
    using (var timeoutCts = new CancellationTokenSource())
    {
        DateTime dueTime = context.CurrentUtcDateTime.AddHours(72);
        Task durableTimeout = context.CreateTimer(dueTime, timeoutCts.Token);

        Task&lt;bool&gt; approvalEvent = context.WaitForExternalEvent&lt;bool&gt;("ApprovalEvent");
        if (approvalEvent == await Task.WhenAny(approvalEvent, durableTimeout))
        {
            timeoutCts.Cancel();
            await context.CallActivityAsync("ProcessApproval", approvalEvent.Result);
        }
        else
        {
            await context.CallActivityAsync("Escalate", null);
        }
    }
}
</code></pre>
<h3 id="discover-the-four-function-types">Discover the four function types</h3>
<h4 id="orchestrator-functions">Orchestrator functions</h4>
<p>Orchestrator functions describe how actions are executed and the order in which actions are executed.</p>
<p>Orchestrator functions are written using ordinary code, but there are strict requirements on how to write the code. Specifically, orchestrator function code must be deterministic.</p>
<p>An orchestration can have many different types of actions, including activity functions, sub-orchestrations, waiting for external events, HTTP, and timers. Orchestrator functions can also interact with entity functions.</p>
<h4 id="activity-functions">Activity functions</h4>
<p>Activity functions are the basic unit of work in a durable function orchestration. For example, you might create an orchestrator function to process an order. The tasks involve checking the inventory, charging the customer, and creating a shipment. Each task would be a separate activity function.</p>
<p>Unlike orchestrator functions, activity functions aren’t restricted in the type of work you can do in them. Activity functions are frequently used to make network calls or run CPU intensive operations.</p>
<p>An activity trigger is used to define an activity function.</p>
<h4 id="entity-functions">Entity functions</h4>
<p>Entity functions define operations for reading and updating small pieces of state. We often refer to these stateful entities as durable entities. Like orchestrator functions, entity functions are functions with a special trigger type,  <em>entity trigger</em>. They can also be invoked from client functions or from orchestrator functions. Unlike orchestrator functions, entity functions do not have any specific code constraints. Entity functions also manage state explicitly rather than implicitly representing state via control flow. Some things to note:</p>
<ul>
<li>Entities are accessed via a unique identifier, the  <em>entity ID</em>. An entity ID is simply a pair of strings that uniquely identifies an entity instance.</li>
<li>Operations on entities require that you specify the  <strong>Entity ID</strong>  of the target entity, and the  <strong>Operation name</strong>, which is a string that specifies the operation to perform.</li>
</ul>
<h4 id="client-functions">Client functions</h4>
<p>What makes a function a client function is its use of the <em>durable client output binding</em>. Orchestrator and entity functions are triggered by their bindings and both of these triggers work by reacting to messages that are enqueued in a task hub. The primary way to deliver these messages is by using an orchestrator client binding, or an entity client binding, from within a <em>client function</em>.</p>
<p>If you want to test an orchestrator or entity function in the Azure portal, you must instead run a client function that starts an orchestrator or entity function as part of its implementation.</p>
<h3 id="explore-task-hubs">Explore task hubs</h3>
<p>A task hub in Durable Functions is a logical container for durable storage resources that are used for orchestrations and entities. Orchestrator, activity, and entity functions can only directly interact with each other when they belong to the same task hub.</p>
<p>If multiple function apps share a storage account, each function app must be configured with a separate task hub name.</p>
<h4 id="azure-storage-resources">Azure Storage resources</h4>
<p>A task hub in Azure Storage consists of the following resources:</p>
<ul>
<li>One or more control queues.</li>
<li>One work-item queue.</li>
<li>One history table.</li>
<li>One instances table.</li>
<li>One storage container containing one or more lease blobs.</li>
<li>A storage container containing large message payloads, if applicable.</li>
</ul>
<p>All of these resources are created automatically in the configured Azure Storage account when orchestrator, entity, or activity functions run or are scheduled to run.</p>
<h4 id="task-hub-names">Task hub names</h4>
<p>Task hubs in Azure Storage are identified by a name that conforms to these rules:</p>
<ul>
<li>Contains only alphanumeric characters</li>
<li>Starts with a letter</li>
<li>Has a minimum length of 3 characters, maximum length of 45 characters</li>
</ul>
<p>The task hub name is declared in the  <em>host.json</em>  file, as shown in the following example:</p>
<pre><code>{
  "version": "2.0",
  "extensions": {
    "durableTask": {
      "hubName": "MyTaskHub"
    }
  }
}
</code></pre>
<h3 id="explore-durable-orchestrations">Explore durable orchestrations</h3>
<p>You can use an  <em>orchestrator function</em>  to orchestrate the execution of other Durable functions within a function app. Orchestrator functions have the following characteristics:</p>
<ul>
<li>Orchestrator functions define function workflows using procedural code. No declarative schemas or designers are needed.</li>
<li>Orchestrator functions can call other durable functions synchronously and asynchronously. Output from called functions can be reliably saved to local variables.</li>
<li>Orchestrator functions are durable and reliable. Execution progress is automatically checkpointed when the function “awaits” or “yields”. Local state is never lost when the process recycles or the VM reboots.</li>
<li>Orchestrator functions can be long-running. The total lifespan of an  <em>orchestration instance</em>  can be seconds, days, months, or never-ending.</li>
</ul>
<h4 id="orchestration-identity">Orchestration identity</h4>
<p>Each  <em>instance</em>  of an orchestration has an instance identifier (also known as an  <em>instance ID</em>). By default, each instance ID is an autogenerated GUID. An orchestration’s instance ID is a required parameter for most instance management operations. They are also important for diagnostics.</p>
<h4 id="reliability">Reliability</h4>
<p>Orchestrator functions reliably maintain their execution state by using the event sourcing design pattern. Instead of directly storing the current state of an orchestration, the Durable Task Framework uses an append-only store to record the full series of actions the function orchestration takes.</p>
<p>Durable Functions uses event sourcing transparently. Behind the scenes, the  <code>await</code>  (C#) or  <code>yield</code>  (JavaScript) operator in an orchestrator function yields control of the orchestrator thread back to the Durable Task Framework dispatcher. The dispatcher then commits any new actions that the orchestrator function scheduled to storage. The transparent commit action appends to the execution history of the orchestration instance. The history is stored in a storage table. The commit action then adds messages to a queue to schedule the actual work. At this point, the orchestrator function can be unloaded from memory.</p>
<p>When an orchestration function is given more work to do, the orchestrator wakes up and re-executes the entire function from the start to rebuild the local state. During the replay, if the code tries to call a function (or do any other async work), the Durable Task Framework consults the execution history of the current orchestration. If it finds that the activity function has already executed and yielded a result, it replays that function’s result and the orchestrator code continues to run.</p>

<table>
<thead>
<tr>
<th>Pattern/Feature</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Sub-orchestrations</td>
<td>Orchestrator functions can call activity functions, but also other orchestrator functions.</td>
</tr>
<tr>
<td>Durable timers</td>
<td>Orchestrations can schedule durable timers to implement delays or to set up timeout handling on async actions.</td>
</tr>
<tr>
<td>External events</td>
<td>Orchestrator functions can wait for external events to update an orchestration instance. This Durable Functions feature often is useful for handling a human interaction or other external callbacks.</td>
</tr>
<tr>
<td>Error handling</td>
<td>Orchestrator functions can use the error-handling features of the programming language.</td>
</tr>
<tr>
<td>Critical sections</td>
<td>To mitigate race conditions when interacting with external systems, orchestrator functions can define  <em>critical sections</em>  using a  <code>LockAsync</code>  method in .NET.</td>
</tr>
<tr>
<td>Calling HTTP endpoints</td>
<td>Orchestrator functions aren’t permitted to do I/O. The typical workaround for this limitation is to wrap any code that needs to do I/O in an activity function.</td>
</tr>
<tr>
<td>Passing multiple parameters</td>
<td>The recommendation is to pass in an array of objects or to use ValueTuples objects in .NET.</td>
</tr>
</tbody>
</table><h3 id="control-timing-in-durable-functions">Control timing in Durable Functions</h3>
<p>Durable Functions provides <em>durable timers</em> for use in orchestrator functions to implement delays or to set up timeouts on async actions. Durable timers should be used in orchestrator functions instead of <code>Thread.Sleep</code> and <code>Task.Delay</code> (C#), or <code>setTimeout()</code> and <code>setInterval()</code> (JavaScript), or <code>time.sleep()</code> (Python).</p>
<p>You create a durable timer by calling the <code>CreateTimer</code> (.NET) method or the <code>createTimer</code> (JavaScript) method of the orchestration trigger binding. The method returns a task that completes on a specified date and time.</p>
<h4 id="usage-for-delay">Usage for delay</h4>
<pre><code>[FunctionName("BillingIssuer")]
public static async Task Run(
    [OrchestrationTrigger] IDurableOrchestrationContext context)
{
    for (int i = 0; i &lt; 10; i++)
    {
        DateTime deadline = context.CurrentUtcDateTime.Add(TimeSpan.FromDays(i + 1));
        await context.CreateTimer(deadline, CancellationToken.None);
        await context.CallActivityAsync("SendBillingEvent");
    }
}
</code></pre>
<h4 id="usage-for-timeout">Usage for timeout</h4>
<pre><code>[FunctionName("TryGetQuote")]
public static async Task&lt;bool&gt; Run(
    [OrchestrationTrigger] IDurableOrchestrationContext context)
{
    TimeSpan timeout = TimeSpan.FromSeconds(30);
    DateTime deadline = context.CurrentUtcDateTime.Add(timeout);

    using (var cts = new CancellationTokenSource())
    {
        Task activityTask = context.CallActivityAsync("GetQuote");
        Task timeoutTask = context.CreateTimer(deadline, cts.Token);

        Task winner = await Task.WhenAny(activityTask, timeoutTask);
        if (winner == activityTask)
        {
            // success case
            cts.Cancel();
            return true;
        }
        else
        {
            // timeout case
            return false;
        }
    }
}
</code></pre>
<h3 id="send-and-wait-for-events">Send and wait for events</h3>
<h4 id="wait-for-events">Wait for events</h4>
<p>The  <code>WaitForExternalEvent</code>  (.NET),  <code>waitForExternalEvent</code>  (JavaScript), and  <code>wait_for_external_event</code>  (Python) methods of the orchestration trigger binding allows an orchestrator function to asynchronously wait and listen for an external event. The listening orchestrator function declares the  <em>name</em>  of the event and the  <em>shape of the data</em>  it expects to receive.</p>
<p>[FunctionName(“BudgetApproval”)] public static async Task Run( [OrchestrationTrigger] IDurableOrchestrationContext context) { bool approved = await context.WaitForExternalEvent(“Approval”); if (approved) { // approval granted - do the approved action } else { // approval denied - send a notification } }</p>
<h4 id="send-events">Send events</h4>
<p>The <code>RaiseEventAsync</code> (.NET) or <code>raiseEvent</code> (JavaScript) method of the orchestration client binding sends the events that <code>WaitForExternalEvent</code> (.NET) or <code>waitForExternalEvent</code> (JavaScript) waits for. The <code>RaiseEventAsync</code> method takes <em>eventName</em> and <em>eventData</em> as parameters. The event data must be JSON-serializable.</p>
<pre><code>[FunctionName("ApprovalQueueProcessor")]
public static async Task Run(
    [QueueTrigger("approval-queue")] string instanceId,
    [DurableClient] IDurableOrchestrationClient client)
{
    await client.RaiseEventAsync(instanceId, "Approval", true);
}
</code></pre>
<h1 id="az-204-develop-solutions-that-use-blob-storage">AZ-204: Develop solutions that use Blob storage</h1>
<h2 id="explore-azure-blob-storage">Explore Azure Blob storage</h2>
<h3 id="explore-azure-blob-storage-1">Explore Azure Blob storage</h3>
<p>Azure Blob storage is Microsoft’s object storage solution for the cloud. Blob storage is optimized for storing massive amounts of unstructured data.</p>
<p>Blob storage is designed for:</p>
<ul>
<li>Serving images or documents directly to a browser.</li>
<li>Storing files for distributed access.</li>
<li>Streaming video and audio.</li>
<li>Writing to log files.</li>
<li>Storing data for backup and restore, disaster recovery, and archiving.</li>
<li>Storing data for analysis by an on-premises or Azure-hosted service.</li>
</ul>
<p>Objects in Blob storage are accessible via the Azure Storage REST API, Azure PowerShell, Azure CLI, or an Azure Storage client library.</p>
<p>An Azure Storage account is the top-level container for all of your Azure Blob storage.</p>
<h4 id="types-of-storage-accounts">Types of storage accounts</h4>

<table>
<thead>
<tr>
<th>Storage account type</th>
<th>Supported storage services</th>
<th>Usage</th>
</tr>
</thead>
<tbody>
<tr>
<td>Standard general-purpose v2</td>
<td>Blob, Queue, and Table storage, Azure Files</td>
<td>Recommended for most scenarios using Azure Storage.</td>
</tr>
<tr>
<td>Premium block blobs</td>
<td>Blob storage</td>
<td>Premium storage account type for block blobs and append blobs. Recommended for scenarios with high transactions rates, or scenarios that use smaller objects or require consistently low storage latency.</td>
</tr>
<tr>
<td>Premium page blobs</td>
<td>Page blobs only</td>
<td>Premium storage account type for page blobs only.</td>
</tr>
<tr>
<td>Premium file shares</td>
<td>Azure Files</td>
<td>Premium storage account type for file shares only. If you want support for NFS file shares in Azure Files, use the premium file shares account type.</td>
</tr>
</tbody>
</table><h4 id="access-tiers-for-block-blob-data">Access tiers for block blob data</h4>
<p>Each access tier in Azure Storage is optimized for a particular pattern of data usage.</p>
<ul>
<li>The  <strong>Hot</strong>  access tier, which is optimized for frequent access of objects in the storage account. The Hot tier has the highest storage costs, but the lowest access costs. New storage accounts are created in the hot tier by default.</li>
<li>The  <strong>Cool</strong>  access tier, which is optimized for storing large amounts of data that is infrequently accessed and stored for at least 30 days. The Cool tier has lower storage costs and higher access costs compared to the Hot tier.</li>
<li>The  <strong>Archive</strong>  tier, which is available only for individual block blobs. The archive tier is optimized for data that can tolerate several hours of retrieval latency and will remain in the Archive tier for at least 180 days. The archive tier is the most cost-effective option for storing data, but accessing that data is more expensive than accessing data in the hot or cool tiers.</li>
</ul>
<h3 id="discover-azure-blob-storage-resource-types">Discover Azure Blob storage resource types</h3>
<p>Blob storage offers three types of resources:</p>
<ul>
<li>The  <strong>storage account</strong>.</li>
<li>A  <strong>container</strong>  in the storage account</li>
<li>A  <strong>blob</strong>  in a container</li>
</ul>
<h4 id="storage-accounts">Storage accounts</h4>
<p>A storage account provides a unique namespace in Azure for your data. eg. for default enpoint: <a href="http://mystorageaccount.blob.core.windows.net">http://mystorageaccount.blob.core.windows.net</a></p>
<h4 id="containers">Containers</h4>
<p>A container organizes a set of blobs, similar to a directory in a file system. A storage account can include an unlimited number of containers, and a container can store an unlimited number of blobs.</p>
<h4 id="blobs">Blobs</h4>
<ul>
<li><strong>Block blobs</strong>  store text and binary data, up to about 190.7 TB.</li>
<li><strong>Append blobs</strong>  are made up of blocks like block blobs, but are optimized for append operations. Append blobs are ideal for scenarios such as logging data from virtual machines.</li>
<li><strong>Page blobs</strong>  store random access files up to 8 TB in size. Page blobs store virtual hard drive (VHD) files and serve as disks for Azure virtual machines.</li>
</ul>
<h3 id="explore-azure-storage-security-features">Explore Azure Storage security features</h3>
<ul>
<li>All data (including metadata) written to Azure Storage is automatically encrypted using Storage Service Encryption (SSE).</li>
<li>Azure Active Directory (Azure AD) and Role-Based Access Control (RBAC) are supported for Azure Storage for both resource management operations and data operation</li>
<li>Data can be secured in transit between an application and Azure by using Client-Side Encryption, HTTPS, or SMB 3.0.</li>
<li>OS and data disks used by Azure virtual machines can be encrypted using Azure Disk Encryption.</li>
<li>Delegated access to the data objects in Azure Storage can be granted using a shared access signature.</li>
</ul>
<h4 id="azure-storage-encryption-for-data-at-rest">Azure Storage encryption for data at rest</h4>
<p>Azure Storage automatically encrypts your data when persisting it to the cloud.Data in Azure Storage is encrypted and decrypted transparently using 256-bit AES encryption, one of the strongest block ciphers available, and is FIPS 140-2 compliant.</p>
<h5 id="encryption-key-management">Encryption key management</h5>
<p>You can rely on Microsoft-managed keys for the encryption of your storage account, or you can manage encryption with your own keys. If you choose to manage encryption with your own keys, you have two options:</p>
<ul>
<li>You can specify a  <em>customer-managed</em>  key to use for encrypting and decrypting all data in the storage account.</li>
<li>You can specify a  <em>customer-provided</em>  key on Blob storage operations.</li>
</ul>
<p>| | Microsoft-managed keys | Customer-managed keys | Customer-provided keys |<br>
| Encryption/decryption operations | Azure | Azure | Azure |<br>
| Azure Storage services supported | All | Blob storage, Azure Files | Blob storage |<br>
| Key storage | Microsoft key store | Azure Key Vault | Azure Key Vault or any other key store |<br>
| Key rotation responsibility | Microsoft | Customer | Customer |<br>
| Key usage | Microsoft | Azure portal, Storage Resource Provider REST API, Azure Storage management libraries, PowerShell, CLI | Azure Storage REST API (Blob storage), Azure Storage client libraries |<br>
| Key access | Microsoft only | Microsoft, Customer | Customer only |</p>
<h3 id="evaluate-azure-storage-redundancy-options">Evaluate Azure Storage redundancy options</h3>
<p>Azure Storage always stores multiple copies of your data so that it is protected from planned and unplanned events.</p>
<p>When deciding which redundancy option is best for your scenario, consider the tradeoffs between lower costs and higher availability.</p>
<h4 id="redundancy-in-the-primary-region">Redundancy in the primary region</h4>
<p>Data in an Azure Storage account is always replicated three times in the primary region.</p>
<ul>
<li><strong>Locally redundant storage (LRS)</strong>: Copies your data synchronously three times within a single physical location in the primary region.</li>
<li><strong>Zone-redundant storage (ZRS)</strong>: Copies your data synchronously across three Azure availability zones in the primary region.</li>
</ul>
<h4 id="redundancy-in-a-secondary-region">Redundancy in a secondary region</h4>
<p>For applications requiring high durability, you can choose to additionally copy the data in your storage account to a secondary region that is hundreds of miles away from the primary region.</p>
<p>When you create a storage account, you select the primary region for the account. The paired secondary region is determined based on the primary region, and can’t be changed.</p>
<ul>
<li><strong>Geo-redundant storage (GRS)</strong>  copies your data synchronously three times within a single physical location in the primary region using LRS. It then copies your data asynchronously to a single physical location in the secondary region. Within the secondary region, your data is copied synchronously three times using LRS.</li>
<li><strong>Geo-zone-redundant storage (GZRS)</strong>  copies your data synchronously across three Azure availability zones in the primary region using ZRS. It then copies your data asynchronously to a single physical location in the secondary region. Within the secondary region, your data is copied synchronously three times using LRS.</li>
</ul>
<h2 id="manage-the-azure-blob-storage-lifecycle">Manage the Azure Blob storage lifecycle</h2>
<p>Data sets have unique lifecycles. Early in the lifecycle, people access some data often. But the need for access drops drastically as the data ages.</p>
<h4 id="access-tiers">Access tiers</h4>
<ul>
<li><strong>Hot</strong>  - Optimized for storing data that is accessed frequently.</li>
<li><strong>Cool</strong>  - Optimized for storing data that is infrequently accessed and stored for at least 30 days.</li>
<li><strong>Archive</strong>  - Optimized for storing data that is rarely accessed and stored for at least 180 days with flexible latency requirements, on the order of hours.</li>
</ul>
<p>The hot and cool tiers support all redundancy options. The archive tier supports only LRS, GRS, and RA-GRS.</p>
<h4 id="manage-the-data-lifecycle">Manage the data lifecycle</h4>
<p>Azure Blob storage lifecycle management offers a rich, rule-based policy for General Purpose v2 and Blob storage accounts. Use the policy to transition your data to the appropriate access tiers or expire at the end of the data’s lifecycle.</p>
<h3 id="discover-blob-storage-lifecycle-policies">Discover Blob storage lifecycle policies</h3>
<p>A lifecycle management policy is a collection of rules in a JSON document. Each rule definition within a policy includes a filter set and an action set. The filter set limits rule actions to a certain set of objects within a container or objects names. The action set applies the tier or delete actions to the filtered set of objects.:</p>
<pre><code>{
  "rules": [
    {
      "name": "rule1",
      "enabled": true,
      "type": "Lifecycle",
      "definition": {...}
    },
    {
      "name": "rule2",
      "type": "Lifecycle",
      "definition": {...}
    }
  ]
}
</code></pre>
<p>A policy is a collection of rules:</p>

<table>
<thead>
<tr>
<th>Parameter name</th>
<th>Parameter type</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>rules</code></td>
<td>An array of rule objects</td>
<td>At least one rule is required in a policy. You can define up to 100 rules in a policy.</td>
</tr>
</tbody>
</table><p>Each rule within the policy has several parameters:</p>

<table>
<thead>
<tr>
<th>Parameter name</th>
<th>Parameter type</th>
<th>Notes</th>
<th>Required</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>name</code></td>
<td>String</td>
<td>It must be unique within a policy.</td>
<td>True</td>
</tr>
<tr>
<td><code>enabled</code></td>
<td>Boolean</td>
<td>An optional boolean to allow a rule to be temporary disabled.</td>
<td>False</td>
</tr>
<tr>
<td><code>type</code></td>
<td>An enum value</td>
<td>The current valid type is Lifecycle.</td>
<td>True</td>
</tr>
<tr>
<td><code>definition</code></td>
<td>An object that defines the lifecycle rule</td>
<td>Each definition is made up of a filter set and an action set.</td>
<td>True</td>
</tr>
</tbody>
</table><h4 id="rules">Rules</h4>
<p>Each rule definition includes a filter set and an action set. The filter set limits rule actions to a certain set of objects within a container or objects names.</p>
<pre><code>{
  "rules": [
    {
      "name": "ruleFoo",
      "enabled": true,
      "type": "Lifecycle",
      "definition": {
        "filters": {
          "blobTypes": [ "blockBlob" ],
          "prefixMatch": [ "container1/foo" ]
        },
        "actions": {
          "baseBlob": {
            "tierToCool": { "daysAfterModificationGreaterThan": 30 },
            "tierToArchive": { "daysAfterModificationGreaterThan": 90 },
            "delete": { "daysAfterModificationGreaterThan": 2555 }
          },
          "snapshot": {
            "delete": { "daysAfterCreationGreaterThan": 90 }
          }
        }
      }
    }
  ]
}
</code></pre>
<h4 id="rule-filters">Rule filters</h4>

<table>
<thead>
<tr>
<th>Filter name</th>
<th>Filter type</th>
<th>Is Required</th>
</tr>
</thead>
<tbody>
<tr>
<td>blobTypes</td>
<td>An array of predefined enum values.</td>
<td>Yes</td>
</tr>
<tr>
<td>prefixMatch</td>
<td>An array of strings for prefixes to be match. Each rule can define up to 10 prefixes. A prefix string must start with a container name.</td>
<td>No</td>
</tr>
<tr>
<td>blobIndexMatch</td>
<td>An array of dictionary values consisting of blob index tag key and value conditions to be matched. Each rule can define up to 10 blob index tag condition.</td>
<td>No</td>
</tr>
</tbody>
</table><h4 id="rule-actions">Rule actions</h4>

<table>
<thead>
<tr>
<th>Action</th>
<th>Base Blob</th>
<th>Snapshot</th>
<th>Version</th>
</tr>
</thead>
<tbody>
<tr>
<td>tierToCool</td>
<td>Supported for blockBlob</td>
<td>Supported</td>
<td>Supported</td>
</tr>
<tr>
<td>enableAutoTierToHotFromCool</td>
<td>Supported for blockBlob</td>
<td>Not supported</td>
<td>Not supported</td>
</tr>
<tr>
<td>tierToArchive</td>
<td>Supported for blockBlob</td>
<td>Supported</td>
<td>Supported</td>
</tr>
<tr>
<td>delete</td>
<td>Supported for blockBlob and appendBlob</td>
<td>Supported</td>
<td>Supported</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>Action run condition</th>
<th>Condition value</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>daysAfterModificationGreaterThan</td>
<td>Integer value indicating the age in days</td>
<td>The condition for base blob actions</td>
</tr>
<tr>
<td>daysAfterCreationGreaterThan</td>
<td>Integer value indicating the age in days</td>
<td>The condition for blob snapshot actions</td>
</tr>
</tbody>
</table><h3 id="implement-blob-storage-lifecycle-policies">Implement Blob storage lifecycle policies</h3>
<h4 id="azure-portal">Azure portal</h4>
<p>There are two ways to add a policy through the Azure portal: Azure portal List view, and Azure portal Code view.</p>
<h4 id="azure-cli">Azure CLI</h4>
<p>To add a lifecycle management policy with Azure CLI, write the policy to a JSON file, then call the <code>az storage account management-policy create</code> command to create the policy.</p>
<h3 id="rehydrate-blob-data-from-the-archive-tier">Rehydrate blob data from the archive tier</h3>
<p>While a blob is in the archive access tier, it’s considered to be offline and can’t be read or modified. In order to read or modify data in an archived blob, you must first rehydrate the blob to an online tier, either the hot or cool tier.</p>
<ul>
<li><strong>Copy an archived blob to an online tier</strong></li>
<li><strong>Change a blob’s access tier to an online tier</strong></li>
</ul>
<h4 id="rehydration-priority">Rehydration priority</h4>
<p>When you rehydrate a blob, you can set the priority for the rehydration operation via the optional  <code>x-ms-rehydrate-priority</code>  header on a  <a href="https://docs.microsoft.com/en-us/rest/api/storageservices/set-blob-tier">Set Blob Tier</a>  or  <strong>Copy Blob/Copy Blob From URL</strong>  operation. Rehydration priority options include:</p>
<ul>
<li><strong>Standard priority</strong>: The rehydration request will be processed in the order it was received and may take up to 15 hours.</li>
<li><strong>High priority</strong>: The rehydration request will be prioritized over standard priority requests and may complete in under one hour for objects under 10 GB in size.</li>
</ul>
<h4 id="copy-an-archived-blob-to-an-online-tier">Copy an archived blob to an online tier</h4>
<p>You can use either the <strong>Copy Blob</strong> or <strong>Copy Blob from URL</strong> operation to copy the blob. You must copy the archived blob to a new blob with a different name or to a different container. Copying an archived blob to an online destination tier is supported within the same storage account only.</p>

<table>
<thead>
<tr>
<th></th>
<th>Hot tier source</th>
<th>Cool tier source</th>
<th>Archive tier source</th>
</tr>
</thead>
<tbody>
<tr>
<td>Hot tier destination</td>
<td>Supported</td>
<td>Supported</td>
<td>Supported within the same account. Requires blob rehydration.</td>
</tr>
<tr>
<td>Cool tier destination</td>
<td>Supported</td>
<td>Supported</td>
<td>Supported within the same account. Requires blob rehydration.</td>
</tr>
<tr>
<td>Archive tier destination</td>
<td>Supported</td>
<td>Supported</td>
<td>Unsupported</td>
</tr>
</tbody>
</table><h4 id="change-a-blobs-access-tier-to-an-online-tier">Change a blob’s access tier to an online tier</h4>
<p>The second option for rehydrating a blob from the archive tier to an online tier is to change the blob’s tier by calling <strong>Set Blob Tier</strong>. With this operation, you can change the tier of the archived blob to either hot or cool.</p>
<p>Changing a blob’s tier doesn’t affect its last modified time. If there is a lifecycle management policy in effect for the storage account, then rehydrating a blob with <strong>Set Blob Tier</strong> can result in a scenario where the lifecycle policy moves the blob back to the archive tier after rehydration because the last modified time is beyond the threshold set for the policy.</p>
<h2 id="work-with-azure-blob-storage">Work with Azure Blob storage</h2>
<h3 id="explore-azure-blob-storage-client-library">Explore Azure Blob storage client library</h3>
<p>The Azure Storage client libraries for .NET offer a convenient interface for making calls to Azure Storage. Below are the classes in the Azure.Storage.Blobs namespace and their purpose:</p>
<ul>
<li>The  <code>BlobClient</code>  allows you to manipulate Azure Storage blobs.</li>
<li>Provides the client configuration options for connecting to Azure Blob Storage.</li>
<li>The  <code>BlobContainerClient</code>  allows you to manipulate Azure Storage containers and their blobs.</li>
<li>The  <code>BlobServiceClient</code>  allows you to manipulate Azure Storage service resources and blob containers.</li>
<li>The  <code>BlobUriBuilder</code>  class provides a convenient way to modify the contents of a Uri instance to point to different Azure Storage resources like an account, container, or blob.</li>
</ul>
<h3 id="manage-container-properties-and-metadata-by-using-.net">Manage container properties and metadata by using .NET</h3>
<ul>
<li><strong>System properties</strong>: System properties exist on each Blob storage resource. Some of them can be read or set, while others are read-only. Under the covers, some system properties correspond to certain standard HTTP headers. The Azure Storage client library for .NET maintains these properties for you.</li>
<li><strong>User-defined metadata</strong>: User-defined metadata consists of one or more name-value pairs that you specify for a Blob storage resource. You can use metadata to store additional values with the resource. Metadata values are for your own purposes only, and do not affect how the resource behaves.</li>
</ul>
<p>Metadata names must be valid HTTP header names and valid C# identifiers, may contain only ASCII characters, and should be treated as case-insensitive.</p>
<h4 id="retrieve-container-properties">Retrieve container properties</h4>
<p>To retrieve container properties, call one of the following methods of the  <code>BlobContainerClient</code>  class:</p>
<ul>
<li><code>GetProperties</code></li>
<li><code>GetPropertiesAsync</code></li>
</ul>
<h4 id="set-and-retrieve-metadata">Set and retrieve metadata</h4>
<p>You can specify metadata as one or more name-value pairs on a blob or container resource. To set metadata, add name-value pairs to an  <code>IDictionary</code>  object, and then call one of the following methods of the  <code>BlobContainerClient</code>  class to write the values:</p>
<ul>
<li><code>SetMetadata</code></li>
<li><code>SetMetadataAsync</code></li>
</ul>
<p>The <code>GetProperties</code> and <code>GetPropertiesAsync</code> methods are used to retrieve metadata in addition to properties as shown earlier.</p>
<h3 id="set-and-retrieve-properties-and-metadata-for-blob-resources-by-using-rest">Set and retrieve properties and metadata for blob resources by using REST</h3>
<h4 id="metadata-header-format">Metadata header format</h4>
<p>Metadata headers are name/value pairs. The format for the header is:</p>
<pre><code>x-ms-meta-name:string-value  
</code></pre>
<p>Names are case-insensitive. Note that metadata names preserve the case with which they were created, but are case-insensitive when set or read.</p>
<h4 id="operations-on-metadata">Operations on metadata</h4>
<p>Note that metadata values can only be read or written in full; partial updates are not supported.</p>
<h5 id="retrieving-properties-and-metadata">Retrieving properties and metadata</h5>
<p>The GET and HEAD operations both retrieve metadata headers for the specified container or blob. These operations return headers only; they do not return a response body.<br>
The URI syntax for retrieving metadata headers on a container is as follows:</p>
<pre><code>GET/HEAD https://myaccount.blob.core.windows.net/mycontainer?restype=container  
</code></pre>
<p>The URI syntax for retrieving metadata headers on a blob is as follows:</p>
<pre><code>GET/HEAD https://myaccount.blob.core.windows.net/mycontainer/myblob?comp=metadata
</code></pre>
<h5 id="setting-metadata-headers">Setting Metadata Headers</h5>
<p>The PUT operation sets metadata headers on the specified container or blob, overwriting any existing metadata on the resource.</p>
<p>The URI syntax for setting metadata headers on a container is as follows:</p>
<pre><code>PUT https://myaccount.blob.core.windows.net/mycontainer?comp=metadata&amp;restype=container

</code></pre>
<p>The URI syntax for setting metadata headers on a blob is as follows:</p>
<pre><code>PUT https://myaccount.blob.core.windows.net/mycontainer/myblob?comp=metadata
</code></pre>
<h4 id="standard-http-properties-for-containers-and-blobs">Standard HTTP properties for containers and blobs</h4>
<p>Properties and metadata are both represented as standard HTTP headers; the difference between them is in the naming of the headers. Metadata headers are named with the header prefix <code>x-ms-meta-</code> and a custom name. Property headers use standard HTTP header names, as specified in the Header Field Definitions section 14 of the HTTP/1.1 protocol specification.</p>
<p>The standard HTTP headers supported on containers include:</p>
<ul>
<li><code>ETag</code></li>
<li><code>Last-Modified</code></li>
</ul>
<p>The standard HTTP headers supported on blobs include:</p>
<ul>
<li><code>ETag</code></li>
<li><code>Last-Modified</code></li>
<li><code>Content-Length</code></li>
<li><code>Content-Type</code></li>
<li><code>Content-MD5</code></li>
<li><code>Content-Encoding</code></li>
<li><code>Content-Language</code></li>
<li><code>Cache-Control</code></li>
<li><code>Origin</code></li>
<li><code>Range</code></li>
</ul>
<h1 id="az-204-develop-solutions-that-use-azure-cosmos-db">AZ-204: Develop solutions that use Azure Cosmos DB</h1>
<h2 id="explore-azure-cosmos-db">Explore Azure Cosmos DB</h2>
<h3 id="identify-key-benefits-of-azure-cosmos-db">Identify key benefits of Azure Cosmos DB</h3>
<p>Azure Cosmos DB is a globally distributed database system that allows you to read and write data from the local replicas of your database and it transparently replicates the data to all the regions associated with your Cosmos account.</p>
<p>Azure Cosmos DB is designed to provide low latency, elastic scalability of throughput, well-defined semantics for data consistency, and high availability. To lower the latency, place the data close to where your users are.</p>
<h4 id="key-benefits-of-global-distribution">Key benefits of global distribution</h4>
<p>With its novel multi-master replication protocol, every region supports both writes and reads. The multi-master capability also enables:</p>
<ul>
<li>Unlimited elastic write and read scalability.</li>
<li>99.999% read and write availability all around the world.</li>
<li>Guaranteed reads and writes served in less than 10 milliseconds at the 99th percentile.</li>
<li></li>
</ul>
<p>Your application can perform near real-time reads and writes against all the regions you chose for your database. Azure Cosmos DB internally handles the data replication between regions with consistency level guarantees of the level you’ve selected.</p>
<h3 id="explore-the-resource-hierarchy">Explore the resource hierarchy</h3>
<p>The Azure Cosmos account is the fundamental unit of global distribution and high availability. Your Azure Cosmos account contains a unique DNS name and you can manage an account by using the Azure portal or the Azure CLI, or by using different language-specific SDKs.</p>
<h4 id="elements-in-an-azure-cosmos-account">Elements in an Azure Cosmos account</h4>
<p>An Azure Cosmos container is the fundamental unit of scalability. You can virtually have an unlimited provisioned throughput (RU/s) and storage on a container. Azure Cosmos DB transparently partitions your container using the logical partition key that you specify in order to elastically scale your provisioned throughput and storage.</p>
<p>Currently, you can create a maximum of 50 Azure Cosmos accounts under an Azure subscription (this is a soft limit that can be increased via support request).</p>
<p><img src="https://docs.microsoft.com/en-us/learn/wwl-azure/explore-azure-cosmos-db/media/cosmos-entities.png" alt="Image showing the hierarchy of Azure Cosmos DB entities: Database accounts are at the top, Databases are grouped under accounts, Containers are grouped under databases."></p>
<h4 id="azure-cosmos-databases">Azure Cosmos databases</h4>
<p>A database is analogous to a namespace. A database is the unit of management for a set of Azure Cosmos containers.</p>
<h4 id="azure-cosmos-containers">Azure Cosmos containers</h4>
<p>An Azure Cosmos container is the unit of scalability both for provisioned throughput and storage. A container is horizontally partitioned and then replicated across multiple regions.</p>
<p>When you create a container, you configure throughput in one of the following modes:</p>
<ul>
<li>
<p><strong>Dedicated provisioned throughput mode</strong>: The throughput provisioned on a container is exclusively reserved for that container and it is backed by the SLAs.</p>
</li>
<li>
<p><strong>Shared provisioned throughput mode</strong>: These containers share the provisioned throughput with the other containers in the same database (excluding containers that have been configured with dedicated provisioned throughput).</p>
</li>
</ul>
<p>A container is a schema-agnostic container of items. Items in a container can have arbitrary schemas.</p>
<h4 id="azure-cosmos-items">Azure Cosmos items</h4>
<p>Depending on which API you use, an Azure Cosmos item can represent either a document in a collection, a row in a table, or a node or edge in a graph.</p>

<table>
<thead>
<tr>
<th>Cosmos entity</th>
<th>SQL API</th>
<th>Cassandra API</th>
<th>Azure Cosmos DB API for MongoDB</th>
<th>Gremlin API</th>
<th>Table API</th>
</tr>
</thead>
<tbody>
<tr>
<td>Azure Cosmos item</td>
<td>Item</td>
<td>Row</td>
<td>Document</td>
<td>Node or edge</td>
<td>Item</td>
</tr>
</tbody>
</table><h3 id="explore-consistency-levels">Explore consistency levels</h3>
<p>Azure Cosmos DB approaches data consistency as a spectrum of choices instead of two extremes. Strong consistency and eventual consistency are at the ends of the spectrum, but there are many consistency choices along the spectrum. Developers can use these options to make precise choices and granular tradeoffs with respect to high availability and performance.</p>
<p>Consistency spectrum:</p>
<ul>
<li><em>strong</em></li>
<li><em>bounded staleness</em></li>
<li><em>session</em></li>
<li><em>consistent prefix</em></li>
<li><em>eventual</em></li>
</ul>
<p><img src="https://docs.microsoft.com/en-us/learn/wwl-azure/explore-azure-cosmos-db/media/five-consistency-levels.png" alt="Image showing data consistency as a spectrum"></p>
<p>The consistency levels are region-agnostic and are guaranteed for all operations regardless of the region from which the reads and writes are served.</p>
<h3 id="choose-the-right-consistency-level">Choose the right consistency level</h3>
<h4 id="sql-api-and-table-api">SQL API and Table API</h4>
<ul>
<li>For many real-world scenarios, session consistency is optimal and it’s the recommended option.
<ul>
<li>If your application requires strong consistency, it is recommended that you use bounded staleness consistency level.</li>
</ul>
</li>
<li>If you need the highest throughput and the lowest latency, then use eventual consistency level.</li>
<li>If you need even higher data durability without sacrificing performance, you can create a custom consistency level at the application layer.</li>
</ul>
<h4 id="cassandra-mongodb-and-gremlin-apis">Cassandra, MongoDB, and Gremlin APIs</h4>
<p>Azure Cosmos DB provides native support for wire protocol-compatible APIs for popular databases. These include MongoDB, Apache Cassandra, and Gremlin.</p>
<h4 id="consistency-guarantees-in-practice">Consistency guarantees in practice</h4>
<ul>
<li>When the consistency level is set to  <strong>bounded staleness</strong>, Cosmos DB guarantees that the clients always read the value of a previous write, with a lag bounded by the staleness window.
<ul>
<li>When the consistency level is set to  <strong>strong</strong>, the staleness window is equivalent to zero, and the clients are guaranteed to read the latest committed value of the write operation.</li>
<li>For the remaining three consistency levels, the staleness window is largely dependent on your workload. For example, if there are no write operations on the database, a read operation with  <strong>eventual</strong>,  <strong>session</strong>, or  <strong>consistent prefix</strong>  consistency levels is likely to yield the same results as a read operation with strong consistency level.</li>
</ul>
</li>
</ul>
<p>If your Azure Cosmos account is configured with a consistency level other than the strong consistency, you can find out the probability that your clients may get strong and consistent reads for your workloads by looking at the <em>Probabilistically Bounded Staleness</em> (PBS) metric.</p>
<h3 id="explore-supported-apis">Explore supported APIs</h3>
<p>These APIs allow your applications to treat Azure Cosmos DB as if it were various other databases technologies, without the overhead of management, and scaling approaches.</p>
<h4 id="coresql-api">Core(SQL) API</h4>
<p>Azure Cosmos DB SQL API accounts provide support for querying items using the Structured Query Language (SQL) syntax, one of the most familiar and popular query languages.</p>
<p>This API stores data in document format. It offers the best end-to-end experience as we have full control over the interface, service, and the SDK client libraries. Any new feature that is rolled out to Azure Cosmos DB is first available on SQL API accounts.</p>
<h4 id="api-for-mongodb">API for MongoDB</h4>
<p>This API stores data in a document structure, via BSON format. It is compatible with MongoDB wire protocol; however, it does not use any native MongoDB related code.</p>
<h4 id="cassandra-api">Cassandra API</h4>
<p>This API stores data in column-oriented schema. Cassandra API is wire protocol compatible with the Apache Cassandra.</p>
<h4 id="table-api">Table API</h4>
<p>This API stores data in key/value format. If you are currently using Azure Table storage, you may see some limitations in latency, scaling, throughput, global distribution, index management, low query performance. Table API overcomes these limitations and it’s recommended to migrate your app if you want to use the benefits of Azure Cosmos DB.</p>
<h4 id="gremlin-api">Gremlin API</h4>
<p>This API allows users to make graph queries and stores data as edges and vertices.</p>
<h3 id="discover-request-units">Discover request units</h3>
<p>With Azure Cosmos DB, you pay for the throughput you provision and the storage you consume on an hourly basis. Throughput must be provisioned to ensure that sufficient system resources are available for your Azure Cosmos database at all times.</p>
<p>The cost of all database operations is normalized by Azure Cosmos DB and is expressed by <em>request units</em> (or RUs, for short). The cost to do a point read, which is fetching a single item by its ID and partition key value, for a 1KB item is 1RU.</p>
<p><img src="https://docs.microsoft.com/en-us/learn/wwl-azure/explore-azure-cosmos-db/media/request-units.png" alt="Image showing how database operations consume request units."></p>
<p>The type of Azure Cosmos account you’re using determines the way consumed RUs get charged. There are three modes in which you can create an account:</p>
<p><strong>Provisioned throughput mode</strong>: In this mode, you provision the number of RUs for your application on a per-second basis in increments of 100 RUs per second.</p>
<ul>
<li><strong>Serverless mode</strong>: In this mode, you don’t have to provision any throughput when creating resources in your Azure Cosmos account. At the end of your billing period, you get billed for the amount of request units that has been consumed by your database operations.</li>
<li><strong>Autoscale mode</strong>: In this mode, you can automatically and instantly scale the throughput (RU/s) of your database or container based on it’s usage. This mode is well suited for mission-critical workloads that have variable or unpredictable traffic patterns, and require SLAs on high performance and scale.</li>
</ul>
<h2 id="implement-partitioning-in-azure-cosmos-db">Implement partitioning in Azure Cosmos DB</h2>
<h3 id="explore-partitions">Explore partitions</h3>
<p>Azure Cosmos DB uses partitioning to scale individual containers in a database to meet the performance needs of your application.</p>
<p>In partitioning, the items in a container are divided into distinct subsets called <em>logical partitions</em>. Logical partitions are formed based on the value of a <em>partition key</em> that is associated with each item in a container.</p>
<p>In addition to a partition key that determines the item’s logical partition, each item in a container has an <em>item ID</em> which is unique within a logical partition. Combining the partition key and the <em>item ID</em> creates the item’s <em>index</em>, which uniquely identifies the item. Choosing a partition key is an important decision that will affect your application’s performance.</p>
<h4 id="logical-partitions">Logical partitions</h4>
<ul>
<li>A logical partition consists of a set of items that have the same partition key.</li>
<li>A logical partition also defines the scope of database transactions.</li>
</ul>
<h4 id="physical-partitions">Physical partitions</h4>
<p>A container is scaled by distributing data and throughput across physical partitions. Internally, one or more logical partitions are mapped to a single physical partition. Typically smaller containers have many logical partitions but they only require a single physical partition. Unlike logical partitions, physical partitions are an internal implementation of the system and they are entirely managed by Azure Cosmos DB.</p>
<p>The number of physical partitions in your container depends on the following:</p>
<ul>
<li>The number of throughput provisioned. The 10,000 RU/s limit for physical partitions implies that logical partitions also have a 10,000 RU/s limit, as each logical partition is only mapped to one physical partition.</li>
<li>The total data storage (each individual physical partition can store up to 50GB data).</li>
</ul>
<p>A partition key design that doesn’t distribute requests evenly might result in too many requests directed to a small subset of partitions that become “hot.”</p>
<h3 id="choose-a-partition-key">Choose a partition key</h3>
<p>A partition key has two components: <strong>partition key path</strong> and the <strong>partition key value</strong>.</p>
<ul>
<li>The partition key path (for example: “/userId”) accepts alphanumeric and underscore(_) characters. You can also use nested objects by using the standard path notation(/).</li>
<li>The partition key value (for example: “Andrew”) can be of string or numeric types.</li>
</ul>
<p>Once you select your partition key, it is not possible to change it in-place.</p>
<p>For  <strong>all</strong>  containers, your partition key should:</p>
<ul>
<li>Be a property that has a value which does not change.</li>
<li>Have a high cardinality. In other words, the property should have a wide range of possible values.</li>
<li>Spread request unit (RU) consumption and data storage evenly across all logical partitions. This ensures even RU consumption and storage distribution across your physical partitions.</li>
</ul>
<h4 id="partition-keys-for-read-heavy-containers">Partition keys for read-heavy containers</h4>
<p>For large read-heavy containers you might want to choose a partition key that appears frequently as a filter in your queries.</p>
<p>However, if your container is small, you probably don’t have enough physical partitions to need to worry about the performance impact of cross-partition queries. If your container could grow to more than a few physical partitions, then you should make sure you pick a partition key that minimizes cross-partition queries.</p>
<p>Your container will require more than a few physical partitions when either of the following are true:</p>
<ul>
<li>Your container will have over 30,000 RU’s provisioned</li>
<li>Your container will store over 100 GB of data</li>
</ul>
<h4 id="using-item-id-as-the-partition-key">Using item ID as the partition key</h4>
<p>If your container has a property that has a wide range of possible values, it is likely a great partition key choice. One possible example of such a property is the <em>item ID</em>. For small read-heavy containers or write-heavy containers of any size, the <em>item ID</em> is naturally a great choice for the partition key.</p>
<p>The  <em>item ID</em>  is a great partition key choice for the following reasons:</p>
<ul>
<li>There are a wide range of possible values (one unique  <em>item ID</em>  per item).</li>
<li>Because there is a unique  <em>item ID</em>  per item, the  <em>item ID</em>  does a great job at evenly balancing RU consumption and data storage.</li>
<li>You can easily do efficient point reads since you’ll always know an item’s partition key if you know its  <em>item ID</em>.</li>
</ul>
<p>Some things to consider when selecting the  <em>item ID</em>  as the partition key include:</p>
<ul>
<li>If the  <em>item ID</em>  is the partition key, it will become a unique identifier throughout your entire container. You won’t be able to have items that have a duplicate  <em>item ID</em>.</li>
<li>If you have a read-heavy container that has a lot of physical partitions, queries will be more efficient if they have an equality filter with the  <em>item ID</em>.</li>
<li>You can’t run stored procedures or triggers across multiple logical partitions.</li>
</ul>
<h3 id="create-a-synthetic-partition-key">Create a synthetic partition key</h3>
<p>It’s the best practice to have a partition key with many distinct values, such as hundreds or thousands. The goal is to distribute your data and workload evenly across the items associated with these partition key values. If such a property doesn’t exist in your data, you can construct a <em>synthetic partition key</em>.</p>
<h4 id="concatenate-multiple-properties-of-an-item">Concatenate multiple properties of an item</h4>
<p>You can form a partition key by concatenating multiple property values into a single artificial <code>partitionKey</code> property.</p>
<h4 id="use-a-partition-key-with-a-random-suffix">Use a partition key with a random suffix</h4>
<p>Another possible strategy to distribute the workload more evenly is to append a random number at the end of the partition key value. When you distribute items in this way, you can perform parallel write operations across partitions.</p>
<h4 id="use-a-partition-key-with-pre-calculated-suffixes">Use a partition key with pre-calculated suffixes</h4>
<p>The random suffix strategy can greatly improve write throughput, but it’s difficult to read a specific item. You don’t know the suffix value that was used when you wrote the item. To make it easier to read individual items, use the pre-calculated suffixes strategy. Instead of using a random number to distribute the items among the partitions, use a number that is calculated based on something that you want to query. Before your application writes the item to the container, it can calculate a hash suffix based on a cloumn that is often used for querying and append it to the partition key date.</p>
<h2 id="work-with-azure-cosmos-db">Work with Azure Cosmos DB</h2>
<h3 id="explore-microsoft-.net-sdk-v3-for-azure-cosmos-db">Explore Microsoft .NET SDK v3 for Azure Cosmos DB</h3>
<p>Below are examples showing some of the key operations you should be familiar with.</p>
<h4 id="cosmosclient">CosmosClient</h4>
<p>Creates a new  <code>CosmosClient</code>  with a connection string.  <code>CosmosClient</code>  is thread-safe. Its recommended to maintain a single instance of  <code>CosmosClient</code>  per lifetime of the application which enables efficient connection management and performance.</p>
<pre><code>CosmosClient client = new CosmosClient(endpoint, key);
</code></pre>
<h2 id="database-examples">Database examples</h2>
<h3 id="create-a-database">Create a database</h3>
<pre><code>// An object containing relevant information about the response
DatabaseResponse databaseResponse = await client.CreateDatabaseIfNotExistsAsync(databaseId, 10000);
</code></pre>
<h3 id="read-a-database-by-id">Read a database by ID</h3>
<p>Reads a database from the Azure Cosmos service as an asynchronous operation.</p>
<pre><code>DatabaseResponse readResponse = await database.ReadAsync();

</code></pre>
<h3 id="delete-a-database">Delete a database</h3>
<p>Delete a Database as an asynchronous operation.</p>
<pre><code>await database.DeleteAsync();
</code></pre>
<h2 id="container-examples">Container examples</h2>
<h3 id="create-a-container">Create a container</h3>
<pre><code>// Set throughput to the minimum value of 400 RU/s
ContainerResponse simpleContainer = await database.CreateContainerIfNotExistsAsync(
    id: containerId,
    partitionKeyPath: partitionKey,
    throughput: 400);

</code></pre>
<h3 id="get-a-container-by-id">Get a container by ID</h3>
<pre><code>Container container = database.GetContainer(containerId);
ContainerProperties containerProperties = await container.ReadContainerAsync();

</code></pre>
<h3 id="delete-a-container">Delete a container</h3>
<p>Delete a Container as an asynchronous operation.</p>
<p>C#Copy</p>
<pre><code>await database.GetContainer(containerId).DeleteContainerAsync();

</code></pre>
<h2 id="item-examples">Item examples</h2>
<h3 id="create-an-item">Create an item</h3>
<p>Use the  <code>Container.CreateItemAsync</code>  method to create an item. The method requires a JSON serializable object that must contain an  <code>id</code>  property, and a  <code>partitionKey</code>.</p>
<p>C#Copy</p>
<pre><code>ItemResponse&lt;SalesOrder&gt; response = await container.CreateItemAsync(salesOrder, new PartitionKey(salesOrder.AccountNumber));

</code></pre>
<h3 id="read-an-item">Read an item</h3>
<p>Use the  <code>Container.ReadItemAsync</code>  method to read an item. The method requires type to serialize the item to along with an  <code>id</code>  property, and a  <code>partitionKey</code>.</p>
<pre><code>string id = "[id]";
string accountNumber = "[partition-key]";
ItemResponse&lt;SalesOrder&gt; response = await container.ReadItemAsync(id, new PartitionKey(accountNumber));

</code></pre>
<h3 id="query-an-item">Query an item</h3>
<p>The  <code>Container.GetItemQueryIterator</code>  method creates a query for items under a container in an Azure Cosmos database using a SQL statement with parameterized values. It returns a  <code>FeedIterator</code>.</p>
<pre><code>QueryDefinition query = new QueryDefinition(
    "select * from sales s where s.AccountNumber = @AccountInput ")
    .WithParameter("@AccountInput", "Account1");

FeedIterator&lt;SalesOrder&gt; resultSet = container.GetItemQueryIterator&lt;SalesOrder&gt;(
    query,
    requestOptions: new QueryRequestOptions()
    {
        PartitionKey = new PartitionKey("Account1"),
        MaxItemCount = 1
    });
</code></pre>
<h3 id="create-stored-procedures">Create stored procedures</h3>
<p>Azure Cosmos DB provides language-integrated, transactional execution of JavaScript that lets you write <strong>stored procedures</strong>, <strong>triggers</strong>, and <strong>user-defined functions (UDFs)</strong>. To call a stored procedure, trigger, or user-defined function, you need to register it.</p>
<h4 id="writing-stored-procedures">Writing stored procedures</h4>
<p>Stored procedures can create, update, read, query, and delete items inside an Azure Cosmos container. Stored procedures are registered per collection, and can operate on any document or an attachment present in that collection.</p>
<p>Here is a simple stored procedure that returns a “Hello World” response.</p>
<pre><code>var helloWorldStoredProc = {
    id: "helloWorld",
    serverScript: function () {
        var context = getContext();
        var response = context.getResponse();

        response.setBody("Hello, World");
    }
}
</code></pre>
<p>The context object provides access to all operations that can be performed in Azure Cosmos DB, as well as access to the request and response objects.</p>
<h4 id="create-an-item-using-stored-procedure">Create an item using stored procedure</h4>
<p>When you create an item by using stored procedure it is inserted into the Azure Cosmos container and an ID for the newly created item is returned. Creating an item is an asynchronous operation and depends on the JavaScript callback functions. The callback function has two parameters:</p>
<ul>
<li>The error object in case the operation fails</li>
<li>A return value</li>
</ul>
<p>The following example stored procedure takes an input parameter named  <code>documentToCreate</code>  and the parameter’s value is the body of a document to be created in the current collection. The callback throws an error if the operation fails. Otherwise, it sets the  <code>id</code>  of the created document as the body of the response to the client.</p>
<pre><code>function createSampleDocument(documentToCreate) {
    var context = getContext();
    var collection = context.getCollection();
    var accepted = collection.createDocument(
        collection.getSelfLink(),
        documentToCreate,
        function (error, documentCreated) {                 
            context.getResponse().setBody(documentCreated.id)
        }
    );
    if (!accepted) return;
}
</code></pre>
<h4 id="arrays-as-input-parameters-for-stored-procedures">Arrays as input parameters for stored procedures</h4>
<p>When defining a stored procedure in the Azure portal, input parameters are always sent as a string to the stored procedure. Even if you pass an array of strings as an input, the array is converted to string and sent to the stored procedure. To work around this, you can define a function within your stored procedure to parse the string as an array.</p>
<h4 id="bounded-execution">Bounded execution</h4>
<p>All Azure Cosmos DB operations must complete within a limited amount of time. Stored procedures have a limited amount of time to run on the server.</p>
<h4 id="transactions-within-stored-procedures">Transactions within stored procedures</h4>
<p>You can implement transactions on items within a container by using a stored procedure. JavaScript functions can implement a continuation-based model to batch or resume execution.<br>
<img src="https://docs.microsoft.com/en-us/training/wwl-azure/work-with-cosmos-db/media/transaction-continuation-model.png" alt="This diagram depicts how the transaction continuation model can be used to repeat a server-side function until the function finishes its entire processing workload."></p>
<h3 id="create-triggers-and-user-defined-functions">Create triggers and user-defined functions</h3>
<p>Azure Cosmos DB supports pre-triggers and post-triggers. Pre-triggers are executed before modifying a database item and post-triggers are executed after modifying a database item. Triggers are not automatically executed, they must be specified for each database operation where you want them to execute.</p>
<h4 id="pre-triggers">Pre-triggers</h4>
<p>Pre-triggers cannot have any input parameters. The request object in the trigger is used to manipulate the request message associated with the operation.</p>
<p>When triggers are registered, you can specify the operations that it can run with. This trigger should be created with a <code>TriggerOperation</code> value of <code>TriggerOperation.Create</code>, which means using the trigger in a replace operation is not permitted.</p>
<h4 id="post-triggers">Post-triggers</h4>
<p>One thing that is important to note is the transactional execution of triggers in Azure Cosmos DB. The post-trigger runs as part of the same transaction for the underlying item itself. An exception during the post-trigger execution will fail the whole transaction. Anything committed will be rolled back and an exception returned.</p>
<h4 id="user-defined-functions">User-defined functions</h4>
<p>This user-defined functions are used inside a queries.</p>
<h1 id="az-204-implement-infrastructure-as-a-service-solutions">AZ-204: Implement infrastructure as a service solutions</h1>
<h2 id="provision-virtual-machines-in-azure">Provision virtual machines in Azure</h2>
<h3 id="explore-azure-virtual-machines">Explore Azure virtual machines</h3>
<p>Azure virtual machines (VM) is one of several types of on-demand, scalable computing resources that Azure offers. Typically, you choose a VM when you need more control over the computing environment than the other choices offer.</p>
<p>An Azure virtual machine gives you the flexibility of virtualization without having to buy and maintain the physical hardware that runs it. However, you still need to maintain the VM by performing tasks, such as configuring, patching, and installing the software that runs on it.</p>
<p>Azure virtual machines can be used in various ways:</p>
<ul>
<li>Development and test</li>
<li>Applications in the cloud</li>
<li>Extended datacenter</li>
</ul>
<h4 id="design-considerations-for-virtual-machine-creation">Design considerations for virtual machine creation</h4>
<p>There are always a multitude of design considerations when you build out an application infrastructure in Azure. These aspects of a VM are important to think about before you start:</p>
<ul>
<li>
<p><strong>Availability</strong>: Azure supports a single instance virtual machine Service Level Agreement of 99.9% provided you deploy the VM with premium storage for all disks.</p>
</li>
<li>
<p><strong>VM size</strong>: The size of the VM that you use is determined by the workload that you want to run.</p>
</li>
<li>
<p><strong>VM limits</strong>: Your subscription has default quota limits in place that could impact the deployment of many VMs for your project. The current limit on a per subscription basis is 20 VMs per region.</p>
</li>
<li>
<p><strong>VM image</strong>: You can either use your own image, or you can use one of the images in the Azure Marketplace.</p>
</li>
<li>
<p><strong>VM disks</strong>: There are two components that make up this area. The type of disks which determines the performance level and the storage account type that contains the disks. Azure provides two types of disks:</p>
<ul>
<li>
<p><strong>Standard disks</strong>: Backed by HDDs, and delivers cost-effective storage while still being performant. Standard disks are ideal for a cost effective dev and test workload.</p>
</li>
<li>
<p><strong>Premium disks</strong>: Backed by SSD-based, high-performance, low-latency disk. Perfect for VMs running production workload.</p>
</li>
</ul>
<p>And, there are two options for the disk storage:</p>
<ul>
<li>
<p><strong>Managed disks</strong>: Managed disks are the newer and recommended disk storage model and they are managed by Azure.</p>
</li>
<li>
<p><strong>Unmanaged disks</strong>: With unmanaged disks, you’re responsible for the storage accounts that hold the virtual hard disks (VHDs) that correspond to your VM disks.</p>
</li>
</ul>
</li>
</ul>
<h4 id="virtual-machine-extensions">Virtual machine extensions</h4>
<p>Windows VMs have extensions which give your VM additional capabilities through post deployment configuration and automated tasks. These common tasks can be accomplished using extensions:</p>
<ul>
<li><strong>Run custom scripts</strong>: The Custom Script Extension helps you configure workloads on the VM by running your script when the VM is provisioned.</li>
<li><strong>Deploy and manage configurations</strong>: The PowerShell Desired State Configuration (DSC) Extension helps you set up DSC on a VM to manage configurations and environments.</li>
<li><strong>Collect diagnostics data</strong>: The Azure Diagnostics Extension helps you configure the VM to collect diagnostics data that can be used to monitor the health of your application.</li>
</ul>
<p><a href="https://docs.microsoft.com/en-us/training/modules/provision-virtual-machines-azure/3-azure-virtual-machine-availability-options">https://docs.microsoft.com/en-us/training/modules/provision-virtual-machines-azure/3-azure-virtual-machine-availability-options</a></p>

    </div>
  </div>
</body>

</html>
