<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>AZ-204</title>
  <link rel="stylesheet" href="https://stackedit.io/style.css" />
</head>

<body class="stackedit">
  <div class="stackedit__left">
    <div class="stackedit__toc">
      
<ul>
<li><a href="#create-azure-app-service-web-apps">Create Azure App Service web apps</a>
<ul>
<li><a href="#explore-azure-app-service">Explore Azure App Service</a></li>
<li><a href="#scale-apps-in-azure-app-service">Scale apps in Azure App Service</a></li>
<li><a href="#explore-azure-app-service-deployment-slots">Explore Azure App Service deployment slots</a></li>
</ul>
</li>
<li><a href="#az-204-implement-azure-functions">AZ-204: Implement Azure Functions</a>
<ul>
<li><a href="#explore-azure-functions">Explore Azure Functions</a>
<ul>
<li></li>
<li><a href="#compare-azure-functions-hosting-options">Compare Azure Functions hosting options</a></li>
<li><a href="#scale-azure-functions">Scale Azure Functions</a></li>
<li><a href="#develop-azure-functions">Develop Azure Functions</a></li>
<li><a href="#create-triggers-and-bindings">Create triggers and bindings</a></li>
<li><a href="#javascript-example">JavaScript example</a></li>
<li><a href="#class-library-example">Class library example</a></li>
<li><a href="#connect-functions-to-azure-services">Connect functions to Azure services</a></li>
</ul>
</li>
<li><a href="#implement-durable-functions">Implement Durable Functions</a></li>
</ul>
</li>
<li><a href="#az-204-develop-solutions-that-use-blob-storage">AZ-204: Develop solutions that use Blob storage</a>
<ul>
<li><a href="#explore-azure-blob-storage">Explore Azure Blob storage</a></li>
<li><a href="#manage-the-azure-blob-storage-lifecycle">Manage the Azure Blob storage lifecycle</a>
<ul>
<li></li>
<li><a href="#discover-blob-storage-lifecycle-policies">Discover Blob storage lifecycle policies</a></li>
<li><a href="#implement-blob-storage-lifecycle-policies">Implement Blob storage lifecycle policies</a></li>
<li><a href="#rehydrate-blob-data-from-the-archive-tier">Rehydrate blob data from the archive tier</a></li>
</ul>
</li>
<li><a href="#work-with-azure-blob-storage">Work with Azure Blob storage</a></li>
</ul>
</li>
<li><a href="#az-204-develop-solutions-that-use-azure-cosmos-db">AZ-204: Develop solutions that use Azure Cosmos DB</a>
<ul>
<li><a href="#explore-azure-cosmos-db">Explore Azure Cosmos DB</a></li>
<li><a href="#implement-partitioning-in-azure-cosmos-db">Implement partitioning in Azure Cosmos DB</a></li>
<li><a href="#work-with-azure-cosmos-db">Work with Azure Cosmos DB</a></li>
<li><a href="#database-examples">Database examples</a></li>
<li><a href="#container-examples">Container examples</a></li>
<li><a href="#item-examples">Item examples</a></li>
</ul>
</li>
<li><a href="#az-204-implement-infrastructure-as-a-service-solutions">AZ-204: Implement infrastructure as a service solutions</a>
<ul>
<li><a href="#provision-virtual-machines-in-azure">Provision virtual machines in Azure</a></li>
<li><a href="#create-and-deploy-azure-resource-manager-templates">Create and deploy Azure Resource Manager templates</a></li>
<li><a href="#create-and-deploy-azure-resource-manager-templates-1">Create and deploy Azure Resource Manager templates</a></li>
<li><a href="#manage-container-images-in-azure-container-registry">Manage container images in Azure Container Registry</a></li>
<li><a href="#run-container-images-in-azure-container-instances">Run container images in Azure Container Instances</a></li>
<li><a href="#deploy-container-and-mount-volume---yaml">Deploy container and mount volume - YAML</a></li>
<li><a href="#mount-multiple-volumes">Mount multiple volumes</a></li>
</ul>
</li>
<li><a href="#az-204-implement-user-authentication-and-authorization">AZ-204 Implement user authentication and authorization</a>
<ul>
<li><a href="#explore-the-microsoft-identity-platform">Explore the Microsoft identity platform</a></li>
<li><a href="#implement-authentication-by-using-the-microsoft-authentication-library">Implement authentication by using the Microsoft Authentication Library</a></li>
<li><a href="#explore-microsoft-graph">Explore Microsoft Graph</a></li>
</ul>
</li>
<li><a href="#az-204-implement-secure-cloud-solutions">AZ-204 Implement secure cloud solutions</a>
<ul>
<li><a href="#implement-azure-key-vault">Implement Azure Key Vault</a></li>
<li><a href="#implement-managed-identities">Implement managed identities</a></li>
<li><a href="#implement-azure-app-configuration">Implement Azure App Configuration</a></li>
</ul>
</li>
<li><a href="#az-204-implement-api-management">AZ-204 Implement API Management</a>
<ul>
<li><a href="#explore-api-management">Explore API Management</a></li>
<li><a href="#understanding-policy-configuration">Understanding policy configuration</a></li>
<li><a href="#examples">Examples</a></li>
<li><a href="#next-unit-create-advanced-policies">Next unit: Create advanced policies</a></li>
<li><a href="#create-advanced-policies---training--microsoft-docs.md">Create advanced policies - Training  Microsoft Docs.md</a></li>
</ul>
</li>
</ul>

    </div>
  </div>
  <div class="stackedit__right">
    <div class="stackedit__html">
      <h1 id="create-azure-app-service-web-apps">Create Azure App Service web apps</h1>
<h2 id="explore-azure-app-service">Explore Azure App Service</h2>
<h3 id="examine-azure-app-service">Examine Azure App Service</h3>
<p>Azure App Service is an HTTP-based service for hosting web applications, REST APIs, and mobile back ends.<br>
Supported programming languages:</p>
<ul>
<li>.NET,</li>
<li>.NET Core,</li>
<li>Java,</li>
<li>Ruby,</li>
<li>Node.js,</li>
<li>PHP,</li>
<li>Python.<br>
Applications run and scale with ease on both Windows and Linux-based environments.</li>
</ul>
<h4 id="features">Features:</h4>
<ul>
<li>Built-in auto scale support: Baked into Azure App Service is the ability to scale up/down or scale out/in.</li>
<li>Continuous integration/deployment support: The Azure portal provides out-of-the-box continuous integration and deployment with Azure DevOps, GitHub, Bitbucket, FTP, or a local Git repository on your development machine.</li>
<li>Deployment slots: When you deploy your web app, web app on Linux, mobile back end, or API app to Azure App Service, you can use a separate deployment slot instead of the default production slot when you’re running in the Standard, Premium, or Isolated App Service plan tier.</li>
<li>App Service on Linux: App Service can also host web apps natively on Linux for supported application stacks.</li>
</ul>
<p>App Service on Linux does have some limitations:</p>
<ul>
<li>App Service on Linux is not supported on Shared pricing tier.</li>
<li>You can’t mix Windows and Linux apps in the same App Service plan.</li>
</ul>
<h3 id="examine-azure-app-service-plans">Examine Azure App Service plans</h3>
<p>In App Service, an app (Web Apps, API Apps, or Mobile Apps) always runs in an <em>App Service plan</em>. An App Service plan defines a set of compute resources for a web app to run. One or more apps can be configured to run on the same computing resources (or in the same App Service plan).</p>
<p>Each App Service plan defines:</p>
<ul>
<li>Region (West US, East US, etc.)</li>
<li>Number of VM instances</li>
<li>Size of VM instances (Small, Medium, Large)</li>
<li>Pricing tier (Free, Shared, Basic, Standard, Premium, PremiumV2, PremiumV3, Isolated)</li>
</ul>
<p>The <em>pricing tier</em> of an App Service plan determines what App Service features you get and how much you pay for the plan. There are a few categories of pricing tiers:</p>
<ul>
<li><strong>Shared compute</strong>: Both <strong>Free</strong> and <strong>Shared</strong> share the resource pools of your apps with the apps of other customers. These tiers allocate CPU quotas to each app that runs on the shared resources, and the resources can’t scale out.</li>
<li><strong>Dedicated compute</strong>: The <strong>Basic</strong>, <strong>Standard</strong>, <strong>Premium</strong>, <strong>PremiumV2</strong>, and <strong>PremiumV3</strong> tiers run apps on dedicated Azure VMs. Only apps in the same App Service plan share the same compute resources. The higher the tier, the more VM instances are available to you for scale-out.</li>
<li><strong>Isolated</strong>: This tier runs dedicated Azure VMs on dedicated Azure Virtual Networks. It provides network isolation on top of compute isolation to your apps. It provides the maximum scale-out capabilities.</li>
<li><strong>Consumption:</strong> This tier is only available to <em>function apps</em>. It scales the functions dynamically depending on workload.</li>
</ul>
<h4 id="how-does-my-app-run-and-scale">How does my app run and scale?</h4>
<p>The App Service plan is the <strong>scale unit</strong> of the App Service apps:</p>
<ul>
<li>An app runs on all the VM instances configured in the App Service plan.</li>
<li>If multiple apps are in the same App Service plan, they all share the same VM instances.</li>
<li>If you have multiple deployment slots for an app, all deployment slots also run on the same VM instances.</li>
<li>If you enable diagnostic logs, perform backups, or run WebJobs, they also use CPU cycles and memory on these VM instances.</li>
</ul>
<h4 id="what-if-my-app-needs-more-capabilities-or-features">What if my app needs more capabilities or features?</h4>
<p>Isolate your app into a new App Service plan when:</p>
<ul>
<li>The app is resource-intensive.</li>
<li>You want to scale the app independently from the other apps in the existing plan.</li>
<li>The app needs resource in a different geographical region.</li>
</ul>
<h3 id="deploy-to-app-service">Deploy to App Service</h3>
<h4 id="automated-deployment">Automated deployment</h4>
<p>Azure supports automated deployment directly from several sources. The following options are available:</p>
<ul>
<li>Azure DevOps</li>
<li>GitHub</li>
<li>Bitbucket</li>
</ul>
<h4 id="manual-deployment">Manual deployment</h4>
<p>There are a few options that you can use to manually push your code to Azure:</p>
<ul>
<li><strong>Git</strong>: App Service web apps feature a Git URL that you can add as a remote repository.</li>
<li><strong>CLI</strong>: <code>webapp up</code> is a feature of the <code>az</code> command-line interface that packages your app and deploys it.</li>
<li><strong>Zip deploy</strong>: Use <code>curl</code> or a similar HTTP utility to send a ZIP of your application files to App Service.</li>
<li><strong>FTP/S</strong>: FTP or FTPS is a traditional way of pushing your code to many hosting environments, including App Service.</li>
</ul>
<h4 id="use-deployment-slots">Use deployment slots</h4>
<p>Whenever possible, use deployment slots when deploying a new production build. When using a Standard App Service Plan tier or better, you can deploy your app to a staging environment and then swap your staging and production slots. The swap operation warms up the necessary worker instances to match your production scale, thus eliminating downtime.</p>
<h3 id="explore-authentication-and-authorization-in-app-service">Explore authentication and authorization in App Service</h3>
<p>Azure App Service provides built-in authentication and authorization support by writing minimal or no code.</p>
<h4 id="why-use-the-built-in-authentication">Why use the built-in authentication?</h4>
<p>The built-in authentication feature for App Service and Azure Functions can save you time and effort by providing out-of-the-box authentication with federated identity providers.</p>
<h4 id="identity-providers">Identity providers</h4>
<p>App Service uses federated identity, in which a third-party identity provider manages the user identities and authentication flow for you. The following identity providers are available by default:</p>
<ul>
<li>Microsoft Identity Platform</li>
<li>Facebook</li>
<li>Google</li>
<li>Twitter</li>
<li>Any OpenID Connect provider</li>
</ul>
<h5 id="how-it-works">How it works</h5>
<p>The authentication and authorization module runs in the same sandbox as your application code. When it’s enabled, every incoming HTTP request passes through it before being handled by your application code. This module handles several things for your app:</p>
<ul>
<li>Authenticates users with the specified provider</li>
<li>Validates, stores, and refreshes tokens</li>
<li>Manages the authenticated session</li>
<li>Injects identity information into request headers</li>
</ul>
<p>The module runs separately from your application code and is configured using app settings.</p>
<h5 id="authentication-flow">Authentication flow</h5>
<ul>
<li>Without provider SDK: The application delegates federated sign-in to App Service. This is typically the case with browser apps, which can present the provider’s login page to the user. The server code manages the sign-in process, so it is also called <em>server-directed flow</em> or <em>server flow</em>.</li>
<li>With provider SDK: The application signs users in to the provider manually and then submits the authentication token to App Service for validation. This is typically the case with browser-less apps, which can’t present the provider’s sign-in page to the user. The application code manages the sign-in process, so it is also called <em>client-directed flow</em> or <em>client flow</em>. This applies to REST APIs, Azure Functions, JavaScript browser clients, and native mobile apps that sign users in using the provider’s SDK.</li>
</ul>

<table>
<thead>
<tr>
<th>Step</th>
<th>Without provider SDK</th>
<th>With provider SDK</th>
</tr>
</thead>
<tbody>
<tr>
<td>Sign user in</td>
<td>Redirects client to <code>/.auth/login/&lt;provider&gt;</code>.</td>
<td>Client code signs user in directly with provider’s SDK and receives an authentication token. For information, see the provider’s documentation.</td>
</tr>
<tr>
<td>Post-authentication</td>
<td>Provider redirects client to <code>/.auth/login/&lt;provider&gt;/callback</code>.</td>
<td>Client code posts token from provider to <code>/.auth/login/&lt;provider&gt;</code> for validation.</td>
</tr>
<tr>
<td>Establish authenticated session</td>
<td>App Service adds authenticated cookie to response.</td>
<td>App Service returns its own authentication token to client code.</td>
</tr>
<tr>
<td>Serve authenticated content</td>
<td>Client includes authentication cookie in subsequent requests (automatically handled by browser).</td>
<td>Client code presents authentication token in <code>X-ZUMO-AUTH</code> header (automatically handled by Mobile Apps client SDKs).</td>
</tr>
</tbody>
</table><h5 id="authorization-behavior">Authorization behavior</h5>
<ul>
<li>Allow unauthenticated requests: This option defers authorization of unauthenticated traffic to your application code.</li>
<li>Require authentication: This option will reject any unauthenticated traffic to your application. This rejection can be a redirect action to one of the configured identity providers.</li>
</ul>
<h3 id="discover-app-service-networking-features">Discover App Service networking features</h3>
<p>By default, apps hosted in App Service are accessible directly through the internet and can reach only internet-hosted endpoints. But for many applications, you need to control the inbound and outbound network traffic.</p>
<p>Deployment-Types:</p>
<ul>
<li>Multi-Tenant pricing SKUs: Free, Shared, Basic, Standard, Premium, PremiumV2, and PremiumV3</li>
<li>Single-Tenant pricing SKU:  Isolated</li>
</ul>
<h4 id="multi-tenant-app-service-networking-features">Multi-tenant App Service networking features</h4>
<p>All the roles in an App Service deployment exist in a multi-tenant network. The roles that handle incoming HTTP or HTTPS requests are called <em>front ends</em>. The roles that host the customer workload are called <em>workers</em>.</p>
<p>Because there are many different customers in the same App Service scale unit, you can’t connect the App Service network directly to your network. Instead of connecting the networks, you need features to handle the various aspects of application communication.</p>

<table>
<thead>
<tr>
<th>Inbound features</th>
<th>Outbound features</th>
</tr>
</thead>
<tbody>
<tr>
<td>App-assigned address</td>
<td>Hybrid Connections</td>
</tr>
<tr>
<td>Access restrictions</td>
<td>Gateway-required virtual network integration</td>
</tr>
<tr>
<td>Service endpoints</td>
<td>Virtual network integration</td>
</tr>
<tr>
<td>Private endpoints</td>
<td></td>
</tr>
</tbody>
</table><p>The following inbound use cases are examples of how to use App Service networking features to control traffic inbound to your app.</p>

<table>
<thead>
<tr>
<th>Inbound use case</th>
<th>Feature</th>
</tr>
</thead>
<tbody>
<tr>
<td>Support IP-based SSL needs for your app</td>
<td>App-assigned address</td>
</tr>
<tr>
<td>Support unshared dedicated inbound address for your app</td>
<td>App-assigned address</td>
</tr>
<tr>
<td>Restrict access to your app from a set of well-defined addresses</td>
<td>Access restrictions</td>
</tr>
</tbody>
</table><h4 id="default-networking-behavior">Default networking behavior</h4>
<p>The Free and Shared SKU plans host customer workloads on multitenant workers. The Basic and higher plans host customer workloads that are dedicated to only one App Service plan.</p>
<h5 id="outbound-addresses">Outbound addresses</h5>
<p>The worker VMs are broken down in large part by the App Service plans.</p>
<ul>
<li>The Free, Shared, Basic, Standard, and Premium plans all use the same worker VM type.</li>
<li>The PremiumV2 plan uses another VM type.</li>
<li>PremiumV3 uses yet another VM type.<br>
When you change the VM family, you get a different set of outbound addresses.</li>
</ul>
<p>The outbound addresses used by your app for making outbound calls are listed in the properties for your app. These addresses are shared by all the apps running on the same worker VM family in the App Service deployment.</p>
<h5 id="find-outbound-ips">Find outbound IPs</h5>
<p>To find the outbound IP addresses currently used by your app in the Azure portal, click <strong>Properties</strong> in your app’s left-hand navigation. If you want to see all the addresses that your app might use in a scale unit, there’s a property called <code>possibleOutboundAddresses</code> that will list them.</p>
<h3 id="configure-web-app-settings">Configure web app settings</h3>
<h4 id="configure-application-settings">Configure application settings</h4>
<p>In App Service, app settings are variables passed as environment variables to the application code.</p>
<p><img src="https://docs.microsoft.com/en-us/learn/wwl-azure/configure-web-app-settings/media/configure-app-settings.png" alt="Navigating to Configuration > Application settings"></p>
<p>For <a href="http://ASP.NET">ASP.NET</a> and <a href="http://ASP.NET">ASP.NET</a> Core developers, setting app settings in App Service are like setting them in <code>&lt;appSettings&gt;</code> in <em>Web.config</em> or <em>appsettings.json</em>, but the values in App Service override the ones in <em>Web.config</em> or <em>appsettings.json</em>.</p>
<p>App settings are always encrypted when stored (encrypted-at-rest).</p>
<h4 id="adding-and-editing-settings">Adding and editing settings</h4>
<p><img src="https://docs.microsoft.com/en-us/learn/wwl-azure/configure-web-app-settings/media/app-configure-slotsetting.png" alt="Selecting deployment slot setting to stick the setting to the current slot."></p>
<p>If you are using deployment slots you can specify if your setting is swappable or not. In the dialog, you can stick the setting to the current slot.</p>
<h5 id="editing-application-settings-in-bulk">Editing application settings in bulk</h5>
<p>To add or edit app settings in bulk, click the <strong>Advanced</strong> edit button.</p>
<pre><code>[
  {
    "name": "&lt;key-1&gt;",
    "value": "&lt;value-1&gt;",
    "slotSetting": false
  },
  {
    "name": "&lt;key-2&gt;",
    "value": "&lt;value-2&gt;",
    "slotSetting": false
  },
  ...
]
</code></pre>
<h3 id="configure-general-settings">Configure general settings</h3>
<ul>
<li><strong>Stack settings</strong>: The software stack to run the app, including the language and SDK versions.</li>
</ul>
<p><img src="https://docs.microsoft.com/en-us/learn/wwl-azure/configure-web-app-settings/media/open-general-linux.png" alt="Establishing the stack settings which includes the programming language."></p>
<ul>
<li><strong>Platform settings</strong>: Lets you configure settings for the hosting platform</li>
<li><strong>Debugging</strong>: Enable remote debugging for <a href="http://ASP.NET">ASP.NET</a>, <a href="http://ASP.NET">ASP.NET</a> Core, or Node.js apps.</li>
<li><strong>Incoming client certificates</strong>: require client certificates in mutual authentication.</li>
</ul>
<h3 id="configure-path-mappings">Configure path mappings</h3>
<p>In the <strong>Configuration &gt; Path mappings</strong> section you can configure handler mappings, and virtual application and directory mappings. The <strong>Path mappings</strong> page will display different options based on the OS type.</p>
<h4 id="windows-apps-uncontainerized">Windows apps (uncontainerized)</h4>
<p>For Windows apps, you can customize the IIS handler mappings and virtual applications and directories. Handler mappings let you add custom script processors to handle requests for specific file extensions.</p>
<ul>
<li><strong>Extension</strong>: The file extension you want to handle, such as *<em>.php</em> or <em>handler.fcgi</em>.</li>
<li><strong>Script processor</strong>: The absolute path of the script processor.</li>
<li><strong>Arguments</strong>: Optional command-line arguments for the script processor.</li>
</ul>
<h4 id="linux-and-containerized-apps">Linux and containerized apps</h4>
<p>You can add custom storage for your containerized app.</p>
<h3 id="enable-diagnostic-logging">Enable diagnostic logging</h3>
<p>| Type | Platform | Location | Description |<br>
| Application logging | Windows, Linux | App Service file system and/or Azure Storage blobs | Logs messages generated by your application code. Each message is assigned one of the following categories: <strong>Critical</strong>, <strong>Error</strong>, <strong>Warning</strong>, <strong>Info</strong>, <strong>Debug</strong>, and <strong>Trace</strong>. |<br>
| Web server logging | Windows | App Service file system or Azure Storage blobs | Raw HTTP request data in the W3C extended log file format.  |<br>
| Detailed error logging | Windows | App Service file system | Copies of the <em>.html</em> error pages that would have been sent to the client browser. |<br>
| Failed request tracing | Windows | App Service file system | Detailed tracing information on failed requests  |<br>
| Deployment logging | Windows, Linux | App Service file system | Helps determine why a deployment failed. Deployment logging happens automatically and there are no configurable settings for deployment logging. |</p>
<h4 id="enable-application-logging-windows">Enable application logging (Windows)</h4>
<p>To enable application logging for Windows apps in the Azure portal, navigate to your app and select <strong>App Service logs</strong>. Available Options:</p>
<ul>
<li>Application Logging (Filesystem)</li>
<li>Application Logging (Blob),</li>
</ul>
<h4 id="enable-application-logging-linuxcontainer">Enable application logging (Linux/Container)</h4>
<p>Available Options:</p>
<ul>
<li>File System</li>
</ul>
<h4 id="enable-web-server-logging">Enable web server logging</h4>
<p>Available Options:</p>
<ul>
<li>Filesystem)</li>
<li>Blob</li>
</ul>
<h4 id="add-log-messages-in-code">Add log messages in code</h4>
<p>In your application code, you use the usual logging facilities to send log messages to the application logs.</p>
<h4 id="stream-logs">Stream logs</h4>
<p>Before you stream logs in real time, enable the log type that you want. Any information written to files ending in .txt, .log, or .htm that are stored in the <code>/LogFiles</code> directory (<code>d:/home/logfiles</code>) is streamed by App Service. Places, where logs can be streamed:</p>
<ul>
<li>Azure portal</li>
<li>Azure CLI</li>
<li>Local console</li>
</ul>
<h4 id="access-log-files">Access log files</h4>
<p>If you configure the Azure Storage blobs option for a log type, you need a client tool that works with Azure Storage.</p>
<p>For logs stored in the App Service file system, the easiest way is to download the ZIP file in the browser</p>
<h3 id="configure-security-certificates">Configure security certificates</h3>
<p>A certificate uploaded into an app is stored in a deployment unit that is bound to the app service plan’s resource group and region combination (internally called a <em>webspace</em>). This makes the certificate accessible to other apps in the same resource group and region combination.</p>
<ul>
<li>Create a free App Service managed certificate</li>
<li>Purchase an App Service certificate</li>
<li>Import a certificate from Key Vault</li>
<li>Upload a private certificate</li>
<li>Upload a public certificate: Public certificates are not used to secure custom domains, but you can load them into your code if you need them to access remote resources.</li>
</ul>
<h4 id="private-certificate-requirements">Private certificate requirements</h4>
<p>The free <strong>App Service managed certificate</strong> and the <strong>App Service certificate</strong> already satisfy the requirements of App Service. If you want to use a private certificate in App Service, your certificate must meet the following requirements:</p>
<ul>
<li>Exported as a password-protected PFX file, encrypted using triple DES.</li>
<li>Contains private key at least 2048 bits long</li>
<li>Contains all intermediate certificates in the certificate chain</li>
</ul>
<p>To secure a custom domain in a TLS binding, the certificate has additional requirements:</p>
<ul>
<li>Contains an Extended Key Usage for server authentication (OID = 1.3.6.1.5.5.7.3.1)</li>
<li>Signed by a trusted certificate authority</li>
</ul>
<h4 id="creating-a-free-managed-certificate">Creating a free managed certificate</h4>
<p>To create custom TLS/SSL bindings or enable client certificates for your App Service app, your App Service plan must be in the <strong>Basic</strong>, <strong>Standard</strong>, <strong>Premium</strong>, or <strong>Isolated</strong> tier.</p>
<p>It’s a TLS/SSL server certificate that’s fully managed by App Service and renewed continuously. You create the certificate and bind it to a custom domain, and let App Service do the rest.</p>
<h4 id="import-an-app-service-certificate">Import an App Service Certificate</h4>
<p>If you purchase an App Service Certificate from Azure, Azure manages the following tasks:</p>
<ul>
<li>Takes care of the purchase process from GoDaddy.</li>
<li>Performs domain verification of the certificate.</li>
<li>Maintains the certificate in Azure Key Vault.</li>
<li>Manages certificate renewal.</li>
<li>Synchronize the certificate automatically with the imported copies in App Service apps.</li>
</ul>
<h4 id="upload-a-private-certificate">Upload a private certificate</h4>
<p>If your certificate authority gives you multiple certificates in the certificate chain, you need to merge the certificates in order. Then you can Export your merged TLS/SSL certificate with the private key that your certificate request was generated with.</p>
<h4 id="enforce-https">Enforce HTTPS</h4>
<p>By default, anyone can still access your app using HTTP. You can redirect all HTTP requests to the HTTPS port.</p>
<p><img src="https://docs.microsoft.com/en-us/learn/wwl-azure/configure-web-app-settings/media/enforce-https.png" alt="Enabling HTTPS Only in your web app."></p>
<h3 id="manage-app-features">Manage app features</h3>
<p>Feature management is a modern software-development practice that decouples feature release from code deployment and enables quick changes to feature availability on demand. It uses a technique called feature flags (also known as feature toggles, feature switches, and so on) to dynamically administer a feature’s lifecycle.</p>
<h4 id="basic-concepts">Basic concepts</h4>
<ul>
<li><strong>Feature flag</strong>: The state of the feature flag triggers whether the code block runs or not.</li>
<li><strong>Feature manager</strong>: A feature manager is an application package that handles the lifecycle of all the feature flags in an application.</li>
<li><strong>Filter</strong>: A filter is a rule for evaluating the state of a feature flag.</li>
</ul>
<p>An effective implementation of feature management consists of at least two components working in concert:</p>
<ul>
<li>An application that makes use of feature flags.</li>
<li>A separate repository that stores the feature flags and their current states.</li>
</ul>
<h4 id="feature-flag-usage-in-code">Feature flag usage in code</h4>
<pre><code>if (featureFlag) {
    // Run the following code
}
</code></pre>
<h4 id="feature-flag-declaration">Feature flag declaration</h4>
<p>Each feature flag has two parts: a name and a list of one or more filters that are used to evaluate if a feature’s state is <em>on</em> (that is, when its value is <code>True</code>). A filter defines a use case for when a feature should be turned on.</p>
<p>The feature manager supports <em>appsettings.json</em> as a configuration source for feature flags.</p>
<h4 id="feature-flag-repository">Feature flag repository</h4>
<p>To use feature flags effectively, you need to externalize all the feature flags used in an application. This approach allows you to change feature flag states without modifying and redeploying the application itself. Azure App Configuration is designed to be a centralized repository for feature flags.</p>
<h2 id="scale-apps-in-azure-app-service">Scale apps in Azure App Service</h2>
<h3 id="examine-autoscale-factors">Examine autoscale factors</h3>
<h4 id="what-is-autoscaling">What is autoscaling?</h4>
<p>Autoscaling is a cloud system or process that adjusts available resources based on the current demand. Autoscaling performs scaling <em>in and out</em>, as opposed to scaling <em>up and down</em>. Autoscaling can be triggered according to a schedule, or by assessing whether the system is running short on resources.</p>
<h4 id="azure-app-service-autoscaling">Azure App Service Autoscaling</h4>
<p>Autoscaling in Azure App Service monitors the resource metrics of a web app as it runs. It detects situations where additional resources are required to handle an increasing workload, and ensures those resources are available before the system becomes overloaded.</p>
<h5 id="autoscaling-rules">Autoscaling rules</h5>
<p>Autoscaling makes its decisions based on rules that you define. A rule specifies the threshold for a metric, and triggers an autoscale event when this threshold is crossed. Autoscaling can also deallocate resources when the workload has diminished.</p>
<h4 id="when-should-you-consider-autoscaling">When should you consider autoscaling?</h4>
<p>Autoscaling provides elasticity for your services. It’s a suitable solution when hosting any application when you can’t easily predict the workload in advance, or when the workload is likely to vary by date or time.</p>
<p>Autoscaling improves availability and fault tolerance.</p>
<p>Autoscaling works by adding or removing web servers. If your web apps perform resource-intensive processing as part of each request, then autoscaling might not be an effective approach. In these situations, manually scaling up may be necessary.</p>
<p>Autoscaling isn’t the best approach to handling long-term growth. Autoscaling has an overhead associated with monitoring resources and determining whether to trigger a scaling event. In this scenario, if you can anticipate the rate of growth, manually scaling the system over time may be a more cost effective approach.</p>
<p>The number of instances of a service is also a factor. The fewer the number of instances initially, the less capacity you have to handle an increasing workload while autoscaling spins up additional instances.</p>
<h3 id="identify-autoscale-factors">Identify autoscale factors</h3>
<p>Autoscaling enables you to specify the conditions under which a web app should be scaled out, and back in again:</p>
<ul>
<li>Scale based on a metric, such as the length of the disk queue, or the number of HTTP requests awaiting processing.</li>
<li>Scale to a specific instance count according to a schedule. For example, you can arrange to scale out at a particular time of day, or on a specific date or day of the week. You also specify an end date, and the system will scale back in at this time.</li>
</ul>
<p>When the web app scales out, Azure starts new instances of the hardware defined by the App Service Plan to the app.<br>
To prevent runaway autoscaling, an App Service Plan has an instance limit.</p>
<h4 id="metrics-for-autoscale-rules">Metrics for autoscale rules</h4>
<ul>
<li><strong>CPU Percentage</strong></li>
<li><strong>Memory Percentage</strong></li>
<li><strong>Disk Queue Length</strong></li>
<li><strong>Http Queue Length</strong></li>
<li><strong>Data In</strong>. This metric is the number of bytes received across all instances.</li>
<li><strong>Data Out</strong>. This metric is the number of bytes sent by all instances.</li>
</ul>
<h4 id="how-an-autoscale-rule-analyzes-metrics">How an autoscale rule analyzes metrics</h4>
<p>Autoscaling works by analyzing trends in metric values over time across all instances.</p>
<p>In the first step, an autoscale rule aggregates the values retrieved for a metric for all instances across a period of time known as the <em>time grain</em>.Each metric has its own intrinsic time grain, but in most cases this period is 1 minute. The aggregated value is known as the <em>time aggregation</em>. The options available are <em>Average</em>, <em>Minimum</em>, <em>Maximum</em>, <em>Total</em>, <em>Last</em>, and <em>Count</em>.</p>
<p>autoscale rule performs a second step that performs a further aggregation of the value calculated by the <em>time aggregation</em> over a longer, user-specified period, known as the <em>Duration</em>. The minimum <em>Duration</em> is 5 minutes.</p>
<p>The aggregation calculation for the <em>Duration</em> can be different from that of the <em>time grain</em>.</p>
<h4 id="autoscale-actions">Autoscale actions</h4>
<p>When an autoscale rule detects that a metric has crossed a threshold, it can perform an autoscale action. An autoscale action can be <em>scale-out</em> or <em>scale-in</em>.</p>
<p>An autoscale action has a <em>cool down</em> period, specified in minutes. During this interval, the scale rule won’t be triggered again. Remember that it takes time to start up or shut down instances, and so any metrics gathered might not show any significant changes for several minutes. The minimum cool down period is five minutes.</p>
<h4 id="pairing-autoscale-rules">Pairing autoscale rules</h4>
<p>You should plan for scaling-in when a workload decreases. Consider defining autoscale rules in pairs in the same autoscale condition.</p>
<h4 id="combining-autoscale-rules">Combining autoscale rules</h4>
<p>A single autoscale condition can contain several autoscale rules (for example, a scale-out rule and the corresponding scale-in rule). However, the autoscale rules in an autoscale condition don’t have to be directly related. When determining whether to scale out, the autoscale action will be performed if <strong>any</strong> of the scale-out rules are met. When scaling in, the autoscale action will run <strong>only if all</strong> of the scale-in rules are met.</p>
<h3 id="enable-autoscale-in-app-service">Enable autoscale in App Service</h3>
<h4 id="enable-autoscaling">Enable autoscaling</h4>
<p>To get started with autoscaling navigate to your App Service plan in the Azure portal and select <strong>Scale out (App Service plan)</strong> in the <strong>Settings</strong> group in the left navigation pane. Selecting <strong>Custom autoscale</strong> reveals condition groups you can use to manage your scale settings.</p>
<p><img src="https://docs.microsoft.com/en-us/learn/wwl-azure/scale-apps-app-service/media/enable-autoscale.png" alt="Enabling autoscale"></p>
<h4 id="add-scale-conditions">Add scale conditions</h4>
<p>Once you enable autoscaling, you can edit the automatically created default scale condition, and you can add your own custom scale conditions.</p>
<p><img src="https://docs.microsoft.com/en-us/learn/wwl-azure/scale-apps-app-service/media/autoscale-conditions.png" alt="The condition page for an App Service Plan showing the default scale condition."></p>
<h4 id="create-scale-rules">Create scale rules</h4>
<p><img src="https://docs.microsoft.com/en-us/learn/wwl-azure/scale-apps-app-service/media/autoscale-rules.png" alt="Create scale rules"></p>
<h4 id="monitor-autoscaling-activity">Monitor autoscaling activity</h4>
<p>The Azure portal enables you to track when autoscaling has occurred through the <strong>Run history</strong> chart.</p>
<p><img src="https://docs.microsoft.com/en-us/learn/wwl-azure/scale-apps-app-service/media/autoscale-run-history.png" alt=""></p>
<p>You can use the <strong>Run history</strong> chart in conjunction with the metrics shown on the <strong>Overview</strong> page to correlate the autoscaling events with resource utilization.</p>
<p><img src="https://docs.microsoft.com/en-us/learn/wwl-azure/scale-apps-app-service/media/service-plan-metrics.png" alt="The metrics shown on the App Service Plan overview page."></p>
<h3 id="explore-autoscale-best-practices">Explore autoscale best practices</h3>
<ul>
<li><strong>Ensure the maximum and minimum values are different and have an adequate margin between them</strong></li>
<li><strong>Choose the appropriate statistic for your diagnostics metric</strong></li>
<li><strong>Choose the thresholds carefully for all metric types</strong>: Don’t define rules like this, because they will lead to a constant scaling out and scaling back in: Increase instances by one count when Thread Count &gt;= 600, Decrease instances by one count when Thread Count &lt;= 600</li>
<li><strong>Considerations for scaling when multiple rules are configured in a profile</strong>: On  <em>scale out</em>, autoscale runs if any rule is met. On  <em>scale-in</em>, autoscale require all rules to be met.</li>
<li><strong>Always select a safe default instance count</strong></li>
<li><strong>Configure autoscale notifications</strong>: Autoscale will post to the Activity Log if any of the following conditions occur:
<ul>
<li>Autoscale issues a scale operation</li>
<li>Autoscale service successfully completes a scale action</li>
<li>Autoscale service fails to take a scale action.</li>
<li>Metrics aren’t available for autoscale service to make a scale decision.</li>
<li>Metrics are available (recovery) again to make a scale decision.<br>
In addition to using activity log alerts, you can also configure email or webhook notifications to get notified.</li>
</ul>
</li>
</ul>
<h2 id="explore-azure-app-service-deployment-slots">Explore Azure App Service deployment slots</h2>
<h3 id="explore-staging-environments">Explore staging environments</h3>
<p>You can use a separate deployment slot instead of the default production slot when you’re running in the <strong>Standard</strong>, <strong>Premium</strong>, or <strong>Isolated</strong> App Service plan tier. Deployment slots are live apps with their own host names.</p>
<p>Deploying your application to a non-production slot has the following benefits:</p>
<ul>
<li>You can validate app changes in a staging deployment slot before swapping it with the production slot.</li>
<li>Deploying an app to a slot first and swapping it into production makes sure that all instances of the slot are warmed up before being swapped into production.</li>
<li>After a swap, the slot with previously staged app now has the previous production app. If the changes swapped into the production slot aren’t as you expect, you can perform the same swap immediately to get your “last known good site” back.</li>
</ul>
<p>Each App Service plan tier supports a different number of deployment slots. There’s no additional charge for using deployment slots.</p>
<h3 id="examine-slot-swapping">Examine slot swapping</h3>
<p>When you swap slots (for example, from a staging slot to the production slot), App Service does the following to ensure that the target slot doesn’t experience downtime:</p>
<ol>
<li>
<p>Apply the following settings from the target slot (for example, the production slot) to all instances of the source slot:</p>
<ul>
<li>Slot-specific app settings and connection strings, if applicable.</li>
<li>Continuous deployment settings, if enabled.</li>
<li>App Service authentication settings, if enabled.</li>
</ul>
<p>Any of these cases trigger all instances in the source slot to restart. During  <strong>swap with preview</strong>, this marks the end of the first phase. The swap operation is paused, and you can validate that the source slot works correctly with the target slot’s settings.</p>
</li>
<li>
<p>Wait for every instance in the source slot to complete its restart.</p>
</li>
<li>
<p>If local cache is enabled, trigger local cache initialization by making an HTTP request to the application root ("/") on each instance of the source slot.</p>
</li>
<li>
<p>If auto swap is enabled with custom warm-up, trigger Application Initiation by making an HTTP request to the application root ("/") on each instance of the source slot.</p>
</li>
<li>
<p>If all instances on the source slot are warmed up successfully, swap the two slots by switching the routing rules for the two slots.</p>
</li>
<li>
<p>Now that the source slot has the pre-swap app previously in the target slot, perform the same operation by applying all settings and restarting the instances.</p>
</li>
</ol>
<p>| Settings that are swapped                                           | Settings that aren’t swapped                 |<br>
| ------------------------------------------------------------------- | -------------------------------------------- | Settings that aren’t swapped |<br>
|–|--|<br>
| General settings, such as framework version, 32/64-bit, web sockets | Publishing endpoints                         |<br>
| App settings (can be configured to stick to a slot)                 | Custom domain names                          |<br>
| Connection strings (can be configured to stick to a slot)           | Non-public certificates and TLS/SSL settings |<br>
| Handler mappings                                                    | Scale settings                               |<br>
| Public certificates                                                 | WebJobs schedulers                           |<br>
| WebJobs content                                                     | IP restrictions                              |<br>
| Hybrid connections                                                  | Always On                                    |<br>
| Virtual network integration                                        | Scale settings |<br>
| Public certificates | WebJobs schedulers |<br>
| WebJobs content | IP restrictions |<br>
| Hybrid connections | Always On |<br>
| Virtual network integration | Diagnostic log settings                      |<br>
| Service endpoints                                                   | Cross-origin resource sharing (CORS)         |<br>
| Azure Content Delivery Network                                      |                                             | |</p>
<p>Settings can be made swappable or be prevented from swapping via configuration.</p>
<h3 id="swap-deployment-slots">Swap deployment slots</h3>
<h4 id="manually-swapping-deployment-slots">Manually swapping deployment slots</h4>
<ol>
<li>Go to your app’s  <strong>Deployment slots</strong>  page and select  <strong>Swap</strong>.</li>
<li>Select the desired <strong>Source</strong> and <strong>Target</strong> slots. Usually, the target is the production slot. Also, select the <strong>Source Changes</strong> and <strong>Target Changes</strong> tabs and verify that the configuration changes are expected.</li>
</ol>
<h5 id="swap-with-preview-multi-phase-swap">Swap with preview (multi-phase swap)</h5>
<p>Before you swap into production as the target slot, validate that the app runs with the swapped settings.</p>
<ol>
<li>Follow the steps above in Swap deployment slots but select  <strong>Perform swap with preview</strong>. The dialog box shows you how the configuration in the source slot changes in phase 1, and how the source and target slot change in phase 2.</li>
<li>When you’re ready to start the swap, select <strong>Start Swap</strong>. When phase 1 finishes, you’re notified in the dialog box. Preview the swap in the source slot by going to <code>https://&lt;app_name&gt;-&lt;source-slot-name&gt;.azurewebsites.net</code>.</li>
<li>When you’re ready to complete the pending swap, select <strong>Complete Swap</strong> in <strong>Swap action</strong> and select <strong>Complete Swap</strong>.</li>
</ol>
<h4 id="configure-auto-swap">Configure auto swap</h4>
<p>Auto swap streamlines Azure DevOps scenarios where you want to deploy your app continuously with zero cold starts and zero downtime for customers of the app.</p>
<h4 id="specify-custom-warm-up">Specify custom warm-up</h4>
<p>Some apps might require custom warm-up actions before the swap. The <code>applicationInitialization</code> configuration element in web.config lets you specify custom initialization actions.</p>
<pre><code>&lt;system.webServer&gt;
 &lt;applicationInitialization&gt;
   &lt;add initializationPage="/" hostName="[app hostname]" /&gt;
   &lt;add initializationPage="/Home/About" hostName="[app hostname]" /&gt;
 &lt;/applicationInitialization&gt; 
&lt;/system.webServer&gt;
</code></pre>
<h4 id="roll-back-and-monitor-a-swap">Roll back and monitor a swap</h4>
<p>If any errors occur in the target slot (for example, the production slot) after a slot swap, restore the slots to their pre-swap states by swapping the same two slots immediately.</p>
<h3 id="route-traffic-in-app-service">Route traffic in App Service</h3>
<p>By default, all client requests to the app’s production URL (<code>http://&lt;app_name&gt;.azurewebsites.net</code>) are routed to the production slot. You can route a portion of the traffic to another slot. This feature is useful if you need user feedback for a new update, but you’re not ready to release it to production.</p>
<h4 id="route-production-traffic-automatically">Route production traffic automatically</h4>
<p>In the  <strong>Traffic %</strong>  column of the slot you want to route to, specify a percentage (between 0 and 100) to represent the amount of total traffic you want to route. Select  <strong>Save</strong>.</p>
<p>After a client is automatically routed to a specific slot, it’s “pinned” to that slot for the life of that client session. On the client browser, you can see which slot your session is pinned to by looking at the <code>x-ms-routing-name</code> cookie in your HTTP headers. A request that’s routed to the “staging” slot has the cookie <code>x-ms-routing-name=staging</code>. A request that’s routed to the production slot has the cookie <code>x-ms-routing-name=self</code>.</p>
<h4 id="route-production-traffic-manually">Route production traffic manually</h4>
<p>In addition to automatic traffic routing, App Service can route requests to a specific slot. This is useful when you want your users to be able to opt in to or opt out of your beta app. To route production traffic manually, you use the <code>x-ms-routing-name</code> query parameter.</p>
<p>To let users opt out of your beta app, for example, you can put this link on your webpage:</p>
<pre><code>&lt;a href="&lt;webappname&gt;.azurewebsites.net/?x-ms-routing-name=self"&gt;Go back to production app&lt;/a&gt;
</code></pre>
<p>By default, new slots are given a routing rule of <code>0%</code>, a default value is displayed in grey. When you explicitly set this value to <code>0%</code> it is displayed in black, your users can access the staging slot manually by using the <code>x-ms-routing-name</code> query parameter.</p>
<h1 id="az-204-implement-azure-functions">AZ-204: Implement Azure Functions</h1>
<h2 id="explore-azure-functions">Explore Azure Functions</h2>
<h4 id="discover-azure-functions">Discover Azure Functions</h4>
<p>Azure Functions are a great solution for processing data, integrating systems, working with the internet-of-things (IoT), and building simple APIs and microservices. Consider Functions for tasks like image or order processing, file maintenance, or for any tasks that you want to run on a schedule. Functions provides templates to get you started with key scenarios.</p>
<p>Azure Functions supports <em>triggers</em>, which are ways to start execution of your code, and <em>bindings</em>, which are ways to simplify coding for input and output data.</p>
<h4 id="compare-azure-functions-and-azure-logic-apps">Compare Azure Functions and Azure Logic Apps</h4>
<p>Both Functions and Logic Apps enable serverless workloads. Azure Functions is a serverless compute service, whereas Azure Logic Apps provides serverless workflows. Both can create complex orchestrations. An orchestration is a collection of functions or steps, called actions in Logic Apps, that are executed to accomplish a complex task.</p>
<p>For Azure Functions, you develop orchestrations by writing code and using the Durable Functions extension.<br>
For Logic Apps, you create orchestrations by using a GUI or editing configuration files.</p>

<table>
<thead>
<tr>
<th></th>
<th>Azure Functions</th>
<th>Logic Apps</th>
</tr>
</thead>
<tbody>
<tr>
<td>Development</td>
<td>Code-first (imperative)</td>
<td>Designer-first (declarative)</td>
</tr>
<tr>
<td>Connectivity</td>
<td>About a dozen built-in binding types, write code for custom bindings</td>
<td>Large collection of connectors,</td>
</tr>
<tr>
<td>Actions</td>
<td>Each activity is an Azure function; write code for activity functions</td>
<td>Large collection of ready-made actions</td>
</tr>
<tr>
<td>Monitoring</td>
<td>Azure Application Insights</td>
<td>Azure portal, Azure Monitor logs</td>
</tr>
<tr>
<td>Management</td>
<td>REST API, Visual Studio</td>
<td>Azure portal, REST API, PowerShell, Visual Studio</td>
</tr>
</tbody>
</table><h4 id="compare-functions-and-webjobs">Compare Functions and WebJobs</h4>
<p>Like Azure Functions, Azure App Service WebJobs with the WebJobs SDK is a code-first integration service that is designed for developers.</p>
<p>Azure Functions is built on the WebJobs SDK, so it shares many of the same event triggers and connections to other Azure services.</p>
<p>| | Functions | WebJobs with WebJobs SDK |<br>
| Serverless app model with automatic scaling | Yes | No |<br>
| Develop and test in browser | Yes | No<br>
| Pay-per-use pricing | Yes | No<br>
| Integration with Logic Apps | Yes | No<br>
| Trigger events | Timer<br>
Azure Storage queues and blobs<br>
Azure Service Bus queues and topics<br>
Azure Cosmos DB<br>
Azure Event Hubs<br>
HTTP/WebHook (GitHub  Slack)<br>
Azure Event Grid | Timer<br>
Azure Storage queues and blobs<br>
Azure Service Bus queues and topics<br>
Azure Cosmos DB<br>
Azure Event Hubs<br>
File system |</p>
<p>For most scenarios, Azure Functions is the best choice.</p>
<h3 id="compare-azure-functions-hosting-options">Compare Azure Functions hosting options</h3>
<p>There are three basic hosting plans available for Azure Functions: Consumption plan, Functions Premium plan, and App service (Dedicated) plan.</p>
<p>The hosting plan you choose dictates the following behaviors:</p>
<ul>
<li>How your function app is scaled.</li>
<li>The resources available to each function app instance.</li>
<li>Support for advanced functionality, such as Azure Virtual Network connectivity.</li>
</ul>

<table>
<thead>
<tr>
<th>Plan</th>
<th>Benefits</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Consumption plan</strong></td>
<td>This is the default hosting plan. It scales automatically and you only pay for compute resources when your functions are running. Instances of the Functions host are dynamically added and removed based on the number of incoming events.</td>
</tr>
<tr>
<td><strong>Premium plan</strong></td>
<td>Automatically scales based on demand using pre-warmed workers, which run applications with no delay after being idle, runs on more powerful instances, and connects to virtual networks.</td>
</tr>
<tr>
<td><strong>Dedicated plan</strong></td>
<td>Run your functions within an App Service plan at regular App Service plan rates. Best for long-running scenarios where Durable Functions can’t be used.</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>Hosting option</th>
<th>Details</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>ASE</strong></td>
<td>App Service Environment (ASE) is an App Service feature that provides a fully isolated and dedicated environment for securely running App Service apps at high scale.</td>
</tr>
<tr>
<td><strong>Kubernetes</strong></td>
<td>Kubernetes provides a fully isolated and dedicated environment running on top of the Kubernetes platform.</td>
</tr>
</tbody>
</table><h4 id="always-on">Always on</h4>
<p>If you run on a Dedicated plan, you should enable the  <strong>Always on</strong>  setting so that your function app runs correctly. On an App Service plan, the functions runtime goes idle after a few minutes of inactivity, so only HTTP triggers will “wake up” your functions. Always on is available only on an App Service plan. On a Consumption plan, the platform activates function apps automatically.</p>
<h4 id="storage-account-requirements">Storage account requirements</h4>
<p>On any plan, a function app requires a general Azure Storage account, which supports Azure Blob, Queue, Files, and Table storage. This is because Functions rely on Azure Storage for operations such as managing triggers and logging function executions, but some storage accounts don’t support queues and tables.</p>
<h3 id="scale-azure-functions">Scale Azure Functions</h3>
<p>In the Consumption and Premium plans, Azure Functions scales CPU and memory resources by adding additional instances of the Functions host. The number of instances is determined on the number of events that trigger a function.</p>
<p>Each instance of the Functions host in the Consumption plan is limited to 1.5 GB of memory and one CPU.<br>
In the Premium plan, the plan size determines the available memory and CPU for all apps in that plan on that instance.</p>
<h4 id="runtime-scaling">Runtime scaling</h4>
<p>Azure Functions uses a component called the <em>scale controller</em> to monitor the rate of events and determine whether to scale out or scale in. The scale controller uses heuristics for each trigger type.</p>
<p>The unit of scale for Azure Functions is the function app. When the function app is scaled out, additional resources are allocated to run multiple instances of the Azure Functions host.</p>
<p><img src="https://docs.microsoft.com/en-us/learn/wwl-azure/explore-azure-functions/media/central-listener.png" alt="Scale controller monitoring events and creating instances"></p>
<h4 id="scaling-behaviors">Scaling behaviors</h4>
<ul>
<li>
<p><strong>Maximum instances:</strong>  A single function app only scales out to a maximum of 200 instances. A single instance may process more than one message or request at a time though, so there isn’t a set limit on number of concurrent executions.</p>
</li>
<li>
<p><strong>New instance rate:</strong>  For HTTP triggers, new instances are allocated, at most, once per second. For non-HTTP triggers, new instances are allocated, at most, once every 30 seconds.</p>
</li>
</ul>
<h4 id="limit-scale-out">Limit scale out</h4>
<p>You may wish to restrict the maximum number of instances an app used to scale out. This is most common for cases where a downstream component like a database has limited throughput. By default, Consumption plan functions scale out to as many as 200 instances, and Premium plan functions will scale out to as many as 100 instances.</p>
<h4 id="azure-functions-scaling-in-an-app-service-plan">Azure Functions scaling in an App service plan</h4>
<p>Using an App Service plan, you can manually scale out by adding more VM instances.</p>
<h3 id="develop-azure-functions">Develop Azure Functions</h3>
<p>A function contains two important pieces - your code, which can be written in a variety of languages, and some config, the <em>function.json</em> file. For compiled languages, this config file is generated automatically from annotations in your code. For scripting languages, you must provide the config file yourself.</p>
<p>The  <em>function.json</em>  file defines the function’s trigger, bindings, and other configuration settings. Every function has one and only one trigger. The runtime uses this config file to determine the events to monitor and how to pass data into and return data from a function execution. The following is an example  <em>function.json</em>  file.</p>
<p>JSON</p>
<pre><code>{
    "disabled":false,
    "bindings":[
        // ... bindings here
        {
            "type": "bindingType",
            "direction": "in",
            "name": "myParamName",
            // ... more depending on binding
        }
    ]
}
</code></pre>
<p>The  <code>bindings</code>  property is where you configure both triggers and bindings. Each binding shares a few common settings and some settings which are specific to a particular type of binding. Every binding requires the following settings:</p>

<table>
<thead>
<tr>
<th>Property</th>
<th>Types</th>
<th>Comments</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>type</code></td>
<td>string</td>
<td>Name of binding. For example,  <code>queueTrigger</code>.</td>
</tr>
<tr>
<td><code>direction</code></td>
<td>string</td>
<td>Indicates whether the binding is for receiving data into the function or sending data from the function. For example,  <code>in</code>  or  <code>out</code>.</td>
</tr>
<tr>
<td><code>name</code></td>
<td>string</td>
<td>The name that is used for the bound data in the function. For example,  <code>myQueue</code>.</td>
</tr>
</tbody>
</table><h4 id="function-app">Function app</h4>
<p>A function app provides an execution context in Azure in which your functions run. As such, it is the unit of deployment and management for your functions. A function app is comprised of one or more individual functions that are managed, deployed, and scaled together.</p>
<h4 id="folder-structure">Folder structure</h4>
<p>The code for all the functions in a specific function app is located in a root project folder that contains a host configuration file. The host.json file contains runtime-specific configurations and is in the root folder of the function app. A <em>bin</em> folder contains packages and other library files that the function app requires. Specific folder structures required by the function app depend on language.</p>
<h4 id="local-development-environments">Local development environments</h4>
<p>Your local functions can connect to live Azure services, and you can debug them on your local computer using the full Functions runtime.</p>
<p>Do not mix local development with portal development in the same function app. When you create and publish functions from a local project, you should not try to maintain or modify project code in the portal.</p>
<h3 id="create-triggers-and-bindings">Create triggers and bindings</h3>
<p>Triggers are what cause a function to run. A trigger defines how a function is invoked and a function must have exactly one trigger. Triggers have associated data, which is often provided as the payload of the function.</p>
<p>Binding to a function is a way of declaratively connecting another resource to the function; bindings may be connected as  <em>input bindings</em>,  <em>output bindings</em>, or both. Data from bindings is provided to the function as parameters. Bindings are optional and a function might have one or multiple input and/or output bindings.</p>
<h4 id="trigger-and-binding-definitions">Trigger and binding definitions</h4>

<table>
<thead>
<tr>
<th>Language</th>
<th>Triggers and bindings are configured by…</th>
</tr>
</thead>
<tbody>
<tr>
<td>C# class library</td>
<td>decorating methods and parameters with C# attributes</td>
</tr>
<tr>
<td>Java</td>
<td>decorating methods and parameters with Java annotations</td>
</tr>
<tr>
<td>JavaScript/PowerShell/Python/TypeScript</td>
<td>updating  <em>function.json</em>  schema</td>
</tr>
</tbody>
</table><h4 id="binding-direction">Binding direction</h4>
<ul>
<li>For triggers, the direction is always  <code>in</code></li>
<li>Input and output bindings use  <code>in</code>  and  <code>out</code></li>
<li>Some bindings support a special direction  <code>inout</code>. If you use  <code>inout</code>, only the  <strong>Advanced editor</strong>  is available via the  <strong>Integrate</strong>  tab in the portal.</li>
</ul>
<h4 id="azure-functions-trigger-and-binding-example">Azure Functions trigger and binding example</h4>
<p>Suppose you want to write a new row to Azure Table storage whenever a new message appears in Azure Queue storage. This scenario can be implemented using an Azure Queue storage trigger and an Azure Table storage output binding.</p>
<p>Here’s a  <em>function.json</em>  file for this scenario.</p>
<p>JSON</p>
<pre><code>{
  "bindings": [
    {
      "type": "queueTrigger",
      "direction": "in",
      "name": "order",
      "queueName": "myqueue-items",
      "connection": "MY_STORAGE_ACCT_APP_SETTING"
    },
    {
      "type": "table",
      "direction": "out",
      "name": "$return",
      "tableName": "outTable",
      "connection": "MY_TABLE_STORAGE_ACCT_APP_SETTING"
    }
  ]
}
</code></pre>
<h5 id="c-script-example">C# script example</h5>
<pre><code>#r "Newtonsoft.Json"

using Microsoft.Extensions.Logging;
using Newtonsoft.Json.Linq;

// From an incoming queue message that is a JSON object, add fields and write to Table storage
// The method return value creates a new row in Table Storage
public static Person Run(JObject order, ILogger log)
{
    return new Person() { 
            PartitionKey = "Orders", 
            RowKey = Guid.NewGuid().ToString(),  
            Name = order["Name"].ToString(),
            MobileNumber = order["MobileNumber"].ToString() };  
}

public class Person
{
    public string PartitionKey { get; set; }
    public string RowKey { get; set; }
    public string Name { get; set; }
    public string MobileNumber { get; set; }
}

</code></pre>
<h3 id="javascript-example">JavaScript example</h3>
<pre><code>// From an incoming queue message that is a JSON object, add fields and write to Table Storage
module.exports = async function (context, order) {
    order.PartitionKey = "Orders";
    order.RowKey = generateRandomId(); 

    context.bindings.order = order;
};

function generateRandomId() {
    return Math.random().toString(36).substring(2, 15) +
        Math.random().toString(36).substring(2, 15);
}

</code></pre>
<h3 id="class-library-example">Class library example</h3>
<pre><code>public static class QueueTriggerTableOutput
{
    [FunctionName("QueueTriggerTableOutput")]
    [return: Table("outTable", Connection = "MY_TABLE_STORAGE_ACCT_APP_SETTING")]
    public static Person Run(
        [QueueTrigger("myqueue-items", Connection = "MY_STORAGE_ACCT_APP_SETTING")]JObject order,
        ILogger log)
    {
        return new Person() {
                PartitionKey = "Orders",
                RowKey = Guid.NewGuid().ToString(),
                Name = order["Name"].ToString(),
                MobileNumber = order["MobileNumber"].ToString() };
    }
}

public class Person
{
    public string PartitionKey { get; set; }
    public string RowKey { get; set; }
    public string Name { get; set; }
    public string MobileNumber { get; set; }
</code></pre>
<h3 id="connect-functions-to-azure-services">Connect functions to Azure services</h3>
<p>Your function project does not directly accept the connection details. It references connection information by name from its configuration provider, allowing them to be changed across environments.</p>
<p>The default configuration provider uses environment variables. These might be set by Application Settings when running in the Azure Functions service, or from the local settings filewhen developing locally.</p>
<h4 id="connection-values">Connection values</h4>
<p>When the connection name resolves to a single exact value, the runtime identifies the value as a <em>connection string</em>, which typically includes a secret.</p>
<p>However, a connection name can also refer to a collection of multiple configuration items. Environment variables can be treated as a collection by using a shared prefix that ends in double underscores <code>__</code>. The group can then be referenced by setting the connection name to this prefix.</p>
<p>For example, the <code>connection</code> property for a Azure Blob trigger definition might be <code>Storage1</code>. As long as there is no single string value configured with <code>Storage1</code> as its name, <code>Storage1__serviceUri</code> would be used for the <code>serviceUri</code> property of the connection.</p>
<h4 id="configure-an-identity-based-connection">Configure an identity-based connection</h4>
<p>Some connections in Azure Functions are configured to use an identity instead of a secret. Support depends on the extension using the connection. In some cases, a connection string may still be required in Functions even though the service to which you are connecting supports identity-based connections.</p>
<p>When hosted in the Azure Functions service, identity-based connections use a managed identity.</p>
<h4 id="grant-permission-to-the-identity">Grant permission to the identity</h4>
<p>Whatever identity is being used must have permissions to perform the intended actions. This is typically done by assigning a role in Azure RBAC or specifying the identity in an access policy, depending on the service to which you are connecting.</p>
<h2 id="implement-durable-functions">Implement Durable Functions</h2>
<h3 id="explore-durable-functions-app-patterns">Explore Durable Functions app patterns</h3>
<p><em>Durable Functions</em>  is an extension of Azure Functions that lets you write stateful functions in a serverless compute environment. It lets you define stateful workflows by writing <em>orchestrator functions</em> and stateful entities by writing <em>entity functions</em>.</p>
<h4 id="supported-languages">Supported languages</h4>
<ul>
<li>C#</li>
<li>JavaScript</li>
<li>Python</li>
<li>F#</li>
<li>PowerShell</li>
</ul>
<h4 id="application-patterns">Application patterns</h4>
<p>The primary use case for Durable Functions is simplifying complex, stateful coordination requirements in serverless applications.</p>
<h5 id="function-chaining">Function chaining</h5>
<p>In the function chaining pattern, a sequence of functions executes in a specific order. In this pattern, the output of one function is applied to the input of another function.</p>
<p><img src="https://docs.microsoft.com/en-us/learn/wwl-azure/implement-durable-functions/media/function-chaining.png" alt="Four functions executing in a specific order"></p>
<pre><code>[FunctionName("Chaining")]
public static async Task&lt;object&gt; Run(
    [OrchestrationTrigger] IDurableOrchestrationContext context)
{
    try
    {
        var x = await context.CallActivityAsync&lt;object&gt;("F1", null);
        var y = await context.CallActivityAsync&lt;object&gt;("F2", x);
        var z = await context.CallActivityAsync&lt;object&gt;("F3", y);
        return  await context.CallActivityAsync&lt;object&gt;("F4", z);
    }
    catch (Exception)
    {
        // Error handling or compensation goes here.
    }
}
</code></pre>
<h5 id="fan-outfan-in">Fan out/fan in</h5>
<p>In the fan out/fan in pattern, you execute multiple functions in parallel and then wait for all functions to finish. Often, some aggregation work is done on the results that are returned from the functions.</p>
<p><img src="https://docs.microsoft.com/en-us/learn/wwl-azure/implement-durable-functions/media/fan-out-fan-in.png" alt="A single function that fans out to three functions that execute in parallel and, after operation of all three ends, sending the result to another function."></p>
<p>With normal functions, you can fan out by having the function send multiple messages to a queue. To fan in you write code to track when the queue-triggered functions end, and then store function outputs.</p>
<pre><code>[FunctionName("FanOutFanIn")]
public static async Task Run(
    [OrchestrationTrigger] IDurableOrchestrationContext context)
{
    var parallelTasks = new List&lt;Task&lt;int&gt;&gt;();

    // Get a list of N work items to process in parallel.
    object[] workBatch = await context.CallActivityAsync&lt;object[]&gt;("F1", null);
    for (int i = 0; i &lt; workBatch.Length; i++)
    {
        Task&lt;int&gt; task = context.CallActivityAsync&lt;int&gt;("F2", workBatch[i]);
        parallelTasks.Add(task);
    }

    await Task.WhenAll(parallelTasks);

    // Aggregate all N outputs and send the result to F3.
    int sum = parallelTasks.Sum(t =&gt; t.Result);
    await context.CallActivityAsync("F3", sum);
}
</code></pre>
<h5 id="async-http-apis">Async HTTP APIs</h5>
<p>The async HTTP API pattern addresses the problem of coordinating the state of long-running operations with external clients. A common way to implement this pattern is by having an HTTP endpoint trigger the long-running action. Then, redirect the client to a status endpoint that the client polls to learn when the operation is finished.</p>
<p><img src="https://docs.microsoft.com/en-us/learn/wwl-azure/implement-durable-functions/media/async-http-api.png" alt="An HTTP endpoint triggers the long running action with the client polling for completion status."></p>
<p>Durable Functions provides <strong>built-in support</strong> for this pattern, simplifying or even removing the code you need to write to interact with long-running function executions.</p>
<p>The following example shows REST commands that start an orchestrator and query its status. For clarity, some protocol details are omitted from the example.</p>
<pre><code>&gt; curl -X POST https://myfunc.azurewebsites.net/orchestrators/DoWork -H "Content-Length: 0" -i
HTTP/1.1 202 Accepted
Content-Type: application/json
Location: https://myfunc.azurewebsites.net/runtime/webhooks/durabletask/b79baf67f717453ca9e86c5da21e03ec

{"id":"b79baf67f717453ca9e86c5da21e03ec", ...}

&gt; curl https://myfunc.azurewebsites.net/runtime/webhooks/durabletask/b79baf67f717453ca9e86c5da21e03ec -i
HTTP/1.1 202 Accepted
Content-Type: application/json
Location: https://myfunc.azurewebsites.net/runtime/webhooks/durabletask/b79baf67f717453ca9e86c5da21e03ec

{"runtimeStatus":"Running","lastUpdatedTime":"2019-03-16T21:20:47Z", ...}

&gt; curl https://myfunc.azurewebsites.net/runtime/webhooks/durabletask/b79baf67f717453ca9e86c5da21e03ec -i
HTTP/1.1 200 OK
Content-Length: 175
Content-Type: application/json

{"runtimeStatus":"Completed","lastUpdatedTime":"2019-03-16T21:20:57Z", ...}
</code></pre>
<p>The Durable Functions extension exposes built-in HTTP APIs that manage long-running orchestrations. You can alternatively implement this pattern yourself by using your own function triggers (such as HTTP, a queue, or Azure Event Hubs) and the orchestration client binding.</p>
<pre><code>public static class HttpStart
{
    [FunctionName("HttpStart")]
    public static async Task&lt;HttpResponseMessage&gt; Run(
        [HttpTrigger(AuthorizationLevel.Function, methods: "post", Route = "orchestrators/{functionName}")] HttpRequestMessage req,
        [DurableClient] IDurableClient starter,
        string functionName,
        ILogger log)
    {
        // Function input comes from the request content.
        object eventData = await req.Content.ReadAsAsync&lt;object&gt;();
        string instanceId = await starter.StartNewAsync(functionName, eventData);

        log.LogInformation($"Started orchestration with ID = '{instanceId}'.");

        return starter.CreateCheckStatusResponse(req, instanceId);
    }
}
</code></pre>
<h5 id="monitor">Monitor</h5>
<p>The monitor pattern refers to a flexible, recurring process in a workflow. An example is polling until specific conditions are met.</p>
<p><img src="https://docs.microsoft.com/en-us/learn/wwl-azure/implement-durable-functions/media/monitor.png" alt="A function running until a specific condition is met."></p>
<pre><code>[FunctionName("MonitorJobStatus")]
public static async Task Run(
    [OrchestrationTrigger] IDurableOrchestrationContext context)
{
    int jobId = context.GetInput&lt;int&gt;();
    int pollingInterval = GetPollingInterval();
    DateTime expiryTime = GetExpiryTime();

    while (context.CurrentUtcDateTime &lt; expiryTime)
    {
        var jobStatus = await context.CallActivityAsync&lt;string&gt;("GetJobStatus", jobId);
        if (jobStatus == "Completed")
        {
            // Perform an action when a condition is met.
            await context.CallActivityAsync("SendAlert", machineId);
            break;
        }

        // Orchestration sleeps until this time.
        var nextCheck = context.CurrentUtcDateTime.AddSeconds(pollingInterval);
        await context.CreateTimer(nextCheck, CancellationToken.None);
    }

    // Perform more work here, or let the orchestration end.
}
</code></pre>
<h5 id="human-interaction">Human interaction</h5>
<p>Many automated processes involve some kind of human interaction. Involving humans in an automated process is tricky because people aren’t as highly available and as responsive as cloud services. An automated process might allow for this interaction by using timeouts and compensation logic.</p>
<p><img src="https://docs.microsoft.com/en-us/learn/wwl-azure/implement-durable-functions/media/human-interaction-pattern.png" alt="A process awaiting human interaction before continuing."></p>
<p>You can implement the pattern in this example by using an orchestrator function. The orchestrator uses a durable timer to request approval. The orchestrator escalates if timeout occurs. The orchestrator waits for an external event, such as a notification that’s generated by a human interaction.</p>
<pre><code>[FunctionName("ApprovalWorkflow")]
public static async Task Run(
    [OrchestrationTrigger] IDurableOrchestrationContext context)
{
    await context.CallActivityAsync("RequestApproval", null);
    using (var timeoutCts = new CancellationTokenSource())
    {
        DateTime dueTime = context.CurrentUtcDateTime.AddHours(72);
        Task durableTimeout = context.CreateTimer(dueTime, timeoutCts.Token);

        Task&lt;bool&gt; approvalEvent = context.WaitForExternalEvent&lt;bool&gt;("ApprovalEvent");
        if (approvalEvent == await Task.WhenAny(approvalEvent, durableTimeout))
        {
            timeoutCts.Cancel();
            await context.CallActivityAsync("ProcessApproval", approvalEvent.Result);
        }
        else
        {
            await context.CallActivityAsync("Escalate", null);
        }
    }
}
</code></pre>
<h3 id="discover-the-four-function-types">Discover the four function types</h3>
<h4 id="orchestrator-functions">Orchestrator functions</h4>
<p>Orchestrator functions describe how actions are executed and the order in which actions are executed.</p>
<p>Orchestrator functions are written using ordinary code, but there are strict requirements on how to write the code. Specifically, orchestrator function code must be deterministic.</p>
<p>An orchestration can have many different types of actions, including activity functions, sub-orchestrations, waiting for external events, HTTP, and timers. Orchestrator functions can also interact with entity functions.</p>
<h4 id="activity-functions">Activity functions</h4>
<p>Activity functions are the basic unit of work in a durable function orchestration. For example, you might create an orchestrator function to process an order. The tasks involve checking the inventory, charging the customer, and creating a shipment. Each task would be a separate activity function.</p>
<p>Unlike orchestrator functions, activity functions aren’t restricted in the type of work you can do in them. Activity functions are frequently used to make network calls or run CPU intensive operations.</p>
<p>An activity trigger is used to define an activity function.</p>
<h4 id="entity-functions">Entity functions</h4>
<p>Entity functions define operations for reading and updating small pieces of state. We often refer to these stateful entities as durable entities. Like orchestrator functions, entity functions are functions with a special trigger type,  <em>entity trigger</em>. They can also be invoked from client functions or from orchestrator functions. Unlike orchestrator functions, entity functions do not have any specific code constraints. Entity functions also manage state explicitly rather than implicitly representing state via control flow. Some things to note:</p>
<ul>
<li>Entities are accessed via a unique identifier, the  <em>entity ID</em>. An entity ID is simply a pair of strings that uniquely identifies an entity instance.</li>
<li>Operations on entities require that you specify the  <strong>Entity ID</strong>  of the target entity, and the  <strong>Operation name</strong>, which is a string that specifies the operation to perform.</li>
</ul>
<h4 id="client-functions">Client functions</h4>
<p>What makes a function a client function is its use of the <em>durable client output binding</em>. Orchestrator and entity functions are triggered by their bindings and both of these triggers work by reacting to messages that are enqueued in a task hub. The primary way to deliver these messages is by using an orchestrator client binding, or an entity client binding, from within a <em>client function</em>.</p>
<p>If you want to test an orchestrator or entity function in the Azure portal, you must instead run a client function that starts an orchestrator or entity function as part of its implementation.</p>
<h3 id="explore-task-hubs">Explore task hubs</h3>
<p>A task hub in Durable Functions is a logical container for durable storage resources that are used for orchestrations and entities. Orchestrator, activity, and entity functions can only directly interact with each other when they belong to the same task hub.</p>
<p>If multiple function apps share a storage account, each function app must be configured with a separate task hub name.</p>
<h4 id="azure-storage-resources">Azure Storage resources</h4>
<p>A task hub in Azure Storage consists of the following resources:</p>
<ul>
<li>One or more control queues.</li>
<li>One work-item queue.</li>
<li>One history table.</li>
<li>One instances table.</li>
<li>One storage container containing one or more lease blobs.</li>
<li>A storage container containing large message payloads, if applicable.</li>
</ul>
<p>All of these resources are created automatically in the configured Azure Storage account when orchestrator, entity, or activity functions run or are scheduled to run.</p>
<h4 id="task-hub-names">Task hub names</h4>
<p>Task hubs in Azure Storage are identified by a name that conforms to these rules:</p>
<ul>
<li>Contains only alphanumeric characters</li>
<li>Starts with a letter</li>
<li>Has a minimum length of 3 characters, maximum length of 45 characters</li>
</ul>
<p>The task hub name is declared in the  <em>host.json</em>  file, as shown in the following example:</p>
<pre><code>{
  "version": "2.0",
  "extensions": {
    "durableTask": {
      "hubName": "MyTaskHub"
    }
  }
}
</code></pre>
<h3 id="explore-durable-orchestrations">Explore durable orchestrations</h3>
<p>You can use an  <em>orchestrator function</em>  to orchestrate the execution of other Durable functions within a function app. Orchestrator functions have the following characteristics:</p>
<ul>
<li>Orchestrator functions define function workflows using procedural code. No declarative schemas or designers are needed.</li>
<li>Orchestrator functions can call other durable functions synchronously and asynchronously. Output from called functions can be reliably saved to local variables.</li>
<li>Orchestrator functions are durable and reliable. Execution progress is automatically checkpointed when the function “awaits” or “yields”. Local state is never lost when the process recycles or the VM reboots.</li>
<li>Orchestrator functions can be long-running. The total lifespan of an  <em>orchestration instance</em>  can be seconds, days, months, or never-ending.</li>
</ul>
<h4 id="orchestration-identity">Orchestration identity</h4>
<p>Each  <em>instance</em>  of an orchestration has an instance identifier (also known as an  <em>instance ID</em>). By default, each instance ID is an autogenerated GUID. An orchestration’s instance ID is a required parameter for most instance management operations. They are also important for diagnostics.</p>
<h4 id="reliability">Reliability</h4>
<p>Orchestrator functions reliably maintain their execution state by using the event sourcing design pattern. Instead of directly storing the current state of an orchestration, the Durable Task Framework uses an append-only store to record the full series of actions the function orchestration takes.</p>
<p>Durable Functions uses event sourcing transparently. Behind the scenes, the  <code>await</code>  (C#) or  <code>yield</code>  (JavaScript) operator in an orchestrator function yields control of the orchestrator thread back to the Durable Task Framework dispatcher. The dispatcher then commits any new actions that the orchestrator function scheduled to storage. The transparent commit action appends to the execution history of the orchestration instance. The history is stored in a storage table. The commit action then adds messages to a queue to schedule the actual work. At this point, the orchestrator function can be unloaded from memory.</p>
<p>When an orchestration function is given more work to do, the orchestrator wakes up and re-executes the entire function from the start to rebuild the local state. During the replay, if the code tries to call a function (or do any other async work), the Durable Task Framework consults the execution history of the current orchestration. If it finds that the activity function has already executed and yielded a result, it replays that function’s result and the orchestrator code continues to run.</p>

<table>
<thead>
<tr>
<th>Pattern/Feature</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Sub-orchestrations</td>
<td>Orchestrator functions can call activity functions, but also other orchestrator functions.</td>
</tr>
<tr>
<td>Durable timers</td>
<td>Orchestrations can schedule durable timers to implement delays or to set up timeout handling on async actions.</td>
</tr>
<tr>
<td>External events</td>
<td>Orchestrator functions can wait for external events to update an orchestration instance. This Durable Functions feature often is useful for handling a human interaction or other external callbacks.</td>
</tr>
<tr>
<td>Error handling</td>
<td>Orchestrator functions can use the error-handling features of the programming language.</td>
</tr>
<tr>
<td>Critical sections</td>
<td>To mitigate race conditions when interacting with external systems, orchestrator functions can define  <em>critical sections</em>  using a  <code>LockAsync</code>  method in .NET.</td>
</tr>
<tr>
<td>Calling HTTP endpoints</td>
<td>Orchestrator functions aren’t permitted to do I/O. The typical workaround for this limitation is to wrap any code that needs to do I/O in an activity function.</td>
</tr>
<tr>
<td>Passing multiple parameters</td>
<td>The recommendation is to pass in an array of objects or to use ValueTuples objects in .NET.</td>
</tr>
</tbody>
</table><h3 id="control-timing-in-durable-functions">Control timing in Durable Functions</h3>
<p>Durable Functions provides <em>durable timers</em> for use in orchestrator functions to implement delays or to set up timeouts on async actions. Durable timers should be used in orchestrator functions instead of <code>Thread.Sleep</code> and <code>Task.Delay</code> (C#), or <code>setTimeout()</code> and <code>setInterval()</code> (JavaScript), or <code>time.sleep()</code> (Python).</p>
<p>You create a durable timer by calling the <code>CreateTimer</code> (.NET) method or the <code>createTimer</code> (JavaScript) method of the orchestration trigger binding. The method returns a task that completes on a specified date and time.</p>
<h4 id="usage-for-delay">Usage for delay</h4>
<pre><code>[FunctionName("BillingIssuer")]
public static async Task Run(
    [OrchestrationTrigger] IDurableOrchestrationContext context)
{
    for (int i = 0; i &lt; 10; i++)
    {
        DateTime deadline = context.CurrentUtcDateTime.Add(TimeSpan.FromDays(i + 1));
        await context.CreateTimer(deadline, CancellationToken.None);
        await context.CallActivityAsync("SendBillingEvent");
    }
}
</code></pre>
<h4 id="usage-for-timeout">Usage for timeout</h4>
<pre><code>[FunctionName("TryGetQuote")]
public static async Task&lt;bool&gt; Run(
    [OrchestrationTrigger] IDurableOrchestrationContext context)
{
    TimeSpan timeout = TimeSpan.FromSeconds(30);
    DateTime deadline = context.CurrentUtcDateTime.Add(timeout);

    using (var cts = new CancellationTokenSource())
    {
        Task activityTask = context.CallActivityAsync("GetQuote");
        Task timeoutTask = context.CreateTimer(deadline, cts.Token);

        Task winner = await Task.WhenAny(activityTask, timeoutTask);
        if (winner == activityTask)
        {
            // success case
            cts.Cancel();
            return true;
        }
        else
        {
            // timeout case
            return false;
        }
    }
}
</code></pre>
<h3 id="send-and-wait-for-events">Send and wait for events</h3>
<h4 id="wait-for-events">Wait for events</h4>
<p>The  <code>WaitForExternalEvent</code>  (.NET),  <code>waitForExternalEvent</code>  (JavaScript), and  <code>wait_for_external_event</code>  (Python) methods of the orchestration trigger binding allows an orchestrator function to asynchronously wait and listen for an external event. The listening orchestrator function declares the  <em>name</em>  of the event and the  <em>shape of the data</em>  it expects to receive.</p>
<p>[FunctionName(“BudgetApproval”)] public static async Task Run( [OrchestrationTrigger] IDurableOrchestrationContext context) { bool approved = await context.WaitForExternalEvent(“Approval”); if (approved) { // approval granted - do the approved action } else { // approval denied - send a notification } }</p>
<h4 id="send-events">Send events</h4>
<p>The <code>RaiseEventAsync</code> (.NET) or <code>raiseEvent</code> (JavaScript) method of the orchestration client binding sends the events that <code>WaitForExternalEvent</code> (.NET) or <code>waitForExternalEvent</code> (JavaScript) waits for. The <code>RaiseEventAsync</code> method takes <em>eventName</em> and <em>eventData</em> as parameters. The event data must be JSON-serializable.</p>
<pre><code>[FunctionName("ApprovalQueueProcessor")]
public static async Task Run(
    [QueueTrigger("approval-queue")] string instanceId,
    [DurableClient] IDurableOrchestrationClient client)
{
    await client.RaiseEventAsync(instanceId, "Approval", true);
}
</code></pre>
<h1 id="az-204-develop-solutions-that-use-blob-storage">AZ-204: Develop solutions that use Blob storage</h1>
<h2 id="explore-azure-blob-storage">Explore Azure Blob storage</h2>
<h3 id="explore-azure-blob-storage-1">Explore Azure Blob storage</h3>
<p>Azure Blob storage is Microsoft’s object storage solution for the cloud. Blob storage is optimized for storing massive amounts of unstructured data.</p>
<p>Blob storage is designed for:</p>
<ul>
<li>Serving images or documents directly to a browser.</li>
<li>Storing files for distributed access.</li>
<li>Streaming video and audio.</li>
<li>Writing to log files.</li>
<li>Storing data for backup and restore, disaster recovery, and archiving.</li>
<li>Storing data for analysis by an on-premises or Azure-hosted service.</li>
</ul>
<p>Objects in Blob storage are accessible via the Azure Storage REST API, Azure PowerShell, Azure CLI, or an Azure Storage client library.</p>
<p>An Azure Storage account is the top-level container for all of your Azure Blob storage.</p>
<h4 id="types-of-storage-accounts">Types of storage accounts</h4>

<table>
<thead>
<tr>
<th>Storage account type</th>
<th>Supported storage services</th>
<th>Usage</th>
</tr>
</thead>
<tbody>
<tr>
<td>Standard general-purpose v2</td>
<td>Blob, Queue, and Table storage, Azure Files</td>
<td>Recommended for most scenarios using Azure Storage.</td>
</tr>
<tr>
<td>Premium block blobs</td>
<td>Blob storage</td>
<td>Premium storage account type for block blobs and append blobs. Recommended for scenarios with high transactions rates, or scenarios that use smaller objects or require consistently low storage latency.</td>
</tr>
<tr>
<td>Premium page blobs</td>
<td>Page blobs only</td>
<td>Premium storage account type for page blobs only.</td>
</tr>
<tr>
<td>Premium file shares</td>
<td>Azure Files</td>
<td>Premium storage account type for file shares only. If you want support for NFS file shares in Azure Files, use the premium file shares account type.</td>
</tr>
</tbody>
</table><h4 id="access-tiers-for-block-blob-data">Access tiers for block blob data</h4>
<p>Each access tier in Azure Storage is optimized for a particular pattern of data usage.</p>
<ul>
<li>The  <strong>Hot</strong>  access tier, which is optimized for frequent access of objects in the storage account. The Hot tier has the highest storage costs, but the lowest access costs. New storage accounts are created in the hot tier by default.</li>
<li>The  <strong>Cool</strong>  access tier, which is optimized for storing large amounts of data that is infrequently accessed and stored for at least 30 days. The Cool tier has lower storage costs and higher access costs compared to the Hot tier.</li>
<li>The  <strong>Archive</strong>  tier, which is available only for individual block blobs. The archive tier is optimized for data that can tolerate several hours of retrieval latency and will remain in the Archive tier for at least 180 days. The archive tier is the most cost-effective option for storing data, but accessing that data is more expensive than accessing data in the hot or cool tiers.</li>
</ul>
<h3 id="discover-azure-blob-storage-resource-types">Discover Azure Blob storage resource types</h3>
<p>Blob storage offers three types of resources:</p>
<ul>
<li>The  <strong>storage account</strong>.</li>
<li>A  <strong>container</strong>  in the storage account</li>
<li>A  <strong>blob</strong>  in a container</li>
</ul>
<h4 id="storage-accounts">Storage accounts</h4>
<p>A storage account provides a unique namespace in Azure for your data. eg. for default enpoint: <a href="http://mystorageaccount.blob.core.windows.net">http://mystorageaccount.blob.core.windows.net</a></p>
<h4 id="containers">Containers</h4>
<p>A container organizes a set of blobs, similar to a directory in a file system. A storage account can include an unlimited number of containers, and a container can store an unlimited number of blobs.</p>
<h4 id="blobs">Blobs</h4>
<ul>
<li><strong>Block blobs</strong>  store text and binary data, up to about 190.7 TB.</li>
<li><strong>Append blobs</strong>  are made up of blocks like block blobs, but are optimized for append operations. Append blobs are ideal for scenarios such as logging data from virtual machines.</li>
<li><strong>Page blobs</strong>  store random access files up to 8 TB in size. Page blobs store virtual hard drive (VHD) files and serve as disks for Azure virtual machines.</li>
</ul>
<h3 id="explore-azure-storage-security-features">Explore Azure Storage security features</h3>
<ul>
<li>All data (including metadata) written to Azure Storage is automatically encrypted using Storage Service Encryption (SSE).</li>
<li>Azure Active Directory (Azure AD) and Role-Based Access Control (RBAC) are supported for Azure Storage for both resource management operations and data operation</li>
<li>Data can be secured in transit between an application and Azure by using Client-Side Encryption, HTTPS, or SMB 3.0.</li>
<li>OS and data disks used by Azure virtual machines can be encrypted using Azure Disk Encryption.</li>
<li>Delegated access to the data objects in Azure Storage can be granted using a shared access signature.</li>
</ul>
<h4 id="azure-storage-encryption-for-data-at-rest">Azure Storage encryption for data at rest</h4>
<p>Azure Storage automatically encrypts your data when persisting it to the cloud.Data in Azure Storage is encrypted and decrypted transparently using 256-bit AES encryption, one of the strongest block ciphers available, and is FIPS 140-2 compliant.</p>
<h5 id="encryption-key-management">Encryption key management</h5>
<p>You can rely on Microsoft-managed keys for the encryption of your storage account, or you can manage encryption with your own keys. If you choose to manage encryption with your own keys, you have two options:</p>
<ul>
<li>You can specify a  <em>customer-managed</em>  key to use for encrypting and decrypting all data in the storage account.</li>
<li>You can specify a  <em>customer-provided</em>  key on Blob storage operations.</li>
</ul>
<p>| | Microsoft-managed keys | Customer-managed keys | Customer-provided keys |<br>
| Encryption/decryption operations | Azure | Azure | Azure |<br>
| Azure Storage services supported | All | Blob storage, Azure Files | Blob storage |<br>
| Key storage | Microsoft key store | Azure Key Vault | Azure Key Vault or any other key store |<br>
| Key rotation responsibility | Microsoft | Customer | Customer |<br>
| Key usage | Microsoft | Azure portal, Storage Resource Provider REST API, Azure Storage management libraries, PowerShell, CLI | Azure Storage REST API (Blob storage), Azure Storage client libraries |<br>
| Key access | Microsoft only | Microsoft, Customer | Customer only |</p>
<h3 id="evaluate-azure-storage-redundancy-options">Evaluate Azure Storage redundancy options</h3>
<p>Azure Storage always stores multiple copies of your data so that it is protected from planned and unplanned events.</p>
<p>When deciding which redundancy option is best for your scenario, consider the tradeoffs between lower costs and higher availability.</p>
<h4 id="redundancy-in-the-primary-region">Redundancy in the primary region</h4>
<p>Data in an Azure Storage account is always replicated three times in the primary region.</p>
<ul>
<li><strong>Locally redundant storage (LRS)</strong>: Copies your data synchronously three times within a single physical location in the primary region.</li>
<li><strong>Zone-redundant storage (ZRS)</strong>: Copies your data synchronously across three Azure availability zones in the primary region.</li>
</ul>
<h4 id="redundancy-in-a-secondary-region">Redundancy in a secondary region</h4>
<p>For applications requiring high durability, you can choose to additionally copy the data in your storage account to a secondary region that is hundreds of miles away from the primary region.</p>
<p>When you create a storage account, you select the primary region for the account. The paired secondary region is determined based on the primary region, and can’t be changed.</p>
<ul>
<li><strong>Geo-redundant storage (GRS)</strong>  copies your data synchronously three times within a single physical location in the primary region using LRS. It then copies your data asynchronously to a single physical location in the secondary region. Within the secondary region, your data is copied synchronously three times using LRS.</li>
<li><strong>Geo-zone-redundant storage (GZRS)</strong>  copies your data synchronously across three Azure availability zones in the primary region using ZRS. It then copies your data asynchronously to a single physical location in the secondary region. Within the secondary region, your data is copied synchronously three times using LRS.</li>
</ul>
<h2 id="manage-the-azure-blob-storage-lifecycle">Manage the Azure Blob storage lifecycle</h2>
<p>Data sets have unique lifecycles. Early in the lifecycle, people access some data often. But the need for access drops drastically as the data ages.</p>
<h4 id="access-tiers">Access tiers</h4>
<ul>
<li><strong>Hot</strong>  - Optimized for storing data that is accessed frequently.</li>
<li><strong>Cool</strong>  - Optimized for storing data that is infrequently accessed and stored for at least 30 days.</li>
<li><strong>Archive</strong>  - Optimized for storing data that is rarely accessed and stored for at least 180 days with flexible latency requirements, on the order of hours.</li>
</ul>
<p>The hot and cool tiers support all redundancy options. The archive tier supports only LRS, GRS, and RA-GRS.</p>
<h4 id="manage-the-data-lifecycle">Manage the data lifecycle</h4>
<p>Azure Blob storage lifecycle management offers a rich, rule-based policy for General Purpose v2 and Blob storage accounts. Use the policy to transition your data to the appropriate access tiers or expire at the end of the data’s lifecycle.</p>
<h3 id="discover-blob-storage-lifecycle-policies">Discover Blob storage lifecycle policies</h3>
<p>A lifecycle management policy is a collection of rules in a JSON document. Each rule definition within a policy includes a filter set and an action set. The filter set limits rule actions to a certain set of objects within a container or objects names. The action set applies the tier or delete actions to the filtered set of objects.:</p>
<pre><code>{
  "rules": [
    {
      "name": "rule1",
      "enabled": true,
      "type": "Lifecycle",
      "definition": {...}
    },
    {
      "name": "rule2",
      "type": "Lifecycle",
      "definition": {...}
    }
  ]
}
</code></pre>
<p>A policy is a collection of rules:</p>

<table>
<thead>
<tr>
<th>Parameter name</th>
<th>Parameter type</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>rules</code></td>
<td>An array of rule objects</td>
<td>At least one rule is required in a policy. You can define up to 100 rules in a policy.</td>
</tr>
</tbody>
</table><p>Each rule within the policy has several parameters:</p>

<table>
<thead>
<tr>
<th>Parameter name</th>
<th>Parameter type</th>
<th>Notes</th>
<th>Required</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>name</code></td>
<td>String</td>
<td>It must be unique within a policy.</td>
<td>True</td>
</tr>
<tr>
<td><code>enabled</code></td>
<td>Boolean</td>
<td>An optional boolean to allow a rule to be temporary disabled.</td>
<td>False</td>
</tr>
<tr>
<td><code>type</code></td>
<td>An enum value</td>
<td>The current valid type is Lifecycle.</td>
<td>True</td>
</tr>
<tr>
<td><code>definition</code></td>
<td>An object that defines the lifecycle rule</td>
<td>Each definition is made up of a filter set and an action set.</td>
<td>True</td>
</tr>
</tbody>
</table><h4 id="rules">Rules</h4>
<p>Each rule definition includes a filter set and an action set. The filter set limits rule actions to a certain set of objects within a container or objects names.</p>
<pre><code>{
  "rules": [
    {
      "name": "ruleFoo",
      "enabled": true,
      "type": "Lifecycle",
      "definition": {
        "filters": {
          "blobTypes": [ "blockBlob" ],
          "prefixMatch": [ "container1/foo" ]
        },
        "actions": {
          "baseBlob": {
            "tierToCool": { "daysAfterModificationGreaterThan": 30 },
            "tierToArchive": { "daysAfterModificationGreaterThan": 90 },
            "delete": { "daysAfterModificationGreaterThan": 2555 }
          },
          "snapshot": {
            "delete": { "daysAfterCreationGreaterThan": 90 }
          }
        }
      }
    }
  ]
}
</code></pre>
<h4 id="rule-filters">Rule filters</h4>

<table>
<thead>
<tr>
<th>Filter name</th>
<th>Filter type</th>
<th>Is Required</th>
</tr>
</thead>
<tbody>
<tr>
<td>blobTypes</td>
<td>An array of predefined enum values.</td>
<td>Yes</td>
</tr>
<tr>
<td>prefixMatch</td>
<td>An array of strings for prefixes to be match. Each rule can define up to 10 prefixes. A prefix string must start with a container name.</td>
<td>No</td>
</tr>
<tr>
<td>blobIndexMatch</td>
<td>An array of dictionary values consisting of blob index tag key and value conditions to be matched. Each rule can define up to 10 blob index tag condition.</td>
<td>No</td>
</tr>
</tbody>
</table><h4 id="rule-actions">Rule actions</h4>

<table>
<thead>
<tr>
<th>Action</th>
<th>Base Blob</th>
<th>Snapshot</th>
<th>Version</th>
</tr>
</thead>
<tbody>
<tr>
<td>tierToCool</td>
<td>Supported for blockBlob</td>
<td>Supported</td>
<td>Supported</td>
</tr>
<tr>
<td>enableAutoTierToHotFromCool</td>
<td>Supported for blockBlob</td>
<td>Not supported</td>
<td>Not supported</td>
</tr>
<tr>
<td>tierToArchive</td>
<td>Supported for blockBlob</td>
<td>Supported</td>
<td>Supported</td>
</tr>
<tr>
<td>delete</td>
<td>Supported for blockBlob and appendBlob</td>
<td>Supported</td>
<td>Supported</td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr>
<th>Action run condition</th>
<th>Condition value</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>daysAfterModificationGreaterThan</td>
<td>Integer value indicating the age in days</td>
<td>The condition for base blob actions</td>
</tr>
<tr>
<td>daysAfterCreationGreaterThan</td>
<td>Integer value indicating the age in days</td>
<td>The condition for blob snapshot actions</td>
</tr>
</tbody>
</table><h3 id="implement-blob-storage-lifecycle-policies">Implement Blob storage lifecycle policies</h3>
<h4 id="azure-portal">Azure portal</h4>
<p>There are two ways to add a policy through the Azure portal: Azure portal List view, and Azure portal Code view.</p>
<h4 id="azure-cli">Azure CLI</h4>
<p>To add a lifecycle management policy with Azure CLI, write the policy to a JSON file, then call the <code>az storage account management-policy create</code> command to create the policy.</p>
<h3 id="rehydrate-blob-data-from-the-archive-tier">Rehydrate blob data from the archive tier</h3>
<p>While a blob is in the archive access tier, it’s considered to be offline and can’t be read or modified. In order to read or modify data in an archived blob, you must first rehydrate the blob to an online tier, either the hot or cool tier.</p>
<ul>
<li><strong>Copy an archived blob to an online tier</strong></li>
<li><strong>Change a blob’s access tier to an online tier</strong></li>
</ul>
<h4 id="rehydration-priority">Rehydration priority</h4>
<p>When you rehydrate a blob, you can set the priority for the rehydration operation via the optional  <code>x-ms-rehydrate-priority</code>  header on a  <a href="https://docs.microsoft.com/en-us/rest/api/storageservices/set-blob-tier">Set Blob Tier</a>  or  <strong>Copy Blob/Copy Blob From URL</strong>  operation. Rehydration priority options include:</p>
<ul>
<li><strong>Standard priority</strong>: The rehydration request will be processed in the order it was received and may take up to 15 hours.</li>
<li><strong>High priority</strong>: The rehydration request will be prioritized over standard priority requests and may complete in under one hour for objects under 10 GB in size.</li>
</ul>
<h4 id="copy-an-archived-blob-to-an-online-tier">Copy an archived blob to an online tier</h4>
<p>You can use either the <strong>Copy Blob</strong> or <strong>Copy Blob from URL</strong> operation to copy the blob. You must copy the archived blob to a new blob with a different name or to a different container. Copying an archived blob to an online destination tier is supported within the same storage account only.</p>

<table>
<thead>
<tr>
<th></th>
<th>Hot tier source</th>
<th>Cool tier source</th>
<th>Archive tier source</th>
</tr>
</thead>
<tbody>
<tr>
<td>Hot tier destination</td>
<td>Supported</td>
<td>Supported</td>
<td>Supported within the same account. Requires blob rehydration.</td>
</tr>
<tr>
<td>Cool tier destination</td>
<td>Supported</td>
<td>Supported</td>
<td>Supported within the same account. Requires blob rehydration.</td>
</tr>
<tr>
<td>Archive tier destination</td>
<td>Supported</td>
<td>Supported</td>
<td>Unsupported</td>
</tr>
</tbody>
</table><h4 id="change-a-blobs-access-tier-to-an-online-tier">Change a blob’s access tier to an online tier</h4>
<p>The second option for rehydrating a blob from the archive tier to an online tier is to change the blob’s tier by calling <strong>Set Blob Tier</strong>. With this operation, you can change the tier of the archived blob to either hot or cool.</p>
<p>Changing a blob’s tier doesn’t affect its last modified time. If there is a lifecycle management policy in effect for the storage account, then rehydrating a blob with <strong>Set Blob Tier</strong> can result in a scenario where the lifecycle policy moves the blob back to the archive tier after rehydration because the last modified time is beyond the threshold set for the policy.</p>
<h2 id="work-with-azure-blob-storage">Work with Azure Blob storage</h2>
<h3 id="explore-azure-blob-storage-client-library">Explore Azure Blob storage client library</h3>
<p>The Azure Storage client libraries for .NET offer a convenient interface for making calls to Azure Storage. Below are the classes in the Azure.Storage.Blobs namespace and their purpose:</p>
<ul>
<li>The  <code>BlobClient</code>  allows you to manipulate Azure Storage blobs.</li>
<li>Provides the client configuration options for connecting to Azure Blob Storage.</li>
<li>The  <code>BlobContainerClient</code>  allows you to manipulate Azure Storage containers and their blobs.</li>
<li>The  <code>BlobServiceClient</code>  allows you to manipulate Azure Storage service resources and blob containers.</li>
<li>The  <code>BlobUriBuilder</code>  class provides a convenient way to modify the contents of a Uri instance to point to different Azure Storage resources like an account, container, or blob.</li>
</ul>
<h3 id="manage-container-properties-and-metadata-by-using-.net">Manage container properties and metadata by using .NET</h3>
<ul>
<li><strong>System properties</strong>: System properties exist on each Blob storage resource. Some of them can be read or set, while others are read-only. Under the covers, some system properties correspond to certain standard HTTP headers. The Azure Storage client library for .NET maintains these properties for you.</li>
<li><strong>User-defined metadata</strong>: User-defined metadata consists of one or more name-value pairs that you specify for a Blob storage resource. You can use metadata to store additional values with the resource. Metadata values are for your own purposes only, and do not affect how the resource behaves.</li>
</ul>
<p>Metadata names must be valid HTTP header names and valid C# identifiers, may contain only ASCII characters, and should be treated as case-insensitive.</p>
<h4 id="retrieve-container-properties">Retrieve container properties</h4>
<p>To retrieve container properties, call one of the following methods of the  <code>BlobContainerClient</code>  class:</p>
<ul>
<li><code>GetProperties</code></li>
<li><code>GetPropertiesAsync</code></li>
</ul>
<h4 id="set-and-retrieve-metadata">Set and retrieve metadata</h4>
<p>You can specify metadata as one or more name-value pairs on a blob or container resource. To set metadata, add name-value pairs to an  <code>IDictionary</code>  object, and then call one of the following methods of the  <code>BlobContainerClient</code>  class to write the values:</p>
<ul>
<li><code>SetMetadata</code></li>
<li><code>SetMetadataAsync</code></li>
</ul>
<p>The <code>GetProperties</code> and <code>GetPropertiesAsync</code> methods are used to retrieve metadata in addition to properties as shown earlier.</p>
<h3 id="set-and-retrieve-properties-and-metadata-for-blob-resources-by-using-rest">Set and retrieve properties and metadata for blob resources by using REST</h3>
<h4 id="metadata-header-format">Metadata header format</h4>
<p>Metadata headers are name/value pairs. The format for the header is:</p>
<pre><code>x-ms-meta-name:string-value  
</code></pre>
<p>Names are case-insensitive. Note that metadata names preserve the case with which they were created, but are case-insensitive when set or read.</p>
<h4 id="operations-on-metadata">Operations on metadata</h4>
<p>Note that metadata values can only be read or written in full; partial updates are not supported.</p>
<h5 id="retrieving-properties-and-metadata">Retrieving properties and metadata</h5>
<p>The GET and HEAD operations both retrieve metadata headers for the specified container or blob. These operations return headers only; they do not return a response body.<br>
The URI syntax for retrieving metadata headers on a container is as follows:</p>
<pre><code>GET/HEAD https://myaccount.blob.core.windows.net/mycontainer?restype=container  
</code></pre>
<p>The URI syntax for retrieving metadata headers on a blob is as follows:</p>
<pre><code>GET/HEAD https://myaccount.blob.core.windows.net/mycontainer/myblob?comp=metadata
</code></pre>
<h5 id="setting-metadata-headers">Setting Metadata Headers</h5>
<p>The PUT operation sets metadata headers on the specified container or blob, overwriting any existing metadata on the resource.</p>
<p>The URI syntax for setting metadata headers on a container is as follows:</p>
<pre><code>PUT https://myaccount.blob.core.windows.net/mycontainer?comp=metadata&amp;restype=container

</code></pre>
<p>The URI syntax for setting metadata headers on a blob is as follows:</p>
<pre><code>PUT https://myaccount.blob.core.windows.net/mycontainer/myblob?comp=metadata
</code></pre>
<h4 id="standard-http-properties-for-containers-and-blobs">Standard HTTP properties for containers and blobs</h4>
<p>Properties and metadata are both represented as standard HTTP headers; the difference between them is in the naming of the headers. Metadata headers are named with the header prefix <code>x-ms-meta-</code> and a custom name. Property headers use standard HTTP header names, as specified in the Header Field Definitions section 14 of the HTTP/1.1 protocol specification.</p>
<p>The standard HTTP headers supported on containers include:</p>
<ul>
<li><code>ETag</code></li>
<li><code>Last-Modified</code></li>
</ul>
<p>The standard HTTP headers supported on blobs include:</p>
<ul>
<li><code>ETag</code></li>
<li><code>Last-Modified</code></li>
<li><code>Content-Length</code></li>
<li><code>Content-Type</code></li>
<li><code>Content-MD5</code></li>
<li><code>Content-Encoding</code></li>
<li><code>Content-Language</code></li>
<li><code>Cache-Control</code></li>
<li><code>Origin</code></li>
<li><code>Range</code></li>
</ul>
<h1 id="az-204-develop-solutions-that-use-azure-cosmos-db">AZ-204: Develop solutions that use Azure Cosmos DB</h1>
<h2 id="explore-azure-cosmos-db">Explore Azure Cosmos DB</h2>
<h3 id="identify-key-benefits-of-azure-cosmos-db">Identify key benefits of Azure Cosmos DB</h3>
<p>Azure Cosmos DB is a globally distributed database system that allows you to read and write data from the local replicas of your database and it transparently replicates the data to all the regions associated with your Cosmos account.</p>
<p>Azure Cosmos DB is designed to provide low latency, elastic scalability of throughput, well-defined semantics for data consistency, and high availability. To lower the latency, place the data close to where your users are.</p>
<h4 id="key-benefits-of-global-distribution">Key benefits of global distribution</h4>
<p>With its novel multi-master replication protocol, every region supports both writes and reads. The multi-master capability also enables:</p>
<ul>
<li>Unlimited elastic write and read scalability.</li>
<li>99.999% read and write availability all around the world.</li>
<li>Guaranteed reads and writes served in less than 10 milliseconds at the 99th percentile.</li>
<li></li>
</ul>
<p>Your application can perform near real-time reads and writes against all the regions you chose for your database. Azure Cosmos DB internally handles the data replication between regions with consistency level guarantees of the level you’ve selected.</p>
<h3 id="explore-the-resource-hierarchy">Explore the resource hierarchy</h3>
<p>The Azure Cosmos account is the fundamental unit of global distribution and high availability. Your Azure Cosmos account contains a unique DNS name and you can manage an account by using the Azure portal or the Azure CLI, or by using different language-specific SDKs.</p>
<h4 id="elements-in-an-azure-cosmos-account">Elements in an Azure Cosmos account</h4>
<p>An Azure Cosmos container is the fundamental unit of scalability. You can virtually have an unlimited provisioned throughput (RU/s) and storage on a container. Azure Cosmos DB transparently partitions your container using the logical partition key that you specify in order to elastically scale your provisioned throughput and storage.</p>
<p>Currently, you can create a maximum of 50 Azure Cosmos accounts under an Azure subscription (this is a soft limit that can be increased via support request).</p>
<p><img src="https://docs.microsoft.com/en-us/learn/wwl-azure/explore-azure-cosmos-db/media/cosmos-entities.png" alt="Image showing the hierarchy of Azure Cosmos DB entities: Database accounts are at the top, Databases are grouped under accounts, Containers are grouped under databases."></p>
<h4 id="azure-cosmos-databases">Azure Cosmos databases</h4>
<p>A database is analogous to a namespace. A database is the unit of management for a set of Azure Cosmos containers.</p>
<h4 id="azure-cosmos-containers">Azure Cosmos containers</h4>
<p>An Azure Cosmos container is the unit of scalability both for provisioned throughput and storage. A container is horizontally partitioned and then replicated across multiple regions.</p>
<p>When you create a container, you configure throughput in one of the following modes:</p>
<ul>
<li>
<p><strong>Dedicated provisioned throughput mode</strong>: The throughput provisioned on a container is exclusively reserved for that container and it is backed by the SLAs.</p>
</li>
<li>
<p><strong>Shared provisioned throughput mode</strong>: These containers share the provisioned throughput with the other containers in the same database (excluding containers that have been configured with dedicated provisioned throughput).</p>
</li>
</ul>
<p>A container is a schema-agnostic container of items. Items in a container can have arbitrary schemas.</p>
<h4 id="azure-cosmos-items">Azure Cosmos items</h4>
<p>Depending on which API you use, an Azure Cosmos item can represent either a document in a collection, a row in a table, or a node or edge in a graph.</p>

<table>
<thead>
<tr>
<th>Cosmos entity</th>
<th>SQL API</th>
<th>Cassandra API</th>
<th>Azure Cosmos DB API for MongoDB</th>
<th>Gremlin API</th>
<th>Table API</th>
</tr>
</thead>
<tbody>
<tr>
<td>Azure Cosmos item</td>
<td>Item</td>
<td>Row</td>
<td>Document</td>
<td>Node or edge</td>
<td>Item</td>
</tr>
</tbody>
</table><h3 id="explore-consistency-levels">Explore consistency levels</h3>
<p>Azure Cosmos DB approaches data consistency as a spectrum of choices instead of two extremes. Strong consistency and eventual consistency are at the ends of the spectrum, but there are many consistency choices along the spectrum. Developers can use these options to make precise choices and granular tradeoffs with respect to high availability and performance.</p>
<p>Consistency spectrum:</p>
<ul>
<li><em>strong</em></li>
<li><em>bounded staleness</em></li>
<li><em>session</em></li>
<li><em>consistent prefix</em></li>
<li><em>eventual</em></li>
</ul>
<p><img src="https://docs.microsoft.com/en-us/learn/wwl-azure/explore-azure-cosmos-db/media/five-consistency-levels.png" alt="Image showing data consistency as a spectrum"></p>
<p>The consistency levels are region-agnostic and are guaranteed for all operations regardless of the region from which the reads and writes are served.</p>
<h3 id="choose-the-right-consistency-level">Choose the right consistency level</h3>
<h4 id="sql-api-and-table-api">SQL API and Table API</h4>
<ul>
<li>For many real-world scenarios, session consistency is optimal and it’s the recommended option.
<ul>
<li>If your application requires strong consistency, it is recommended that you use bounded staleness consistency level.</li>
</ul>
</li>
<li>If you need the highest throughput and the lowest latency, then use eventual consistency level.</li>
<li>If you need even higher data durability without sacrificing performance, you can create a custom consistency level at the application layer.</li>
</ul>
<h4 id="cassandra-mongodb-and-gremlin-apis">Cassandra, MongoDB, and Gremlin APIs</h4>
<p>Azure Cosmos DB provides native support for wire protocol-compatible APIs for popular databases. These include MongoDB, Apache Cassandra, and Gremlin.</p>
<h4 id="consistency-guarantees-in-practice">Consistency guarantees in practice</h4>
<ul>
<li>When the consistency level is set to  <strong>bounded staleness</strong>, Cosmos DB guarantees that the clients always read the value of a previous write, with a lag bounded by the staleness window.
<ul>
<li>When the consistency level is set to  <strong>strong</strong>, the staleness window is equivalent to zero, and the clients are guaranteed to read the latest committed value of the write operation.</li>
<li>For the remaining three consistency levels, the staleness window is largely dependent on your workload. For example, if there are no write operations on the database, a read operation with  <strong>eventual</strong>,  <strong>session</strong>, or  <strong>consistent prefix</strong>  consistency levels is likely to yield the same results as a read operation with strong consistency level.</li>
</ul>
</li>
</ul>
<p>If your Azure Cosmos account is configured with a consistency level other than the strong consistency, you can find out the probability that your clients may get strong and consistent reads for your workloads by looking at the <em>Probabilistically Bounded Staleness</em> (PBS) metric.</p>
<h3 id="explore-supported-apis">Explore supported APIs</h3>
<p>These APIs allow your applications to treat Azure Cosmos DB as if it were various other databases technologies, without the overhead of management, and scaling approaches.</p>
<h4 id="coresql-api">Core(SQL) API</h4>
<p>Azure Cosmos DB SQL API accounts provide support for querying items using the Structured Query Language (SQL) syntax, one of the most familiar and popular query languages.</p>
<p>This API stores data in document format. It offers the best end-to-end experience as we have full control over the interface, service, and the SDK client libraries. Any new feature that is rolled out to Azure Cosmos DB is first available on SQL API accounts.</p>
<h4 id="api-for-mongodb">API for MongoDB</h4>
<p>This API stores data in a document structure, via BSON format. It is compatible with MongoDB wire protocol; however, it does not use any native MongoDB related code.</p>
<h4 id="cassandra-api">Cassandra API</h4>
<p>This API stores data in column-oriented schema. Cassandra API is wire protocol compatible with the Apache Cassandra.</p>
<h4 id="table-api">Table API</h4>
<p>This API stores data in key/value format. If you are currently using Azure Table storage, you may see some limitations in latency, scaling, throughput, global distribution, index management, low query performance. Table API overcomes these limitations and it’s recommended to migrate your app if you want to use the benefits of Azure Cosmos DB.</p>
<h4 id="gremlin-api">Gremlin API</h4>
<p>This API allows users to make graph queries and stores data as edges and vertices.</p>
<h3 id="discover-request-units">Discover request units</h3>
<p>With Azure Cosmos DB, you pay for the throughput you provision and the storage you consume on an hourly basis. Throughput must be provisioned to ensure that sufficient system resources are available for your Azure Cosmos database at all times.</p>
<p>The cost of all database operations is normalized by Azure Cosmos DB and is expressed by <em>request units</em> (or RUs, for short). The cost to do a point read, which is fetching a single item by its ID and partition key value, for a 1KB item is 1RU.</p>
<p><img src="https://docs.microsoft.com/en-us/learn/wwl-azure/explore-azure-cosmos-db/media/request-units.png" alt="Image showing how database operations consume request units."></p>
<p>The type of Azure Cosmos account you’re using determines the way consumed RUs get charged. There are three modes in which you can create an account:</p>
<p><strong>Provisioned throughput mode</strong>: In this mode, you provision the number of RUs for your application on a per-second basis in increments of 100 RUs per second.</p>
<ul>
<li><strong>Serverless mode</strong>: In this mode, you don’t have to provision any throughput when creating resources in your Azure Cosmos account. At the end of your billing period, you get billed for the amount of request units that has been consumed by your database operations.</li>
<li><strong>Autoscale mode</strong>: In this mode, you can automatically and instantly scale the throughput (RU/s) of your database or container based on it’s usage. This mode is well suited for mission-critical workloads that have variable or unpredictable traffic patterns, and require SLAs on high performance and scale.</li>
</ul>
<h2 id="implement-partitioning-in-azure-cosmos-db">Implement partitioning in Azure Cosmos DB</h2>
<h3 id="explore-partitions">Explore partitions</h3>
<p>Azure Cosmos DB uses partitioning to scale individual containers in a database to meet the performance needs of your application.</p>
<p>In partitioning, the items in a container are divided into distinct subsets called <em>logical partitions</em>. Logical partitions are formed based on the value of a <em>partition key</em> that is associated with each item in a container.</p>
<p>In addition to a partition key that determines the item’s logical partition, each item in a container has an <em>item ID</em> which is unique within a logical partition. Combining the partition key and the <em>item ID</em> creates the item’s <em>index</em>, which uniquely identifies the item. Choosing a partition key is an important decision that will affect your application’s performance.</p>
<h4 id="logical-partitions">Logical partitions</h4>
<ul>
<li>A logical partition consists of a set of items that have the same partition key.</li>
<li>A logical partition also defines the scope of database transactions.</li>
</ul>
<h4 id="physical-partitions">Physical partitions</h4>
<p>A container is scaled by distributing data and throughput across physical partitions. Internally, one or more logical partitions are mapped to a single physical partition. Typically smaller containers have many logical partitions but they only require a single physical partition. Unlike logical partitions, physical partitions are an internal implementation of the system and they are entirely managed by Azure Cosmos DB.</p>
<p>The number of physical partitions in your container depends on the following:</p>
<ul>
<li>The number of throughput provisioned. The 10,000 RU/s limit for physical partitions implies that logical partitions also have a 10,000 RU/s limit, as each logical partition is only mapped to one physical partition.</li>
<li>The total data storage (each individual physical partition can store up to 50GB data).</li>
</ul>
<p>A partition key design that doesn’t distribute requests evenly might result in too many requests directed to a small subset of partitions that become “hot.”</p>
<h3 id="choose-a-partition-key">Choose a partition key</h3>
<p>A partition key has two components: <strong>partition key path</strong> and the <strong>partition key value</strong>.</p>
<ul>
<li>The partition key path (for example: “/userId”) accepts alphanumeric and underscore(_) characters. You can also use nested objects by using the standard path notation(/).</li>
<li>The partition key value (for example: “Andrew”) can be of string or numeric types.</li>
</ul>
<p>Once you select your partition key, it is not possible to change it in-place.</p>
<p>For  <strong>all</strong>  containers, your partition key should:</p>
<ul>
<li>Be a property that has a value which does not change.</li>
<li>Have a high cardinality. In other words, the property should have a wide range of possible values.</li>
<li>Spread request unit (RU) consumption and data storage evenly across all logical partitions. This ensures even RU consumption and storage distribution across your physical partitions.</li>
</ul>
<h4 id="partition-keys-for-read-heavy-containers">Partition keys for read-heavy containers</h4>
<p>For large read-heavy containers you might want to choose a partition key that appears frequently as a filter in your queries.</p>
<p>However, if your container is small, you probably don’t have enough physical partitions to need to worry about the performance impact of cross-partition queries. If your container could grow to more than a few physical partitions, then you should make sure you pick a partition key that minimizes cross-partition queries.</p>
<p>Your container will require more than a few physical partitions when either of the following are true:</p>
<ul>
<li>Your container will have over 30,000 RU’s provisioned</li>
<li>Your container will store over 100 GB of data</li>
</ul>
<h4 id="using-item-id-as-the-partition-key">Using item ID as the partition key</h4>
<p>If your container has a property that has a wide range of possible values, it is likely a great partition key choice. One possible example of such a property is the <em>item ID</em>. For small read-heavy containers or write-heavy containers of any size, the <em>item ID</em> is naturally a great choice for the partition key.</p>
<p>The  <em>item ID</em>  is a great partition key choice for the following reasons:</p>
<ul>
<li>There are a wide range of possible values (one unique  <em>item ID</em>  per item).</li>
<li>Because there is a unique  <em>item ID</em>  per item, the  <em>item ID</em>  does a great job at evenly balancing RU consumption and data storage.</li>
<li>You can easily do efficient point reads since you’ll always know an item’s partition key if you know its  <em>item ID</em>.</li>
</ul>
<p>Some things to consider when selecting the  <em>item ID</em>  as the partition key include:</p>
<ul>
<li>If the  <em>item ID</em>  is the partition key, it will become a unique identifier throughout your entire container. You won’t be able to have items that have a duplicate  <em>item ID</em>.</li>
<li>If you have a read-heavy container that has a lot of physical partitions, queries will be more efficient if they have an equality filter with the  <em>item ID</em>.</li>
<li>You can’t run stored procedures or triggers across multiple logical partitions.</li>
</ul>
<h3 id="create-a-synthetic-partition-key">Create a synthetic partition key</h3>
<p>It’s the best practice to have a partition key with many distinct values, such as hundreds or thousands. The goal is to distribute your data and workload evenly across the items associated with these partition key values. If such a property doesn’t exist in your data, you can construct a <em>synthetic partition key</em>.</p>
<h4 id="concatenate-multiple-properties-of-an-item">Concatenate multiple properties of an item</h4>
<p>You can form a partition key by concatenating multiple property values into a single artificial <code>partitionKey</code> property.</p>
<h4 id="use-a-partition-key-with-a-random-suffix">Use a partition key with a random suffix</h4>
<p>Another possible strategy to distribute the workload more evenly is to append a random number at the end of the partition key value. When you distribute items in this way, you can perform parallel write operations across partitions.</p>
<h4 id="use-a-partition-key-with-pre-calculated-suffixes">Use a partition key with pre-calculated suffixes</h4>
<p>The random suffix strategy can greatly improve write throughput, but it’s difficult to read a specific item. You don’t know the suffix value that was used when you wrote the item. To make it easier to read individual items, use the pre-calculated suffixes strategy. Instead of using a random number to distribute the items among the partitions, use a number that is calculated based on something that you want to query. Before your application writes the item to the container, it can calculate a hash suffix based on a cloumn that is often used for querying and append it to the partition key date.</p>
<h2 id="work-with-azure-cosmos-db">Work with Azure Cosmos DB</h2>
<h3 id="explore-microsoft-.net-sdk-v3-for-azure-cosmos-db">Explore Microsoft .NET SDK v3 for Azure Cosmos DB</h3>
<p>Below are examples showing some of the key operations you should be familiar with.</p>
<h4 id="cosmosclient">CosmosClient</h4>
<p>Creates a new  <code>CosmosClient</code>  with a connection string.  <code>CosmosClient</code>  is thread-safe. Its recommended to maintain a single instance of  <code>CosmosClient</code>  per lifetime of the application which enables efficient connection management and performance.</p>
<pre><code>CosmosClient client = new CosmosClient(endpoint, key);
</code></pre>
<h2 id="database-examples">Database examples</h2>
<h3 id="create-a-database">Create a database</h3>
<pre><code>// An object containing relevant information about the response
DatabaseResponse databaseResponse = await client.CreateDatabaseIfNotExistsAsync(databaseId, 10000);
</code></pre>
<h3 id="read-a-database-by-id">Read a database by ID</h3>
<p>Reads a database from the Azure Cosmos service as an asynchronous operation.</p>
<pre><code>DatabaseResponse readResponse = await database.ReadAsync();

</code></pre>
<h3 id="delete-a-database">Delete a database</h3>
<p>Delete a Database as an asynchronous operation.</p>
<pre><code>await database.DeleteAsync();
</code></pre>
<h2 id="container-examples">Container examples</h2>
<h3 id="create-a-container">Create a container</h3>
<pre><code>// Set throughput to the minimum value of 400 RU/s
ContainerResponse simpleContainer = await database.CreateContainerIfNotExistsAsync(
    id: containerId,
    partitionKeyPath: partitionKey,
    throughput: 400);

</code></pre>
<h3 id="get-a-container-by-id">Get a container by ID</h3>
<pre><code>Container container = database.GetContainer(containerId);
ContainerProperties containerProperties = await container.ReadContainerAsync();

</code></pre>
<h3 id="delete-a-container">Delete a container</h3>
<p>Delete a Container as an asynchronous operation.</p>
<p>C#Copy</p>
<pre><code>await database.GetContainer(containerId).DeleteContainerAsync();

</code></pre>
<h2 id="item-examples">Item examples</h2>
<h3 id="create-an-item">Create an item</h3>
<p>Use the  <code>Container.CreateItemAsync</code>  method to create an item. The method requires a JSON serializable object that must contain an  <code>id</code>  property, and a  <code>partitionKey</code>.</p>
<p>C#Copy</p>
<pre><code>ItemResponse&lt;SalesOrder&gt; response = await container.CreateItemAsync(salesOrder, new PartitionKey(salesOrder.AccountNumber));

</code></pre>
<h3 id="read-an-item">Read an item</h3>
<p>Use the  <code>Container.ReadItemAsync</code>  method to read an item. The method requires type to serialize the item to along with an  <code>id</code>  property, and a  <code>partitionKey</code>.</p>
<pre><code>string id = "[id]";
string accountNumber = "[partition-key]";
ItemResponse&lt;SalesOrder&gt; response = await container.ReadItemAsync(id, new PartitionKey(accountNumber));

</code></pre>
<h3 id="query-an-item">Query an item</h3>
<p>The  <code>Container.GetItemQueryIterator</code>  method creates a query for items under a container in an Azure Cosmos database using a SQL statement with parameterized values. It returns a  <code>FeedIterator</code>.</p>
<pre><code>QueryDefinition query = new QueryDefinition(
    "select * from sales s where s.AccountNumber = @AccountInput ")
    .WithParameter("@AccountInput", "Account1");

FeedIterator&lt;SalesOrder&gt; resultSet = container.GetItemQueryIterator&lt;SalesOrder&gt;(
    query,
    requestOptions: new QueryRequestOptions()
    {
        PartitionKey = new PartitionKey("Account1"),
        MaxItemCount = 1
    });
</code></pre>
<h3 id="create-stored-procedures">Create stored procedures</h3>
<p>Azure Cosmos DB provides language-integrated, transactional execution of JavaScript that lets you write <strong>stored procedures</strong>, <strong>triggers</strong>, and <strong>user-defined functions (UDFs)</strong>. To call a stored procedure, trigger, or user-defined function, you need to register it.</p>
<h4 id="writing-stored-procedures">Writing stored procedures</h4>
<p>Stored procedures can create, update, read, query, and delete items inside an Azure Cosmos container. Stored procedures are registered per collection, and can operate on any document or an attachment present in that collection.</p>
<p>Here is a simple stored procedure that returns a “Hello World” response.</p>
<pre><code>var helloWorldStoredProc = {
    id: "helloWorld",
    serverScript: function () {
        var context = getContext();
        var response = context.getResponse();

        response.setBody("Hello, World");
    }
}
</code></pre>
<p>The context object provides access to all operations that can be performed in Azure Cosmos DB, as well as access to the request and response objects.</p>
<h4 id="create-an-item-using-stored-procedure">Create an item using stored procedure</h4>
<p>When you create an item by using stored procedure it is inserted into the Azure Cosmos container and an ID for the newly created item is returned. Creating an item is an asynchronous operation and depends on the JavaScript callback functions. The callback function has two parameters:</p>
<ul>
<li>The error object in case the operation fails</li>
<li>A return value</li>
</ul>
<p>The following example stored procedure takes an input parameter named  <code>documentToCreate</code>  and the parameter’s value is the body of a document to be created in the current collection. The callback throws an error if the operation fails. Otherwise, it sets the  <code>id</code>  of the created document as the body of the response to the client.</p>
<pre><code>function createSampleDocument(documentToCreate) {
    var context = getContext();
    var collection = context.getCollection();
    var accepted = collection.createDocument(
        collection.getSelfLink(),
        documentToCreate,
        function (error, documentCreated) {                 
            context.getResponse().setBody(documentCreated.id)
        }
    );
    if (!accepted) return;
}
</code></pre>
<h4 id="arrays-as-input-parameters-for-stored-procedures">Arrays as input parameters for stored procedures</h4>
<p>When defining a stored procedure in the Azure portal, input parameters are always sent as a string to the stored procedure. Even if you pass an array of strings as an input, the array is converted to string and sent to the stored procedure. To work around this, you can define a function within your stored procedure to parse the string as an array.</p>
<h4 id="bounded-execution">Bounded execution</h4>
<p>All Azure Cosmos DB operations must complete within a limited amount of time. Stored procedures have a limited amount of time to run on the server.</p>
<h4 id="transactions-within-stored-procedures">Transactions within stored procedures</h4>
<p>You can implement transactions on items within a container by using a stored procedure. JavaScript functions can implement a continuation-based model to batch or resume execution.</p>
<p><img src="https://docs.microsoft.com/en-us/training/wwl-azure/work-with-cosmos-db/media/transaction-continuation-model.png" alt="This diagram depicts how the transaction continuation model can be used to repeat a server-side function until the function finishes its entire processing workload."></p>
<h3 id="create-triggers-and-user-defined-functions">Create triggers and user-defined functions</h3>
<p>Azure Cosmos DB supports pre-triggers and post-triggers. Pre-triggers are executed before modifying a database item and post-triggers are executed after modifying a database item. Triggers are not automatically executed, they must be specified for each database operation where you want them to execute.</p>
<h4 id="pre-triggers">Pre-triggers</h4>
<p>Pre-triggers cannot have any input parameters. The request object in the trigger is used to manipulate the request message associated with the operation.</p>
<p>When triggers are registered, you can specify the operations that it can run with. This trigger should be created with a <code>TriggerOperation</code> value of <code>TriggerOperation.Create</code>, which means using the trigger in a replace operation is not permitted.</p>
<h4 id="post-triggers">Post-triggers</h4>
<p>One thing that is important to note is the transactional execution of triggers in Azure Cosmos DB. The post-trigger runs as part of the same transaction for the underlying item itself. An exception during the post-trigger execution will fail the whole transaction. Anything committed will be rolled back and an exception returned.</p>
<h4 id="user-defined-functions">User-defined functions</h4>
<p>This user-defined functions are used inside a queries.</p>
<h1 id="az-204-implement-infrastructure-as-a-service-solutions">AZ-204: Implement infrastructure as a service solutions</h1>
<h2 id="provision-virtual-machines-in-azure">Provision virtual machines in Azure</h2>
<h3 id="explore-azure-virtual-machines">Explore Azure virtual machines</h3>
<p>Azure virtual machines (VM) is one of several types of on-demand, scalable computing resources that Azure offers. Typically, you choose a VM when you need more control over the computing environment than the other choices offer.</p>
<p>An Azure virtual machine gives you the flexibility of virtualization without having to buy and maintain the physical hardware that runs it. However, you still need to maintain the VM by performing tasks, such as configuring, patching, and installing the software that runs on it.</p>
<p>Azure virtual machines can be used in various ways:</p>
<ul>
<li>Development and test</li>
<li>Applications in the cloud</li>
<li>Extended datacenter</li>
</ul>
<h4 id="design-considerations-for-virtual-machine-creation">Design considerations for virtual machine creation</h4>
<p>There are always a multitude of design considerations when you build out an application infrastructure in Azure. These aspects of a VM are important to think about before you start:</p>
<ul>
<li>
<p><strong>Availability</strong>: Azure supports a single instance virtual machine Service Level Agreement of 99.9% provided you deploy the VM with premium storage for all disks.</p>
</li>
<li>
<p><strong>VM size</strong>: The size of the VM that you use is determined by the workload that you want to run.</p>
</li>
<li>
<p><strong>VM limits</strong>: Your subscription has default quota limits in place that could impact the deployment of many VMs for your project. The current limit on a per subscription basis is 20 VMs per region.</p>
</li>
<li>
<p><strong>VM image</strong>: You can either use your own image, or you can use one of the images in the Azure Marketplace.</p>
</li>
<li>
<p><strong>VM disks</strong>: There are two components that make up this area. The type of disks which determines the performance level and the storage account type that contains the disks. Azure provides two types of disks:</p>
<ul>
<li>
<p><strong>Standard disks</strong>: Backed by HDDs, and delivers cost-effective storage while still being performant. Standard disks are ideal for a cost effective dev and test workload.</p>
</li>
<li>
<p><strong>Premium disks</strong>: Backed by SSD-based, high-performance, low-latency disk. Perfect for VMs running production workload.</p>
</li>
</ul>
<p>And, there are two options for the disk storage:</p>
<ul>
<li>
<p><strong>Managed disks</strong>: Managed disks are the newer and recommended disk storage model and they are managed by Azure.</p>
</li>
<li>
<p><strong>Unmanaged disks</strong>: With unmanaged disks, you’re responsible for the storage accounts that hold the virtual hard disks (VHDs) that correspond to your VM disks.</p>
</li>
</ul>
</li>
</ul>
<h4 id="virtual-machine-extensions">Virtual machine extensions</h4>
<p>Windows VMs have extensions which give your VM additional capabilities through post deployment configuration and automated tasks. These common tasks can be accomplished using extensions:</p>
<ul>
<li><strong>Run custom scripts</strong>: The Custom Script Extension helps you configure workloads on the VM by running your script when the VM is provisioned.</li>
<li><strong>Deploy and manage configurations</strong>: The PowerShell Desired State Configuration (DSC) Extension helps you set up DSC on a VM to manage configurations and environments.</li>
<li><strong>Collect diagnostics data</strong>: The Azure Diagnostics Extension helps you configure the VM to collect diagnostics data that can be used to monitor the health of your application.</li>
</ul>
<h3 id="compare-virtual-machine-availability-options">Compare virtual machine availability options</h3>
<h4 id="availability-zones">Availability zones</h4>
<p>An Availability Zone is a physically separate zone, within an Azure region. There are three Availability Zones per supported Azure region.</p>
<p>Azure services that support Availability Zones fall into two categories:</p>
<ul>
<li><strong>Zonal services</strong>: Where a resource is pinned to a specific zone (for example, virtual machines, managed disks, Standard IP addresses), or</li>
<li><strong>Zone-redundant services</strong>: When the Azure platform replicates automatically across zones (for example, zone-redundant storage, SQL Database).</li>
</ul>
<h4 id="availability-sets">Availability sets</h4>
<p>An availability set is a logical grouping of VMs that allows Azure to understand how your application is built to provide for redundancy and availability. An availability set is composed of two additional groupings that protect against hardware failures and allow updates to safely be applied - fault domains (FDs) and update domains (UDs).</p>
<h5 id="fault-domains">Fault domains</h5>
<p>A fault domain is a logical group of underlying hardware that share a common power source and network switch, similar to a rack within an on-premises datacenter. As you create VMs within an availability set, the Azure platform automatically distributes your VMs across these fault domains.</p>
<p><img src="https://docs.microsoft.com/en-us/training/wwl-azure/provision-virtual-machines-azure/media/virtual-machine-fault-domains.png" alt="Image showing a representation of a Fault domains. Two separate hardware racks are shown with VMs and databases distributed across each."></p>
<h5 id="update-domains">Update domains</h5>
<p>An update domain is a logical group of underlying hardware that can undergo maintenance or be rebooted at the same time. As you create VMs within an availability set, the Azure platform automatically distributes your VMs across these update domains. This approach ensures that at least one instance of your application always remains running as the Azure platform undergoes periodic maintenance.</p>
<p><img src="https://docs.microsoft.com/en-us/training/wwl-azure/provision-virtual-machines-azure/media/virtual-machine-update-domains.png" alt="Conceptual drawing of the update domain and fault domain configuration. Image shows groups of hardware that can be maintained or rebooted at the same time."></p>
<h4 id="virtual-machine-scale-sets">Virtual machine scale sets</h4>
<p>Azure virtual machine scale sets let you create and manage a group of load balanced VMs.</p>
<h4 id="load-balancer">Load balancer</h4>
<p>Combine the <a href="https://docs.microsoft.com/en-us/azure/load-balancer/load-balancer-overview">Azure Load Balancer</a> with an availability zone or availability set to get the most application resiliency. An Azure load balancer is a Layer-4 (TCP, UDP) load balancer that provides high availability by distributing incoming traffic among healthy VMs.</p>
<p>You define a front-end IP configuration that contains one or more public IP addresses.</p>
<p>Virtual machines connect to a load balancer using their virtual network interface card (NIC). To distribute traffic to the VMs, a back-end address pool contains the IP addresses of the virtual (NICs) connected to the load balancer.</p>
<p>To control the flow of traffic, you define load balancer rules for specific ports and protocols that map to your VMs.</p>
<h3 id="determine-appropriate-virtual-machine-size">Determine appropriate virtual machine size</h3>

<table>
<thead>
<tr>
<th>VM Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>General Purpose</td>
<td>Balanced CPU-to-memory ratio. Ideal for testing and development, small to medium databases, and low to medium traffic web servers.</td>
</tr>
<tr>
<td>Compute Optimized</td>
<td>High CPU-to-memory ratio. Good for medium traffic web servers, network appliances, batch processes, and application servers.</td>
</tr>
<tr>
<td>Memory Optimized</td>
<td>High memory-to-CPU ratio. Great for relational database servers, medium to large caches, and in-memory analytics.</td>
</tr>
<tr>
<td>Storage Optimized</td>
<td>High disk throughput and IO ideal for Big Data, SQL, NoSQL databases, data warehousing and large transactional databases.</td>
</tr>
<tr>
<td>GPU</td>
<td>Specialized virtual machines targeted for heavy graphic rendering and video editing, as well as model training and inferencing (ND) with deep learning. Available with single or multiple GPUs.</td>
</tr>
<tr>
<td>High Performance Compute</td>
<td>Our fastest and most powerful CPU virtual machines with optional high-throughput network interfaces (RDMA).</td>
</tr>
</tbody>
</table><h2 id="create-and-deploy-azure-resource-manager-templates">Create and deploy Azure Resource Manager templates</h2>
<h3 id="explore-azure-resource-manager">Explore Azure Resource Manager</h3>
<p>Azure Resource Manager is the deployment and management service for Azure. It provides a management layer that enables you to create, update, and delete resources in your Azure subscription.</p>
<p>When a user sends a request from any of the Azure tools, APIs, or SDKs, Resource Manager receives the request. It authenticates and authorizes the request. Resource Manager sends the request to the Azure service, which takes the requested action. Because all requests are handled through the same API, you see consistent results and capabilities in all the different tools.</p>
<p><img src="https://docs.microsoft.com/en-us/training/wwl-azure/create-deploy-azure-resource-manager-templates/media/consistent-management-layer.png" alt="Resource Manager request model showing how Azure tools, APIs, or SDKs, interact with Azure Resource Manager."></p>
<h4 id="why-choose-azure-resource-manager-templates">Why choose Azure Resource Manager templates?</h4>
<ul>
<li><strong>Declarative syntax</strong>: Azure Resource Manager templates allow you to create and deploy an entire Azure infrastructure declaratively.</li>
<li><strong>Repeatable results</strong>: Repeatedly deploy your infrastructure throughout the development lifecycle and have confidence your resources are deployed in a consistent manner. Templates are idempotent, which means you can deploy the same template many times and get the same resource types in the same state.
<ul>
<li><strong>Orchestration</strong>: You don’t have to worry about the complexities of ordering operations. Resource Manager orchestrates the deployment of interdependent resources so they’re created in the correct order.</li>
</ul>
</li>
</ul>
<h4 id="template-file">Template file</h4>
<p>Within your template, you can write template expressions that extend the capabilities of JSON. These expressions make use of the <a href="https://docs.microsoft.com/en-us/azure/azure-resource-manager/templates/template-functions">functions</a> provided by Resource Manager.</p>
<p>The template has the following sections:</p>
<ul>
<li>
<p>Parameters - Provide values during deployment that allow the same template to be used with different environments.</p>
</li>
<li>
<p>Variables - Define values that are reused in your templates. They can be constructed from parameter values.</p>
</li>
<li>
<p>User-defined functions - Create customized functions that simplify your template.</p>
</li>
<li>
<p>Resources - Specify the resources to deploy.</p>
</li>
<li>
<p>Outputs - Return values from the deployed resources.</p>
</li>
</ul>
<h2 id="create-and-deploy-azure-resource-manager-templates-1">Create and deploy Azure Resource Manager templates</h2>
<h3 id="deploy-multi-tiered-solutions">Deploy multi-tiered solutions</h3>
<p>With Resource Manager, you can create a template (in JSON format) that defines the infrastructure and configuration of your Azure solution.</p>
<p>When you deploy a template, Resource Manager converts the template into REST API operations. For example, when Resource Manager receives a template with the following resource definition:</p>
<pre><code>"resources": [
  {
    "type": "Microsoft.Storage/storageAccounts",
    "apiVersion": "2019-04-01",
    "name": "mystorageaccount",
    "location": "westus",
    "sku": {
      "name": "Standard_LRS"
    },
    "kind": "StorageV2",
    "properties": {}
  }
]
</code></pre>
<p>It converts the definition to the following REST API operation, which is sent to the <code>Microsoft.Storage</code> resource provider:</p>
<pre><code>PUT
https://management.azure.com/subscriptions/{subscriptionId}/resourceGroups/{resourceGroupName}/providers/Microsoft.Storage/storageAccounts/mystorageaccount?api-version=2019-04-01
REQUEST BODY
{
  "location": "westus",
  "sku": {
    "name": "Standard_LRS"
  },
  "kind": "StorageV2",
  "properties": {}
}
</code></pre>
<p>Notice that the <strong>apiVersion</strong> you set in the template for the resource is used as the API version for the REST operation. You can repeatedly deploy the template and have confidence it will continue to work.</p>
<p>You can deploy a template using any of the following options:</p>
<ul>
<li>Azure portal</li>
<li>Azure CLI</li>
<li>PowerShell</li>
<li>REST API</li>
<li>Button in GitHub repository</li>
<li>Azure Cloud Shell</li>
</ul>
<h4 id="defining-multi-tiered-templates">Defining multi-tiered templates</h4>
<p>How you define templates and resource groups is entirely up to you and how you want to manage your solution.</p>
<p><img src="https://docs.microsoft.com/en-us/training/wwl-azure/create-deploy-azure-resource-manager-templates/media/three-tier-template.png" alt="Image showing the deployment of a virtual machine, app service, and SQL DB to a resource group in a single Azure Resource Manager template"></p>
<p>Often, it makes sense to divide your deployment requirements into a set of targeted, purpose-specific templates. You can easily reuse these templates for different solutions. To deploy a particular solution, you create a master template that links all the required templates.</p>
<p><img src="https://docs.microsoft.com/en-us/training/wwl-azure/create-deploy-azure-resource-manager-templates/media/nested-tiers-template.png" alt="Image showing three templates nested in a parent template. Each of the nested templates deploys one of the components of a 3-tier solution to a single resource group"></p>
<p>If you envision your tiers having separate lifecycles, you can deploy your three tiers to separate resource groups. The resources can still be linked to resources in other resource groups.</p>
<p>Azure Resource Manager analyzes dependencies to ensure resources are created in the correct order. If one resource relies on a value from another resource, you set a dependency.</p>
<p>You can also use the template for updates to the infrastructure. For example, you can add a resource to your solution and add configuration rules for the resources that are already deployed. If the template specifies creating a resource but that resource already exists, Azure Resource Manager performs an update instead of creating a new asset.</p>
<p>Resource Manager provides extensions for scenarios when you need additional operations such as installing particular software that isn’t included in the setup.</p>
<p>Finally, the template becomes part of the source code for your app.</p>
<p>After creating your template, you may wish to share it with other users in your organization. Template specs enable you to store a template as a resource type. You use role-based access control to manage access to the template spec.</p>
<h3 id="explore-conditional-deployment">Explore conditional deployment</h3>
<p>Sometimes you need to optionally deploy a resource in an Azure Resource Manager template. Use the <code>condition</code> element to specify whether the resource is deployed.<br>
Conditional deployment doesn’t cascade to child resources.</p>
<h4 id="new-or-existing-resource">New or existing resource</h4>
<p>You can use conditional deployment to create a new resource or use an existing one.</p>
<pre><code>{
  "$schema": "https://schema.management.azure.com/schemas/2019-04-01/deploymentTemplate.json#",
  "contentVersion": "1.0.0.0",
  "parameters": {
    "storageAccountName": {
      "type": "string"
    },
    "location": {
      "type": "string",
      "defaultValue": "[resourceGroup().location]"
    },
    "newOrExisting": {
      "type": "string",
      "defaultValue": "new",
      "allowedValues": [
        "new",
        "existing"
      ]
    }
  },
  "functions": [],
  "resources": [
    {
      "condition": "[equals(parameters('newOrExisting'), 'new')]",
      "type": "Microsoft.Storage/storageAccounts",
      "apiVersion": "2019-06-01",
      "name": "[parameters('storageAccountName')]",
      "location": "[parameters('location')]",
      "sku": {
        "name": "Standard_LRS",
        "tier": "Standard"
      },
      "kind": "StorageV2",
      "properties": {
        "accessTier": "Hot"
      }
    }
  ]
}
</code></pre>
<h4 id="runtime-functions">Runtime functions</h4>
<p>If you use a <code>reference</code> or <code>list</code> function with a resource that is conditionally deployed, the function is evaluated even if the resource isn’t deployed. You get an error if the function refers to a resource that doesn’t exist.</p>
<p>Use the <code>if</code> function to make sure the function is only evaluated for conditions when the resource is deployed.</p>
<p>You set a resource as dependent on a conditional resource exactly as you would any other resource. When a conditional resource isn’t deployed, Azure Resource Manager automatically removes it from the required dependencies.</p>
<h3 id="set-the-correct-deployment-mode">Set the correct deployment mode</h3>
<p>When deploying your resources, you specify that the deployment is either an incremental update or a complete update. The difference between these two modes is how Resource Manager handles existing resources in the resource group that aren’t in the template. The default mode is incremental.</p>
<p>For both modes, Resource Manager tries to create all resources specified in the template. If the resource already exists in the resource group and its settings are unchanged, no operation is taken for that resource. If you change the property values for a resource, the resource is updated with those new values. If you try to update the location or type of an existing resource, the deployment fails with an error. Instead, deploy a new resource with the location or type that you need.</p>
<h4 id="complete-mode">Complete mode</h4>
<p>In complete mode, Resource Manager <strong>deletes</strong> resources that exist in the resource group that aren’t specified in the template.</p>
<h4 id="incremental-mode">Incremental mode</h4>
<p>In incremental mode, Resource Manager <strong>leaves unchanged</strong> resources that exist in the resource group but aren’t specified in the template.</p>
<p>However, when redeploying an existing resource in incremental mode, the outcome is different. Specify all properties for the resource, not just the ones you’re updating.</p>
<h4 id="set-deployment-mode">Set deployment mode</h4>
<p>To set the deployment mode when deploying with PowerShell, use the <code>Mode</code> parameter.</p>
<pre><code>New-AzResourceGroupDeployment `
  -Mode Complete `
  -Name ExampleDeployment `
  -ResourceGroupName ExampleResourceGroup `
  -TemplateFile c:\MyTemplates\storage.json
</code></pre>
<h2 id="manage-container-images-in-azure-container-registry">Manage container images in Azure Container Registry</h2>
<h3 id="discover-the-azure-container-registry">Discover the Azure Container Registry</h3>
<p>Azure Container Registry (ACR) is a managed, private Docker registry service based on the open-source Docker Registry 2.0.</p>
<p>Use the Azure Container Registry (ACR) service with your existing container development and deployment pipelines, or use Azure Container Registry Tasks to build container images in Azure.</p>
<h4 id="use-cases">Use cases</h4>
<p>Pull images from an Azure container registry to various deployment targets:</p>
<ul>
<li><strong>Scalable orchestration systems</strong> that manage containerized applications across clusters of hosts, including Kubernetes, DC/OS, and Docker Swarm.</li>
<li><strong>Azure services</strong> that support building and running applications at scale, including Azure Kubernetes Service (AKS), App Service, Batch, Service Fabric, and others.</li>
</ul>
<p>Developers can also push to a container registry as part of a container development workflow. For example, target a container registry from a continuous integration and delivery tool such as Azure Pipelines or Jenkins.</p>
<p>Configure ACR Tasks to automatically rebuild application images when their base images are updated, or automate image builds when your team commits code to a Git repository. Create multi-step tasks to automate building, testing, and patching multiple container images in parallel in the cloud.</p>
<h4 id="azure-container-registry-service-tiers">Azure Container Registry service tiers</h4>
<p>Azure Container Registry is available in multiple service tiers.</p>

<table>
<thead>
<tr>
<th>Tier</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Basic</td>
<td>A cost-optimized entry point for developers learning about Azure Container Registry. The included storage and image throughput are most appropriate for lower usage scenarios.</td>
</tr>
<tr>
<td>Standard</td>
<td>Standard registries should satisfy the needs of most production scenarios.</td>
</tr>
<tr>
<td>Premium</td>
<td>Premium registries provide the highest amount of included storage and concurrent operations, enabling high-volume scenarios.</td>
</tr>
</tbody>
</table><h4 id="supported-images-and-artifacts">Supported images and artifacts</h4>
<p>Grouped in a repository, each image is a read-only snapshot of a Docker-compatible container. In addition to Docker container images, Azure Container Registry stores related content formats such as Helm charts and images built to the Open Container Initiative (OCI) Image Format Specification.</p>
<h4 id="automated-image-builds">Automated image builds</h4>
<p>Use Azure Container Registry Tasks (ACR Tasks) to streamline building, testing, pushing, and deploying images in Azure.</p>
<h3 id="explore-storage-capabilities">Explore storage capabilities</h3>
<p>Every Basic, Standard, and Premium Azure container registry benefits from advanced Azure storage features like encryption-at-rest for image data security and geo-redundancy for image data protection.</p>
<ul>
<li>
<p><strong>Encryption-at-rest:</strong> All container images in your registry are encrypted at rest.</p>
</li>
<li>
<p><strong>Regional storage:</strong> Azure Container Registry stores data in the region where the registry is created, to help customers meet data residency and compliance requirements. Azure may also store registry data in a paired region in the same geography.</p>
</li>
<li>
<p><strong>Zone redundancy:</strong> A feature of the Premium service tier, zone redundancy uses Azure availability zones to replicate your registry to a minimum of three separate zones in each enabled region.</p>
</li>
<li>
<p><strong>Scalable storage:</strong> Azure Container Registry allows you to create as many repositories, images, layers, or tags as you need, up to the registry storage limit. Very high numbers of repositories and tags can impact the performance of your registry.<br>
.</p>
</li>
</ul>
<h3 id="build-and-manage-containers-with-tasks">Build and manage containers with tasks</h3>
<p>ACR Tasks is a suite of features within Azure Container Registry. It provides cloud-based container image building for platforms. ACR Tasks enables automated builds triggered by source code updates, updates to a container’s base image, or timers.</p>
<h4 id="task-scenarios">Task scenarios</h4>
<p>ACR Tasks supports several scenarios to build and maintain container images and other artifacts.</p>
<ul>
<li>
<p><strong>Quick task</strong> - Build and push a single container image to a container registry on-demand, in Azure, without needing a local Docker Engine installation. Think <code>docker build</code>, <code>docker push</code> in the cloud.</p>
</li>
<li>
<p><strong>Automatically triggered tasks</strong> - Enable one or more <em>triggers</em> to build an image:</p>
<ul>
<li>Trigger on source code update</li>
<li>Trigger on base image update</li>
<li>Trigger on a schedule</li>
</ul>
</li>
<li>
<p><strong>Multi-step task</strong> - Extend the single image build-and-push capability of ACR Tasks with multi-step, multi-container-based workflows.</p>
</li>
</ul>
<p>Each ACR Task has an associated source code context - the location of a set of source files used to build a container image or other artifact. Example contexts include a Git repository or a local filesystem.</p>
<h3 id="explore-elements-of-a-dockerfile">Explore elements of a Dockerfile</h3>
<p>A Dockerfile is a text file that contains the instructions we use to build and run a Docker image. The following aspects of the image are defined:</p>
<ul>
<li>The base or parent image we use to create the new image</li>
<li>Commands to update the base OS and install additional software</li>
<li>Build artifacts to include, such as a developed application</li>
<li>Services to expose, such a storage and network configuration</li>
<li>Command to run when the container is launched</li>
</ul>
<p>There are several commands in a Dockerfile that allow us to manipulate the structure of the image.</p>
<p>Each of these steps creates a cached container image as we build the final container image. These temporary images are layered on top of the previous and presented as single image once all steps complete.</p>
<p>The <code>ENTRYPOINT</code> in the file indicates which process will execute once we run a container from an image.</p>
<h2 id="run-container-images-in-azure-container-instances">Run container images in Azure Container Instances</h2>
<h3 id="explore-azure-container-instances">Explore Azure Container Instances</h3>
<p>Azure Container Instances (ACI) offers the fastest and simplest way to run a container in Azure, without having to manage any virtual machines and without having to adopt a higher-level service. It is a great solution for any scenario that can operate in isolated containers, including simple applications, task automation, and build jobs.</p>
<ul>
<li><strong>Fast startup</strong>: ACI can start containers in Azure in seconds, without the need to provision and manage VMs</li>
<li><strong>Container access</strong>: ACI enables exposing your container groups directly to the internet with an IP address and a fully qualified domain name (FQDN)</li>
<li><strong>Hypervisor-level security</strong>: Isolate your application as completely as it would be in a VM</li>
<li><strong>Customer data</strong>: The ACI service stores the minimum customer data required to ensure your container groups are running as expected</li>
<li><strong>Custom sizes</strong>: ACI provides optimum utilization by allowing exact specifications of CPU cores and memory</li>
<li><strong>Persistent storage</strong>: Mount Azure Files shares directly to a container to retrieve and persist state</li>
<li><strong>Linux and Windows</strong>: Schedule both Windows and Linux containers using the same API.</li>
</ul>
<h4 id="container-groups">Container groups</h4>
<p>The top-level resource in Azure Container Instances is the <em>container group</em>. A container group is a collection of containers that get scheduled on the same host machine. The containers in a container group share a lifecycle, resources, local network, and storage volumes. It’s similar in concept to a <em>pod</em> in Kubernetes.</p>
<p>The following diagram shows an example of a container group that includes multiple containers:</p>
<p><img src="https://docs.microsoft.com/en-us/training/wwl-azure/create-run-container-images-azure-container-instances/media/container-groups-example.png" alt="Example container group with two containers, one listening on port 80 and the other listening on port 5000."></p>
<p>This example container group:</p>
<ul>
<li>Is scheduled on a single host machine.</li>
<li>Is assigned a DNS name label.</li>
<li>Exposes a single public IP address, with one exposed port.</li>
<li>Consists of two containers. One container listens on port 80, while the other listens on port 5000.</li>
<li>Includes two Azure file shares as volume mounts, and each container mounts one of the shares locally.</li>
</ul>
<p>Multi-container groups currently support only Linux containers. For Windows containers, Azure Container Instances only supports deployment of a single instance.</p>
<h4 id="deployment">Deployment</h4>
<p>There are two common ways to deploy a multi-container group: use a Resource Manager template or a YAML file. A Resource Manager template is recommended when you need to deploy additional Azure service resources (for example, an Azure Files share) when you deploy the container instances. Due to the YAML format’s more concise nature, a YAML file is recommended when your deployment includes only container instances.</p>
<h4 id="resource-allocation">Resource allocation</h4>
<p>Azure Container Instances allocates resources such as CPUs, memory, and optionally GPUs (preview) to a container group by adding the resource requests of the instances in the group.</p>
<h4 id="networking">Networking</h4>
<p>Container groups share an IP address and a port namespace on that IP address. To enable external clients to reach a container within the group, you must expose the port on the IP address and from the container. Because containers within the group share a port namespace, port mapping isn’t supported. Containers within a group can reach each other via localhost on the ports that they have exposed, even if those ports aren’t exposed externally on the group’s IP address.</p>
<h4 id="storage">Storage</h4>
<p>You can specify external volumes to mount within a container group. You can map those volumes into specific paths within the individual containers in a group. Supported volumes include:</p>
<ul>
<li>Azure file share</li>
<li>Secret</li>
<li>Empty directory</li>
<li>Cloned git repo</li>
</ul>
<h4 id="common-scenarios">Common scenarios</h4>
<p>Multi-container groups are useful in cases where you want to divide a single functional task into a small number of container images. These images can then be delivered by different teams and have separate resource requirements.</p>
<p>Example usage could include:</p>
<ul>
<li>A container serving a web application and a container pulling the latest content from source control.</li>
<li>An application container and a logging container.</li>
<li>An application container and a monitoring container. The monitoring container periodically makes a request to the application to ensure that it’s running and responding correctly, and raises an alert if it’s not.</li>
<li>A front-end container and a back-end container.</li>
</ul>
<h3 id="run-containerized-tasks-with-restart-policies">Run containerized tasks with restart policies</h3>
<p>The ease and speed of deploying containers in Azure Container Instances provides a compelling platform for executing run-once tasks like build, test, and image rendering in a container instance.</p>
<p>With a configurable restart policy, you can specify that your containers are stopped when their processes have completed. Because container instances are billed by the second, you’re charged only for the compute resources used while the container executing your task is running.</p>
<h4 id="container-restart-policy">Container restart policy</h4>
<p>When you create a container group in Azure Container Instances, you can specify one of three restart policy settings.</p>

<table>
<thead>
<tr>
<th>Restart policy</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>Always</code></td>
<td>Containers in the container group are always restarted (default).</td>
</tr>
<tr>
<td><code>Never</code></td>
<td>Containers in the container group are never restarted.</td>
</tr>
<tr>
<td><code>OnFailure</code></td>
<td>Containers in the container group are restarted only when the process executed in the container fails.</td>
</tr>
</tbody>
</table><p>When Azure Container Instances stops a container whose restart policy is <code>Never</code> or <code>OnFailure</code>, the container’s status is set to <strong>Terminated</strong>.</p>
<h3 id="set-environment-variables-in-container-instances">Set environment variables in container instances</h3>
<p>Setting environment variables in your container instances allows you to provide dynamic configuration of the application or script run by the container. This is similar to the <code>--env</code> command-line argument to <code>docker run</code>.</p>
<p>If you need to pass secrets as environment variables, Azure Container Instances supports secure values for both Windows and Linux containers.</p>
<p>In the example below two variables are passed to the container when it is created.</p>
<pre><code>az container create \
    --resource-group myResourceGroup \
    --name mycontainer2 \
    --image mcr.microsoft.com/azuredocs/aci-wordcount:latest 
    --restart-policy OnFailure \
    --environment-variables 'NumWords'='5' 'MinLength'='8'\
</code></pre>
<h4 id="secure-values">Secure values</h4>
<p>Objects with secure values are intended to hold sensitive information like passwords or keys for your application. Using secure values for environment variables is both safer and more flexible than including it in your container’s image.</p>
<p>Environment variables with secure values aren’t visible in your container’s properties. Container properties viewed in the Azure portal or Azure CLI display only a secure variable’s name, not its value.</p>
<p>Set a secure environment variable by specifying the <code>secureValue</code> property instead of the regular <code>value</code> for the variable’s type.</p>
<pre><code>apiVersion: 2018-10-01
location: eastus
name: securetest
properties:
  containers:
  - name: mycontainer
    properties:
      environmentVariables:
        - name: 'NOTSECRET'
          value: 'my-exposed-value'
        - name: 'SECRET'
          secureValue: 'my-secret-value'
      image: nginx
      ports: []
      resources:
        requests:
          cpu: 1.0
          memoryInGB: 1.5
  osType: Linux
  restartPolicy: Always
tags: null
type: Microsoft.ContainerInstance/containerGroups
</code></pre>
<h3 id="mount-an-azure-file-share-in-azure-container-instances">Mount an Azure file share in Azure Container Instances</h3>
<p>By default, Azure Container Instances are stateless. If the container crashes or stops, all of its state is lost. To persist state beyond the lifetime of the container, you must mount a volume from an external store. As shown in this unit, Azure Container Instances can mount an Azure file share created with Azure Files.</p>
<h4 id="limitations">Limitations</h4>
<ul>
<li>You can only mount Azure Files shares to Linux containers.</li>
<li>Azure file share volume mount requires the Linux container run as <em>root</em>.</li>
</ul>
<h4 id="deploy-container-and-mount-volume">Deploy container and mount volume</h4>
<pre><code>az container create \
    --resource-group $ACI_PERS_RESOURCE_GROUP \
    --name hellofiles \
    --image mcr.microsoft.com/azuredocs/aci-hellofiles \
    --dns-name-label aci-demo \
    --ports 80 \
    --azure-file-volume-account-name $ACI_PERS_STORAGE_ACCOUNT_NAME \
    --azure-file-volume-account-key $STORAGE_KEY \
    --azure-file-volume-share-name $ACI_PERS_SHARE_NAME \
    --azure-file-volume-mount-path /aci/logs/
</code></pre>
<p>The <code>--dns-name-label</code> value must be unique within the Azure region where you create the container instance. Update the value in the preceding command if you receive a <strong>DNS name label</strong> error message when you execute the command.</p>
<h2 id="deploy-container-and-mount-volume---yaml">Deploy container and mount volume - YAML</h2>
<p>You can also deploy a container group and mount a volume in a container with the Azure CLI and a YAML template. Deploying by YAML template is the preferred method when deploying container groups consisting of multiple containers.</p>
<pre><code>apiVersion: '2019-12-01'
location: eastus
name: file-share-demo
properties:
  containers:
  - name: hellofiles
    properties:
      environmentVariables: []
      image: mcr.microsoft.com/azuredocs/aci-hellofiles
      ports:
      - port: 80
      resources:
        requests:
          cpu: 1.0
          memoryInGB: 1.5
      volumeMounts:
      - mountPath: /aci/logs/
        name: filesharevolume
  osType: Linux
  restartPolicy: Always
  ipAddress:
    type: Public
    ports:
      - port: 80
    dnsNameLabel: aci-demo
  volumes:
  - name: filesharevolume
    azureFile:
      sharename: acishare
      storageAccountName: &lt;Storage account name&gt;
      storageAccountKey: &lt;Storage account key&gt;
tags: {}
type: Microsoft.ContainerInstance/containerGroups
</code></pre>
<h2 id="mount-multiple-volumes">Mount multiple volumes</h2>
<p>To mount multiple volumes in a container instance, you must deploy using an Azure Resource Manager template or a YAML file. To use a template or YAML file, provide the share details and define the volumes by populating the <code>volumes</code> array in the <code>properties</code> section of the template.</p>
<p>For example, if you created two Azure Files shares named <em>share1</em> and <em>share2</em> in storage account <em>myStorageAccount</em>, the <code>volumes</code> array in a Resource Manager template would appear similar to the following:</p>
<pre><code>"volumes": [{
  "name": "myvolume1",
  "azureFile": {
    "shareName": "share1",
    "storageAccountName": "myStorageAccount",
    "storageAccountKey": "&lt;storage-account-key&gt;"
  }
},
{
  "name": "myvolume2",
  "azureFile": {
    "shareName": "share2",
    "storageAccountName": "myStorageAccount",
    "storageAccountKey": "&lt;storage-account-key&gt;"
  }
}]
</code></pre>
<p>Next, for each container in the container group in which you’d like to mount the volumes, populate the <code>volumeMounts</code> array in the <code>properties</code> section of the container definition. For example, this mounts the two volumes, <em>myvolume1</em> and <em>myvolume2</em>, previously defined:</p>
<pre><code>"volumeMounts": [{
  "name": "myvolume1",
  "mountPath": "/mnt/share1/"
},
{
  "name": "myvolume2",
  "mountPath": "/mnt/share2/"
}]
</code></pre>
<h1 id="az-204-implement-user-authentication-and-authorization">AZ-204 Implement user authentication and authorization</h1>
<h2 id="explore-the-microsoft-identity-platform">Explore the Microsoft identity platform</h2>
<h3 id="explore-the-microsoft-identity-platform-1">Explore the Microsoft identity platform</h3>
<p>The Microsoft identity platform helps you build applications your users and customers can sign in to using their Microsoft identities or social accounts, and provide authorized access to your own APIs or Microsoft APIs like Microsoft Graph.</p>
<p>There are several components that make up the Microsoft identity platform:</p>
<ul>
<li>
<p><strong>OAuth 2.0 and OpenID Connect standard-compliant authentication service</strong> enabling developers to authenticate several identity types, including:</p>
<ul>
<li>Work or school accounts, provisioned through Azure Active Directory</li>
<li>Personal Microsoft account, like Skype, Xbox, and <a href="http://Outlook.com">Outlook.com</a></li>
<li>Social or local accounts, by using Azure Active Directory B2C</li>
</ul>
</li>
<li>
<p><strong>Open-source libraries</strong>: Microsoft Authentication Libraries (MSAL) and support for other standards-compliant libraries</p>
</li>
<li>
<p><strong>Application management portal</strong>: A registration and configuration experience in the Azure portal, along with the other Azure management capabilities.</p>
</li>
<li>
<p><strong>Application configuration API and PowerShell</strong>: Programmatic configuration of your applications through the Microsoft Graph API and PowerShell so you can automate your DevOps tasks.</p>
</li>
</ul>
<h4 id="explore-service-principals">Explore service principals</h4>
<p>To delegate Identity and Access Management functions to Azure Active Directory, an application must be registered with an Azure Active Directory tenant. When you register your application with Azure Active Directory, you’re creating an identity configuration for your application that allows it to integrate with Azure Active Directory. When you register an app in the Azure portal, you choose whether it is:</p>
<ul>
<li><strong>Single tenant</strong>: only accessible in your tenant</li>
<li><strong>Multi-tenant</strong>: accessible in other tenants</li>
</ul>
<p>If you register an application in the portal, an application object as well as a service principal object are automatically created in your home tenant. You also have a globally unique ID for your app (the app or client ID). In the portal, you can then add secrets or certificates and scopes to make your app work, customize the branding of your app in the sign-in dialog, and more.<br>
You can also create service principal objects in a tenant using Azure PowerShell, Azure CLI, Microsoft Graph, and other tools.</p>
<h4 id="application-object">Application object</h4>
<p>An Azure Active Directory application is defined by its one and only application object, which resides in the Azure Active Directory tenant where the application was registered (known as the application’s “home” tenant). An application object is used as a template or blueprint to create one or more service principal objects. A service principal is created in every tenant where the application is used. Similar to a class in object-oriented programming, the application object has some static properties that are applied to all the created service principals (or application instances).</p>
<p>The application object describes three aspects of an application: how the service can issue tokens in order to access the application, resources that the application might need to access, and the actions that the application can take.</p>
<p>The Microsoft Graph Application entity defines the schema for an application object’s properties.</p>
<h4 id="service-principal-object">Service principal object</h4>
<p>To access resources that are secured by an Azure Active Directory tenant, the entity that requires access must be represented by a security principal. This is true for both users (user principal) and applications (service principal).</p>
<p>The security principal defines the access policy and permissions for the user/application in the Azure Active Directory tenant. This enables core features such as authentication of the user/application during sign-in, and authorization during resource access.</p>
<p>There are three types of service principal:</p>
<ul>
<li>
<p><strong>Application</strong> - This type of service principal is the local representation, or application instance, of a global application object in a single tenant or directory. A service principal is created in each tenant where the application is used and references the globally unique app object. The service principal object defines what the app can actually do in the specific tenant, who can access the app, and what resources the app can access.</p>
</li>
<li>
<p><strong>Managed identity</strong> - This type of service principal is used to represent a managed identity. Managed identities provide an identity for applications to use when connecting to resources that support Azure Active Directory authentication. When a managed identity is enabled, a service principal representing that managed identity is created in your tenant. Service principals representing managed identities can be granted access and permissions, but cannot be updated or modified directly.</p>
</li>
<li>
<p><strong>Legacy</strong> - This type of service principal represents a legacy app, which is an app created before app registrations were introduced or an app created through legacy experiences. A legacy service principal can have credentials, service principal names, reply URLs, and other properties that an authorized user can edit, but does not have an associated app registration. The service principal can only be used in the tenant where it was created.</p>
</li>
</ul>
<h4 id="relationship-between-application-objects-and-service-principals">Relationship between application objects and service principals</h4>
<p>The application object is the <em>global</em> representation of your application for use across all tenants, and the service principal is the <em>local</em> representation for use in a specific tenant. The application object serves as the template from which common and default properties are <em>derived</em> for use in creating corresponding service principal objects.</p>
<p>An application object has:</p>
<ul>
<li>A 1:1 relationship with the software application, and</li>
<li>A 1:many relationship with its corresponding service principal object(s).</li>
</ul>
<p>A service principal must be created in each tenant where the application is used, enabling it to establish an identity for sign-in and/or access to resources being secured by the tenant. A single-tenant application has only one service principal (in its home tenant), created and consented for use during application registration. A multi-tenant application also has a service principal created in each tenant where a user from that tenant has consented to its use.</p>
<h4 id="discover-permissions-and-consent">Discover permissions and consent</h4>
<p>Applications that integrate with the Microsoft identity platform follow an authorization model that gives users and administrators control over how data can be accessed.</p>
<p>The Microsoft identity platform implements the OAuth 2.0 authorization protocol. OAuth 2.0 is a method through which a third-party app can access web-hosted resources on behalf of a user. Any web-hosted resource that integrates with the Microsoft identity platform has a resource identifier, or <em>application ID URI</em>.</p>
<p>Here are some examples of Microsoft web-hosted resources:</p>
<ul>
<li>Microsoft Graph: <code>https://graph.microsoft.com</code></li>
<li>Microsoft 365 Mail API: <code>https://outlook.office.com</code></li>
<li>Azure Key Vault: <code>https://vault.azure.net</code></li>
</ul>
<p>The same is true for any third-party resources that have integrated with the Microsoft identity platform. Any of these resources also can define a set of permissions that can be used to divide the functionality of that resource into smaller chunks. When a resource’s functionality is chunked into small permission sets, third-party apps can be built to request only the permissions that they need to perform their function. Users and administrators can know what data the app can access.</p>
<p>In OAuth 2.0, these types of permission sets are called <em>scopes</em>. They’re also often referred to as <em>permissions</em>. In the Microsoft identity platform, a permission is represented as a string value. An app requests the permissions it needs by specifying the permission in the <code>scope</code> query parameter. Identity platform supports several well-defined OpenID Connect scopes as well as resource-based permissions (each permission is indicated by appending the permission value to the resource’s identifier or application ID URI). For example, the permission string <code>https://graph.microsoft.com/Calendars.Read</code> is used to request permission to read users calendars in Microsoft Graph.</p>
<p>An app most commonly requests these permissions by specifying the scopes in requests to the Microsoft identity platform authorize endpoint. However, some high-privilege permissions can be granted only through administrator consent. They can be requested or granted by using the administrator consent endpoint.</p>
<h4 id="permission-types">Permission types</h4>
<p>The Microsoft identity platform supports two types of permissions: <em>delegated permissions</em> and <em>application permissions</em>.</p>
<ul>
<li>
<p><strong>Delegated permissions</strong> are used by apps that have a signed-in user present. For these apps, either the user or an administrator consents to the permissions that the app requests. The app is delegated with the permission to act as a signed-in user when it makes calls to the target resource.</p>
</li>
<li>
<p><strong>Application permissions</strong> are used by apps that run without a signed-in user present, for example, apps that run as background services or daemons. Only an administrator can consent to application permissions.</p>
</li>
</ul>
<h4 id="consent-types">Consent types</h4>
<p>Applications in Microsoft identity platform rely on consent in order to gain access to necessary resources or APIs. There are a number of kinds of consent that your app may need to know about in order to be successful. If you are defining permissions, you will also need to understand how your users will gain access to your app or API.</p>
<p>There are three consent types: <em>static user consent</em>, <em>incremental and dynamic user consent</em>, and <em>admin consent</em>.</p>
<h5 id="static-user-consent">Static user consent</h5>
<p>In the static user consent scenario, you must specify all the permissions the app needs in its configuration in the Azure portal. If the user (or administrator, as appropriate) has not granted consent for this app, then Microsoft identity platform will prompt the user to provide consent at this time. Static permissions also enables administrators to consent on behalf of all users in the organization.</p>
<p>While static permissions of the app defined in the Azure portal keep the code nice and simple, it presents some possible issues for developers:</p>
<ul>
<li>
<p>The app needs to request all the permissions it would ever need upon the user’s first sign-in. This can lead to a long list of permissions that discourages end users from approving the app’s access on initial sign-in.</p>
</li>
<li>
<p>The app needs to know all of the resources it would ever access ahead of time. It is difficult to create apps that could access an arbitrary number of resources.</p>
</li>
</ul>
<h5 id="incremental-and-dynamic-user-consent">Incremental and dynamic user consent</h5>
<p>With the Microsoft identity platform endpoint, you can ignore the static permissions defined in the app registration information in the Azure portal and request permissions incrementally instead. You can ask for a minimum set of permissions upfront and request more over time as the customer uses additional app features.</p>
<p>To do so, you can specify the scopes your app needs at any time by including the new scopes in the <code>scope</code> parameter when requesting an access token - without the need to pre-define them in the application registration information. If the user hasn’t yet consented to new scopes added to the request, they’ll be prompted to consent only to the new permissions. Incremental, or dynamic consent, only applies to delegated permissions and not to application permissions.</p>
<p>Dynamic consent can be convenient, but presents a big challenge for permissions that require admin consent, since the admin consent experience doesn’t know about those permissions at consent time. If you require admin privileged permissions or if your app uses dynamic consent, you must register all of the permissions in the Azure portal (not just the subset of permissions that require admin consent). This enables tenant admins to consent on behalf of all their users.</p>
<h5 id="admin-consent">Admin consent</h5>
<p>Admin consent is required when your app needs access to certain high-privilege permissions. Admin consent ensures that administrators have some additional controls before authorizing apps or users to access highly privileged data from the organization.</p>
<p>Admin consent done on behalf of an organization still requires the static permissions registered for the app. Set those permissions for apps in the app registration portal if you need an admin to give consent on behalf of the entire organization. This reduces the cycles required by the organization admin to set up the application.</p>
<h4 id="requesting-individual-user-consent">Requesting individual user consent</h4>
<p>In an OpenID Connect or OAuth 2.0 authorization request, an app can request the permissions it needs by using the scope query parameter. For example, when a user signs in to an app, the app sends a request like the following example.</p>
<pre><code>GET https://login.microsoftonline.com/common/oauth2/v2.0/authorize?
client_id=6731de76-14a6-49ae-97bc-6eba6914391e
&amp;response_type=code
&amp;redirect_uri=http%3A%2F%2Flocalhost%2Fmyapp%2F
&amp;response_mode=query
&amp;scope=
https%3A%2F%2Fgraph.microsoft.com%2Fcalendars.read%20
https%3A%2F%2Fgraph.microsoft.com%2Fmail.send
&amp;state=12345
</code></pre>
<p>The <code>scope</code> parameter is a space-separated list of delegated permissions that the app is requesting. Each permission is indicated by appending the permission value to the resource’s identifier (the application ID URI). In the request example, the app needs permission to read the user’s calendar and send mail as the user.</p>
<p>After the user enters their credentials, the Microsoft identity platform checks for a matching record of <em>user consent</em>. If the user hasn’t consented to any of the requested permissions in the past, and if the administrator hasn’t consented to these permissions on behalf of the entire organization, the Microsoft identity platform asks the user to grant the requested permissions.</p>
<h3 id="discover-conditional-access">Discover conditional access</h3>
<p>The Conditional Access feature in Azure Active Directory offers one of several ways that you can use to secure your app and protect a service. Conditional Access enables developers and enterprise customers to protect services in a multitude of ways including:</p>
<ul>
<li>Multifactor authentication</li>
<li>Allowing only Intune enrolled devices to access specific services</li>
<li>Restricting user locations and IP ranges</li>
</ul>
<h4 id="how-does-conditional-access-impact-an-app">How does Conditional Access impact an app?</h4>
<p>In most common cases, Conditional Access does not change an app’s behavior or require any changes from the developer. Only in certain cases when an app indirectly or silently requests a token for a service does an app require code changes to handle Conditional Access challenges. It may be as simple as performing an interactive sign-in request.</p>
<p>Specifically, the following scenarios require code to handle Conditional Access challenges:</p>
<ul>
<li>Apps performing the on-behalf-of flow</li>
<li>Apps accessing multiple services/resources</li>
<li>Single-page apps using MSAL.js</li>
<li>Web apps calling a resource</li>
</ul>
<p>Conditional Access policies can be applied to the app and also a web API your app accesses. Depending on the scenario, an enterprise customer can apply and remove Conditional Access policies at any time. For your app to continue functioning when a new policy is applied, implement challenge handling.</p>
<h4 id="conditional-access-examples">Conditional Access examples</h4>
<p>Some scenarios require code changes to handle Conditional Access whereas others work as is. Here are a few scenarios using Conditional Access to do multifactor authentication that gives some insight into the difference.</p>
<ul>
<li>
<p>You are building a single-tenant iOS app and apply a Conditional Access policy. The app signs in a user and doesn’t request access to an API. When the user signs in, the policy is automatically invoked and the user needs to perform multifactor authentication.</p>
</li>
<li>
<p>You are building a native app that uses a middle tier service to access a downstream API. An enterprise customer at the company using this app applies a policy to the downstream API. When an end user signs in, the native app requests access to the middle tier and sends the token. The middle tier performs on-behalf-of flow to request access to the downstream API. At this point, a claims “challenge” is presented to the middle tier. The middle tier sends the challenge back to the native app, which needs to comply with the Conditional Access policy.</p>
</li>
</ul>
<h2 id="implement-authentication-by-using-the-microsoft-authentication-library">Implement authentication by using the Microsoft Authentication Library</h2>
<h3 id="explore-the-microsoft-authentication-library">Explore the Microsoft Authentication Library</h3>
<p>The Microsoft Authentication Library (MSAL) enables developers to acquire tokens from the Microsoft identity platform in order to authenticate users and access secured web APIs.</p>
<p>MSAL gives you many ways to get tokens, with a consistent API for a number of platforms. Using MSAL provides the following benefits:</p>
<ul>
<li>No need to directly use the OAuth libraries or code against the protocol in your application.</li>
<li>Acquires tokens on behalf of a user or on behalf of an application.</li>
<li>Maintains a token cache and refreshes tokens for you when they are close to expire.</li>
<li>Helps you specify which audience you want your application to sign in.</li>
<li>Helps you set up your application from configuration files.</li>
<li>Helps you troubleshoot your app by exposing actionable exceptions, logging, and telemetry.</li>
</ul>
<h4 id="application-types-and-scenarios">Application types and scenarios</h4>
<p>Using MSAL, a token can be acquired from a number of application types: web applications, web APIs, single-page apps (JavaScript), mobile and native applications, and daemons and server-side applications. MSAL currently supports the platforms and frameworks listed in the table below.</p>
<ul>
<li>MSAL for Android</li>
<li>MSAL Angular</li>
<li>MSAL for iOS and macOS</li>
<li>MSAL Go (Preview)</li>
<li>MSAL Java</li>
<li>MSAL.js</li>
<li><a href="http://MSAL.NET">MSAL.NET</a></li>
<li>MSAL Node</li>
<li>MSAL Python</li>
<li>MSAL React</li>
</ul>
<h4 id="authentication-flows">Authentication flows</h4>

<table>
<thead>
<tr>
<th>Flow</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Authorization code</td>
<td>Native and web apps securely obtain tokens in the name of the user</td>
</tr>
<tr>
<td>Client credentials</td>
<td>Service applications run without user interaction</td>
</tr>
<tr>
<td>On-behalf-of</td>
<td>The application calls a service/web API, which in turns calls Microsoft Graph</td>
</tr>
<tr>
<td>Implicit</td>
<td>Used in browser-based applications</td>
</tr>
<tr>
<td>Device code</td>
<td>Enables sign-in to a device by using another device that has a browser</td>
</tr>
<tr>
<td>Integrated Windows</td>
<td>Windows computers silently acquire an access token when they are domain joined</td>
</tr>
<tr>
<td>Interactive</td>
<td>Mobile and desktops applications call Microsoft Graph in the name of a user</td>
</tr>
<tr>
<td>Username/password</td>
<td>The application signs in a user by using their username and password</td>
</tr>
</tbody>
</table><h3 id="public-client-and-confidential-client-applications">Public client, and confidential client applications</h3>
<p>Security tokens can be acquired by multiple types of applications. These applications tend to be separated into the following two categories. Each is used with different libraries and objects.</p>
<ul>
<li>
<p><strong>Public client applications</strong>: Are apps that run on devices or desktop computers or in a web browser. They’re not trusted to safely keep application secrets, so they only access web APIs on behalf of the user.</p>
</li>
<li>
<p><strong>Confidential client applications</strong>: Are apps that run on servers (web apps, web API apps, or even service/daemon apps). They’re considered difficult to access, and for that reason capable of keeping an application secret.</p>
</li>
</ul>
<h4 id="initialize-client-applications">Initialize client applications</h4>
<p>With <a href="http://MSAL.NET">MSAL.NET</a> 3.x, the recommended way to instantiate an application is by using the application builders: <code>PublicClientApplicationBuilder</code> and <code>ConfidentialClientApplicationBuilder</code>. They offer a powerful mechanism to configure the application either from the code, or from a configuration file, or even by mixing both approaches.</p>
<p>Before initializing an application, you first need to register it so that your app can be integrated with the Microsoft identity platform. After registration, you may need the following information (which can be found in the Azure portal):</p>
<ul>
<li>The client ID</li>
<li>The identity provider URL (named the instance) and the sign-in audience for your application. These two parameters are collectively known as the authority.</li>
<li>The tenant ID if you are writing a line of business application solely for your organization.</li>
<li>The application secret (client secret string) or certificate (of type X509Certificate2) if it’s a confidential client app.</li>
<li>For web apps, and sometimes for public client apps (in particular when your app needs to use a broker), you’ll have also set the redirectUri where the identity provider will contact back your application with the security tokens.</li>
</ul>
<h4 id="initializing-public-and-confidential-client-applications-from-code">Initializing public and confidential client applications from code</h4>
<pre><code>IPublicClientApplication app = PublicClientApplicationBuilder.Create(clientId).Build();
</code></pre>
<pre><code>string redirectUri = "https://myapp.azurewebsites.net";
IConfidentialClientApplication app = ConfidentialClientApplicationBuilder.Create(clientId)
    .WithClientSecret(clientSecret)
    .WithRedirectUri(redirectUri )
    .Build();
</code></pre>
<h4 id="builder-modifiers">Builder modifiers</h4>
<p>In the code snippets using application builders, a number of <code>.With</code> methods can be applied as modifiers.</p>
<h5 id="modifiers-common-to-public-and-confidential-client-applications">Modifiers common to public and confidential client applications</h5>
<p>The table below lists some of the modifiers you can set on a public, or client confidential client.</p>

<table>
<thead>
<tr>
<th>Modifier</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>.WithAuthority()</code></td>
<td>Sets the application default authority to an Azure Active Directory authority, with the possibility of choosing the Azure Cloud, the audience, the tenant (tenant ID or domain name), or providing directly the authority URI.</td>
</tr>
<tr>
<td><code>.WithTenantId(string tenantId)</code></td>
<td>Overrides the tenant ID, or the tenant description.</td>
</tr>
<tr>
<td><code>.WithClientId(string)</code></td>
<td>Overrides the client ID.</td>
</tr>
<tr>
<td><code>.WithRedirectUri(string redirectUri)</code></td>
<td>Overrides the default redirect URI. In the case of public client applications, this will be useful for scenarios requiring a broker.</td>
</tr>
<tr>
<td><code>.WithComponent(string)</code></td>
<td>Sets the name of the library using <a href="http://MSAL.NET">MSAL.NET</a> (for telemetry reasons).</td>
</tr>
<tr>
<td><code>.WithDebugLoggingCallback()</code></td>
<td>If called, the application will call <code>Debug.Write</code> simply enabling debugging traces.</td>
</tr>
<tr>
<td><code>.WithLogging()</code></td>
<td>If called, the application will call a callback with debugging traces.</td>
</tr>
<tr>
<td><code>.WithTelemetry(TelemetryCallback telemetryCallback)</code></td>
<td>Sets the delegate used to send telemetry.</td>
</tr>
</tbody>
</table><h5 id="modifiers-specific-to-confidential-client-applications">Modifiers specific to confidential client applications</h5>
<p>The modifiers you can set on a confidential client application builder are:</p>

<table>
<thead>
<tr>
<th>Modifier</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>.WithCertificate(X509Certificate2 certificate)</code></td>
<td>Sets the certificate identifying the application with Azure Active Directory.</td>
</tr>
<tr>
<td><code>.WithClientSecret(string clientSecret)</code></td>
<td>Sets the client secret (app password) identifying the application with Azure Active Directory.</td>
</tr>
</tbody>
</table><h3 id="implement-shared-access-signatures">Implement shared access signatures</h3>
<h4 id="discover-shared-access-signatures">Discover shared access signatures</h4>
<p>A shared access signature (SAS) is a URI that grants restricted access rights to Azure Storage resources. You can provide a shared access signature to clients that you want to grant delegate access to certain storage account resources.</p>
<p>A SAS is a signed URI that points to one or more storage resources and includes a token that contains a special set of query parameters. The token indicates how the resources may be accessed by the client. One of the query parameters, the signature, is constructed from the SAS parameters and signed with the key that was used to create the SAS. This signature is used by Azure Storage to authorize access to the storage resource.</p>
<p>Azure Storage supports three types of shared access signatures:</p>
<ul>
<li>
<p><strong>User delegation SAS</strong> (recommended best practice): A user delegation SAS is secured with Azure Active Directory credentials and also by the permissions specified for the SAS. A user delegation SAS applies to Blob storage only.</p>
</li>
<li>
<p><strong>Service SAS</strong>: A service SAS is secured with the storage account key. A service SAS delegates access to a resource in the following Azure Storage services: Blob storage, Queue storage, Table storage, or Azure Files.</p>
</li>
<li>
<p><strong>Account SAS</strong>: An account SAS is secured with the storage account key. An account SAS delegates access to resources in one or more of the storage services. All of the operations available via a service or user delegation SAS are also available via an account SAS.</p>
</li>
</ul>
<p>When you use a SAS to access data stored in Azure Storage, you need two components. The first is a URI to the resource you want to access. The second part is a SAS token that you’ve created to authorize access to that resource.</p>
<p>In a single URI, such as <code>https://medicalrecords.blob.core.windows.net/patient-images/patient-116139-nq8z7f.jpg?sp=r&amp;st=2020-01-20T11:42:32Z&amp;se=2020-01-20T19:42:32Z&amp;spr=https&amp;sv=2019-02-02&amp;sr=b&amp;sig=SrW1HZ5Nb6MbRzTbXCaPm%2BJiSEn15tC91Y4umMPwVZs%3D</code>, you can separate the URI from the SAS token as follows:</p>
<ul>
<li><strong>URI:</strong> <code>https://medicalrecords.blob.core.windows.net/patient-images/patient-116139-nq8z7f.jpg?</code></li>
<li><strong>SAS token:</strong> <code>sp=r&amp;st=2020-01-20T11:42:32Z&amp;se=2020-01-20T19:42:32Z&amp;spr=https&amp;sv=2019-02-02&amp;sr=b&amp;sig=SrW1HZ5Nb6MbRzTbXCaPm%2BJiSEn15tC91Y4umMPwVZs%3D</code></li>
</ul>
<p>The SAS token itself is made up of several components.</p>

<table>
<thead>
<tr>
<th>Component</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>sp=r</code></td>
<td>Controls the access rights. The values can be <code>a</code> for add, <code>c</code> for create, <code>d</code> for delete, <code>l</code> for list, <code>r</code> for read, or <code>w</code> for write. This example is read only. The example <code>sp=acdlrw</code> grants all the available rights.</td>
</tr>
<tr>
<td><code>st=2020-01-20T11:42:32Z</code></td>
<td>The date and time when access starts.</td>
</tr>
<tr>
<td><code>se=2020-01-20T19:42:32Z</code></td>
<td>The date and time when access ends. This example grants eight hours of access.</td>
</tr>
<tr>
<td><code>sv=2019-02-02</code></td>
<td>The version of the storage API to use.</td>
</tr>
<tr>
<td><code>sr=b</code></td>
<td>The kind of storage being accessed. In this example, b is for blob.</td>
</tr>
<tr>
<td><code>sig=SrW1HZ5Nb6MbRzTbXCaPm%2BJiSEn15tC91Y4umMPwVZs%3D</code></td>
<td>The cryptographic signature.</td>
</tr>
</tbody>
</table><h4 id="best-practices">Best practices</h4>
<p>To reduce the potential risks of using a SAS, Microsoft provides some guidance:</p>
<ul>
<li>To securely distribute a SAS and prevent man-in-the-middle attacks, always use HTTPS.</li>
<li>The most secure SAS is a user delegation SAS.</li>
<li>Try to set your expiration time to the smallest useful value.</li>
<li>Apply the rule of minimum-required privileges. Only grant the access that’s required.</li>
<li>There are some situations where a SAS isn’t the correct solution. When there’s an unacceptable risk of using a SAS, create a middle-tier service to manage users and their access to storage.</li>
</ul>
<h4 id="choose-when-to-use-shared-access-signatures">Choose when to use shared access signatures</h4>
<p>Use a SAS when you want to provide secure access to resources in your storage account to any client who does not otherwise have permissions to those resources.</p>
<p>A common scenario where a SAS is useful is a service where users read and write their own data to your storage account. In a scenario where a storage account stores user data, there are two typical design patterns:</p>
<ul>
<li>
<p>Clients upload and download data via a front-end proxy service, which performs authentication. This front-end proxy service has the advantage of allowing validation of business rules, but for large amounts of data or high-volume transactions, creating a service that can scale to match demand may be expensive or difficult.</p>
<p><img src="https://docs.microsoft.com/en-us/training/wwl-azure/implement-shared-access-signatures/media/storage-proxy-service.png" alt="Scenario diagram: Front-end proxy service"></p>
</li>
<li>
<p>A lightweight service authenticates the client as needed and then generates a SAS. Once the client application receives the SAS, they can access storage account resources directly with the permissions defined by the SAS and for the interval allowed by the SAS. The SAS mitigates the need for routing all data through the front-end proxy service.</p>
<p><img src="https://docs.microsoft.com/en-us/training/wwl-azure/implement-shared-access-signatures/media/storage-provider-service.png" alt="Scenario diagram: SAS provider service"></p>
</li>
</ul>
<p>Many real-world services may use a hybrid of these two approaches. For example, some data might be processed and validated via the front-end proxy, while other data is saved and/or read directly using SAS.</p>
<p>Additionally, a SAS is required to authorize access to the source object in a copy operation in certain scenarios:</p>
<ul>
<li>
<p>When you copy a blob to another blob that resides in a different storage account, you must use a SAS to authorize access to the source blob. You can optionally use a SAS to authorize access to the destination blob as well.</p>
</li>
<li>
<p>When you copy a file to another file that resides in a different storage account, you must use a SAS to authorize access to the source file. You can optionally use a SAS to authorize access to the destination file as well.</p>
</li>
<li>
<p>When you copy a blob to a file, or a file to a blob, you must use a SAS to authorize access to the source object, even if the source and destination objects reside within the same storage account.</p>
</li>
</ul>
<h3 id="explore-stored-access-policies">Explore stored access policies</h3>
<p>A stored access policy provides an additional level of control over service-level shared access signatures (SAS) on the server side. Establishing a stored access policy groups shared access signatures and provides additional restrictions for signatures that are bound by the policy.</p>
<p>The following storage resources support stored access policies:</p>
<ul>
<li>Blob containers</li>
<li>File shares</li>
<li>Queues</li>
<li>Tables</li>
</ul>
<h4 id="creating-a-stored-access-policy">Creating a stored access policy</h4>
<p>The access policy for a SAS consists of the start time, expiry time, and permissions for the signature. You can specify all of these parameters on the signature URI and none within the stored access policy; all on the stored access policy and none on the URI; or some combination of the two. However, you cannot specify a given parameter on both the SAS token and the stored access policy.</p>
<p>To create or modify a stored access policy, call the <code>Set ACL</code> operation for the resource with a request body that specifies the terms of the access policy. The body of the request includes a unique signed identifier of your choosing, up to 64 characters in length, and the optional parameters of the access policy.</p>
<p>Below are examples of creating a stored access policy by using C# .NET and the Azure CLI.</p>
<pre><code>BlobSignedIdentifier identifier = new BlobSignedIdentifier
{
    Id = "stored access policy identifier",
    AccessPolicy = new BlobAccessPolicy
    {
        ExpiresOn = DateTimeOffset.UtcNow.AddHours(1),
        Permissions = "rw"
    }
};

blobContainer.SetAccessPolicy(permissions: new BlobSignedIdentifier[] { identifier });
</code></pre>
<pre><code>az storage container policy create \
    --name &lt;stored access policy identifier&gt; \
    --container-name &lt;container name&gt; \
    --start &lt;start time UTC datetime&gt; \
    --expiry &lt;expiry time UTC datetime&gt; \
    --permissions &lt;(a)dd, (c)reate, (d)elete, (l)ist, (r)ead, or (w)rite&gt; \
    --account-key &lt;storage account key&gt; \
    --account-name &lt;storage account name&gt; \
</code></pre>
<h4 id="modifying-or-revoking-a-stored-access-policy">Modifying or revoking a stored access policy</h4>
<p>To modify the parameters of the stored access policy you can call the access control list operation for the resource type to replace the existing policy.</p>
<p>To revoke a stored access policy you can delete it, rename it by changing the signed identifier, or change the expiry time to a value in the past.</p>
<p>To remove a single access policy, call the resource’s <code>Set ACL</code> operation, passing in the set of signed identifiers that you wish to maintain on the container. To remove all access policies from the resource, call the <code>Set ACL</code> operation with an empty request body.</p>
<h2 id="explore-microsoft-graph">Explore Microsoft Graph</h2>
<h3 id="discover-microsoft-graph">Discover Microsoft Graph</h3>
<p>Microsoft Graph is the gateway to data and intelligence in Microsoft 365. It provides a unified programmability model that you can use to access the tremendous amount of data in Microsoft 365, Windows 10, and Enterprise Mobility + Security.</p>
<p><img src="https://docs.microsoft.com/en-us/training/wwl-azure/microsoft-graph/media/microsoft-graph-data-connectors.png" alt="Microsoft Graph, Microsoft Graph data connect, and Microsoft Graph connectors enable extending Microsoft 365 experiences and building intelligent apps."></p>
<p>In the Microsoft 365 platform, three main components facilitate the access and flow of data:</p>
<ul>
<li>
<p>The Microsoft Graph API offers a single endpoint, <code>https://graph.microsoft.com</code>. You can use REST APIs or SDKs to access the endpoint. Microsoft Graph also includes a powerful set of services that manage user and device identity, access, compliance, security, and help protect organizations from data leakage or loss.</p>
</li>
<li>
<p>Microsoft Graph connectors work in the incoming direction, <strong>delivering data external to the Microsoft cloud into Microsoft Graph services and applications</strong>, to enhance Microsoft 365 experiences such as Microsoft Search.</p>
</li>
<li>
<p>Microsoft Graph Data Connect provides a set of tools to streamline secure and scalable <strong>delivery of Microsoft Graph data to popular Azure data stores</strong>. The cached data serves as data sources for Azure development tools that you can use to build intelligent applications.</p>
</li>
</ul>
<h3 id="query-microsoft-graph-by-using-rest">Query Microsoft Graph by using REST</h3>
<p>Microsoft Graph is a RESTful web API that enables you to access Microsoft Cloud service resources. After you register your app and get authentication tokens for a user or service, you can make requests to the Microsoft Graph API.</p>
<p>The Microsoft Graph API defines most of its resources, methods, and enumerations in the OData namespace, <code>microsoft.graph</code>, in the Microsoft Graph metadata. A small number of API sets are defined in their sub-namespaces, such as the call records APIwhich defines resources like callRecord in <code>microsoft.graph.callRecords</code>.</p>
<h4 id="call-a-rest-api-method">Call a REST API method</h4>
<p>To read from or write to a resource such as a user or an email message, you construct a request that looks like the following:</p>
<pre><code>{HTTP method} https://graph.microsoft.com/{version}/{resource}?{query-parameters}
</code></pre>
<p>The components of a request include:</p>
<ul>
<li><code>{HTTP method}</code> - The HTTP method used on the request to Microsoft Graph.</li>
<li><code>{version}</code> - The version of the Microsoft Graph API your application is using.</li>
<li><code>{resource}</code> - The resource in Microsoft Graph that you’re referencing.</li>
<li><code>{query-parameters}</code> - Optional OData query options or REST method parameters that customize the response.</li>
</ul>
<p>After you make a request, a response is returned that includes:</p>
<ul>
<li>Status code - An HTTP status code that indicates success or failure.</li>
<li>Response message - The data that you requested or the result of the operation.</li>
<li><code>nextLink</code> - If your request returns a lot of data, you need to page through it by using the URL returned in <code>@odata.nextLink</code>.</li>
</ul>
<h4 id="http-methods">HTTP methods</h4>
<p>Microsoft Graph uses the HTTP method on your request to determine what your request is doing. The API supports the following methods.</p>

<table>
<thead>
<tr>
<th>Method</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>GET</td>
<td>Read data from a resource.</td>
</tr>
<tr>
<td>POST</td>
<td>Create a new resource, or perform an action.</td>
</tr>
<tr>
<td>PATCH</td>
<td>Update a resource with new values.</td>
</tr>
<tr>
<td>PUT</td>
<td>Replace a resource with a new one.</td>
</tr>
<tr>
<td>DELETE</td>
<td>Remove a resource.</td>
</tr>
</tbody>
</table><ul>
<li>For the CRUD methods <code>GET</code> and <code>DELETE</code>, no request body is required.</li>
<li>The <code>POST</code>, <code>PATCH</code>, and <code>PUT</code> methods require a request body, usually specified in JSON format, that contains additional information, such as the values for properties of the resource.</li>
</ul>
<h4 id="version">Version</h4>
<p>Microsoft Graph currently supports two versions: <code>v1.0</code> and <code>beta</code>.</p>
<ul>
<li><code>v1.0</code> includes generally available APIs.</li>
<li><code>beta</code> includes APIs that are currently in preview.</li>
</ul>
<h4 id="resource">Resource</h4>
<p>A resource can be an entity or complex type, commonly defined with properties. Entities differ from complex types by always including an <strong>id</strong> property.</p>
<p>Your URL will include the resource you are interacting with in the request, such as <code>me</code>, <strong>user</strong>, <strong>group</strong>, <strong>drive</strong>, and <strong>site</strong>. Often, top-level resources also include <em>relationships</em>, which you can use to access additional resources, like <code>me/messages</code> or <code>me/drive</code>. You can also interact with resources using <em>methods</em>; for example, to send an email, use <code>me/sendMail</code>.</p>
<p>Each resource might require different permissions to access it. You will often need a higher level of permissions to create or update a resource than to read it. For details about required permissions, see the method reference topic.</p>
<h4 id="query-parameters">Query parameters</h4>
<p>Query parameters can be OData system query options, or other strings that a method accepts to customize its response.</p>
<pre><code>GET https://graph.microsoft.com/v1.0/me/messages?filter=emailAddress eq 'jon@contoso.com'
</code></pre>
<h3 id="query-microsoft-graph-by-using-sdks">Query Microsoft Graph by using SDKs</h3>
<p>The Microsoft Graph SDKs are designed to simplify building high-quality, efficient, and resilient applications that access Microsoft Graph. The SDKs include two components: a service library and a core library.</p>
<ul>
<li>
<p>The service library contains models and request builders that are generated from Microsoft Graph metadata to provide a rich, strongly typed, and discoverable experience when working with the many datasets available in Microsoft Graph.</p>
</li>
<li>
<p>The core library provides a set of features that enhance working with all the Microsoft Graph services.</p>
</li>
</ul>
<h4 id="install-the-microsoft-graph-.net-sdk">Install the Microsoft Graph .NET SDK</h4>
<p>The Microsoft Graph .NET SDK is included in the following NuGet packages:</p>
<ul>
<li><a href="https://github.com/microsoftgraph/msgraph-sdk-dotnet">Microsoft.Graph</a> - Contains the models and request builders for accessing the <code>v1.0</code> endpoint with the fluent API.</li>
<li><a href="https://github.com/microsoftgraph/msgraph-beta-sdk-dotnet">Microsoft.Graph.Beta</a> - Contains the models and request builders for accessing the <code>beta</code> endpoint with the fluent API.</li>
<li><a href="https://github.com/microsoftgraph/msgraph-sdk-dotnet">Microsoft.Graph.Core</a> - The core library for making calls to Microsoft Graph.</li>
<li><a href="https://github.com/microsoftgraph/msgraph-sdk-dotnet-auth">Microsoft.Graph.Auth</a> - Provides an authentication scenario-based wrapper of the Microsoft Authentication Library (MSAL) for use with the Microsoft Graph SDK.</li>
</ul>
<h4 id="create-a-microsoft-graph-client">Create a Microsoft Graph client</h4>
<p>The Microsoft Graph client is designed to make it simple to make calls to Microsoft Graph. You can use a single client instance for the lifetime of the application. The following code examples show how to create an instance of a Microsoft Graph client. The authentication provider will handle acquiring access tokens for the application.</p>
<pre><code>// Build a client application.
IPublicClientApplication publicClientApplication = PublicClientApplicationBuilder
            .Create("INSERT-CLIENT-APP-ID")
            .Build();
// Create an authentication provider by passing in a client application and graph scopes.
DeviceCodeProvider authProvider = new DeviceCodeProvider(publicClientApplication, graphScopes);
// Create a new instance of GraphServiceClient with the authentication provider.
GraphServiceClient graphClient = new GraphServiceClient(authProvider);
</code></pre>
<h4 id="read-information-from-microsoft-graph">Read information from Microsoft Graph</h4>
<pre><code>// GET https://graph.microsoft.com/v1.0/me

var user = await graphClient.Me
    .Request()
    .GetAsync();
</code></pre>
<h4 id="retrieve-a-list-of-entities">Retrieve a list of entities</h4>
<p>Retrieving a list of entities is similar to retrieving a single entity except there a number of other options for configuring the request. The <code>$filter</code> query parameter can be used to reduce the result set to only those rows that match the provided condition. The <code>$orderBy</code> query parameter will request that the server provide the list of entities sorted by the specified properties.</p>
<pre><code>// GET https://graph.microsoft.com/v1.0/me/messages?$select=subject,sender&amp;$filter=&lt;some condition&gt;&amp;orderBy=receivedDateTime

var messages = await graphClient.Me.Messages
    .Request()
    .Select(m =&gt; new {
        m.Subject,
        m.Sender
    })
    .Filter("&lt;filter condition&gt;")
    .OrderBy("receivedDateTime")
    .GetAsync();
</code></pre>
<h4 id="delete-an-entity">Delete an entity</h4>
<pre><code>// DELETE https://graph.microsoft.com/v1.0/me/messages/{message-id}

string messageId = "AQMkAGUy...";
var message = await graphClient.Me.Messages[messageId]
    .Request()
    .DeleteAsync();
</code></pre>
<h4 id="create-a-new-entity">Create a new entity</h4>
<pre><code>// POST https://graph.microsoft.com/v1.0/me/calendars

var calendar = new Calendar
{
    Name = "Volunteer"
};

var newCalendar = await graphClient.Me.Calendars
    .Request()
    .AddAsync(calendar);
</code></pre>
<h3 id="apply-best-practices-to-microsoft-graph">Apply best practices to Microsoft Graph</h3>
<h4 id="authentication">Authentication</h4>
<p>To access the data in Microsoft Graph, your application will need to acquire an OAuth 2.0 access token, and present it to Microsoft Graph in either of the following:</p>
<ul>
<li>The HTTP <em>Authorization</em> request header, as a <em>Bearer</em> token</li>
<li>The graph client constructor, when using a Microsoft Graph client library</li>
</ul>
<p>Use the Microsoft Authentication Library API (MSAL),  to acquire the access token to Microsoft Graph.</p>
<p>Apply the following best practices for consent and authorization in your app:</p>
<ul>
<li>
<p><strong>Use least privilege</strong>. Only request permissions that are absolutely necessary, and only when you need them.</p>
</li>
<li>
<p><strong>Use the correct permission type based on scenarios</strong>. If you’re building an interactive application where a signed in user is present, your application should use <em>delegated</em> permissions. If, however, your application runs without a signed-in user, such as a background service or daemon, your application should use application permissions.</p>
</li>
<li>
<p><strong>Consider the end user and admin experience</strong>. For example:</p>
<ul>
<li>
<p>Consider who will be consenting to your application, either end users or administrators, and configure your application to request permissions appropriately.</p>
</li>
<li>
<p>Ensure that you understand the difference between static, dynamic and incremental consent</p>
</li>
</ul>
</li>
<li>
<p><strong>Consider multi-tenant applications</strong>. Expect customers to have various application and consent controls in different states. For example:</p>
<ul>
<li>
<p>Tenant administrators can disable the ability for end users to consent to applications. In this case, an administrator would need to consent on behalf of their users.</p>
</li>
<li>
<p>Tenant administrators can set custom authorization policies such as blocking users from reading other user’s profiles, or limiting self-service group creation to a limited set of users. In this case, your application should expect to handle 403 error response when acting on behalf of a user.</p>
</li>
</ul>
</li>
</ul>
<h4 id="handle-responses-effectively">Handle responses effectively</h4>
<p>Depending on the requests you make to Microsoft Graph, your applications should be prepared to handle different types of responses. For example:</p>
<ul>
<li>
<p><strong>Pagination</strong>: When querying a resource collection, you should expect that Microsoft Graph will the return result set in multiple pages, due to server-side page size limits.</p>
</li>
<li>
<p><strong>Evolvable enumerations</strong>: Adding members to existing enumerations can break applications already using these enums. Evolvable enums is a mechanism that Microsoft Graph API uses to add new members to existing enumerations without causing a breaking change for applications. By default, a GET operation returns only known members for properties of evolvable enum types and your application needs to handle only the known members. If you design your application to handle unknown members as well, you can opt-in to receive those members by using an HTTP <code>Prefer</code> request header.</p>
</li>
</ul>
<h4 id="storing-data-locally">Storing data locally</h4>
<p>Your application should ideally make calls to Microsoft Graph to retrieve data in real time as necessary. You should only cache or store data locally if necessary for a specific scenario, and if that use case is covered by your terms of use and privacy policy, and does not violate the Microsoft APIs Terms of Use.</p>
<h1 id="az-204-implement-secure-cloud-solutions">AZ-204 Implement secure cloud solutions</h1>
<h2 id="implement-azure-key-vault">Implement Azure Key Vault</h2>
<h3 id="explore-azure-key-vault">Explore Azure Key Vault</h3>
<p>Azure Key Vault is a cloud service for securely storing and accessing secrets. A secret is anything that you want to tightly control access to, such as API keys, passwords, certificates, or cryptographic keys.</p>
<p>The Azure Key Vault service supports two types of containers:</p>
<ul>
<li>Vaults support storing software and HSM-backed keys, secrets, and certificates.</li>
<li>Managed HSM pools only support HSM-backed keys.</li>
</ul>
<p>Azure Key Vault helps solve the following problems:</p>
<ul>
<li>
<p><strong>Secrets Management:</strong> Azure Key Vault can be used to Securely store and tightly control access to tokens, passwords, certificates, API keys, and other secrets</p>
</li>
<li>
<p><strong>Key Management:</strong> Azure Key Vault can also be used as a Key Management solution. Azure Key Vault makes it easy to create and control the encryption keys used to encrypt your data.</p>
</li>
<li>
<p><strong>Certificate Management:</strong> Azure Key Vault is also a service that lets you easily provision, manage, and deploy public and private Secure Sockets Layer/Transport Layer Security (SSL/TLS) certificates for use with Azure and your internal connected resources.</p>
</li>
</ul>
<p>Azure Key Vault has two service tiers:</p>
<ul>
<li>Standard, which encrypts with a software key</li>
<li>Premium tier, which includes hardware security module(HSM)-protected keys.</li>
</ul>
<h4 id="key-benefits-of-using-azure-key-vault">Key benefits of using Azure Key Vault</h4>
<ul>
<li>
<p><strong>Centralized application secrets:</strong> Centralizing storage of application secrets in Azure Key Vault allows you to control their distribution. For example, your applications can securely access connection string by using URIs. These URIs allow the applications to retrieve specific versions of a secret.</p>
</li>
<li>
<p><strong>Securely store secrets and keys:</strong> Access to a key vault requires proper authentication and authorization before a caller (user or application) can get access. Authentication is done via Azure Active Directory. Authorization may be done via Azure role-based access control (Azure RBAC) or Key Vault access policy. Azure RBAC is used when dealing with the management of the vaults and key vault access policy is used when attempting to access data stored in a vault. Azure Key Vaults may be either software-protected or, with the Azure Key Vault Premium tier, hardware-protected by hardware security modules (HSMs).</p>
</li>
<li>
<p><strong>Monitor access and use:</strong> You can monitor activity by enabling logging for your vaults. You have control over your logs and you may secure them by restricting access and you may also delete logs that you no longer need. Azure Key Vault can be configured to:</p>
<ul>
<li>Archive to a storage account.</li>
<li>Stream to an event hub.</li>
<li>Send the logs to Azure Monitor logs.</li>
</ul>
</li>
<li>
<p><strong>Simplified administration of application secrets:</strong> Security information must be secured, it must follow a life cycle, and it must be highly available. Azure Key Vault simplifies the process of meeting these requirements by:</p>
<ul>
<li>Removing the need for in-house knowledge of Hardware Security Modules</li>
<li>Scaling up on short notice to meet your organization’s usage spikes.</li>
<li>Replicating the contents of your Key Vault within a region and to a secondary region. Data replication ensures high availability and takes away the need of any action from the administrator to trigger the failover.</li>
<li>Providing standard Azure administration options via the portal, Azure CLI and PowerShell.</li>
<li>Automating certain tasks on certificates that you purchase from Public CAs, such as enrollment and renewal.</li>
</ul>
</li>
</ul>
<h3 id="discover-azure-key-vault-best-practices">Discover Azure Key Vault best practices</h3>
<p>Azure Key Vault is a tool for securely storing and accessing secrets.</p>
<h4 id="authentication-1">Authentication</h4>
<p>To do any operations with Key Vault, you first need to authenticate to it. There are three ways to authenticate to Key Vault:</p>
<ul>
<li>
<p><strong>Managed identities for Azure resources</strong>: When you deploy an app on a virtual machine in Azure, you can assign an identity to your virtual machine that has access to Key Vault or other Azure resources. Azure automatically rotates the service principal client secret associated with the identity. We recommend this approach as a best practice.</p>
</li>
<li>
<p><strong>Service principal and certificate</strong>: You can use a service principal and an associated certificate that has access to Key Vault. We don’t recommend this approach because the application owner or developer must rotate the certificate.</p>
</li>
<li>
<p><strong>Service principal and secret</strong>: Although you can use a service principal and a secret to authenticate to Key Vault, we don’t recommend it. It’s hard to automatically rotate the bootstrap secret that’s used to authenticate to Key Vault.</p>
</li>
</ul>
<h4 id="encryption-of-data-in-transit">Encryption of data in transit</h4>
<p>Azure Key Vault enforces Transport Layer Security (TLS) protocol to protect data when it’s traveling between Azure Key Vault and clients.</p>
<p>Perfect Forward Secrecy (PFS) protects connections between customers’ client systems and Microsoft cloud services by unique keys. Connections also use RSA-based 2,048-bit encryption key lengths.</p>
<h4 id="azure-key-vault-best-practices">Azure Key Vault best practices</h4>
<ul>
<li>
<p><strong>Use separate key vaults:</strong> Recommended to use a vault per application per environment.</p>
</li>
<li>
<p><strong>Control access to your vault:</strong> Key Vault data is sensitive and business critical, you need to secure access to your key vaults by allowing only authorized applications and users.</p>
</li>
<li>
<p><strong>Backup:</strong> Create regular back ups of your vault on update/delete/create of objects within a Vault.</p>
</li>
<li>
<p><strong>Logging:</strong> Be sure to turn on logging and alerts.</p>
</li>
<li>
<p><strong>Recovery options:</strong> Turn on soft-delete and purge protection if you want to guard against force deletion of the secret.</p>
</li>
</ul>
<h3 id="authenticate-to-azure-key-vault">Authenticate to Azure Key Vault</h3>
<p>Authentication with Key Vault works in conjunction with Azure Active Directory, which is responsible for authenticating the identity of any given security principal.</p>
<p>For applications, there are two ways to obtain a service principal:</p>
<ul>
<li>
<p>Enable a system-assigned <strong>managed identity</strong> for the application. With managed identity, Azure internally manages the application’s service principal and automatically authenticates the application with other Azure services.</p>
</li>
<li>
<p>If you cannot use managed identity, you instead register the application with your Azure AD tenant. Registration also creates a second application object that identifies the app across all tenants.</p>
</li>
</ul>
<h4 id="authentication-to-key-vault-in-application-code">Authentication to Key Vault in application code</h4>
<p>Key Vault SDK is using Azure Identity client library, which allows seamless authentication to Key Vault across environments with same code. Available Azure Identity client libraries:</p>
<ul>
<li>Azure Identity SDK .NET</li>
<li>Azure Identity SDK Python</li>
<li>Azure Identity SDK Java</li>
<li>Azure Identity SDK JavaScript</li>
</ul>
<h4 id="authentication-to-key-vault-with-rest">Authentication to Key Vault with REST</h4>
<p>Access tokens must be sent to the service using the HTTP Authorization header:</p>
<pre><code>PUT /keys/MYKEY?api-version=&lt;api_version&gt;  HTTP/1.1  
Authorization: Bearer &lt;access_token&gt;
</code></pre>
<p>When an access token is not supplied, or when a token is not accepted by the service, an HTTP 401 error will be returned to the client and will include the <code>WWW-Authenticate</code> header, for example:</p>
<pre><code>401 Not Authorized  
WWW-Authenticate: Bearer authorization="…", resource="…"
</code></pre>
<p>The parameters on the <code>WWW-Authenticate</code> header are:</p>
<ul>
<li>
<p>authorization: The address of the OAuth2 authorization service that may be used to obtain an access token for the request.</p>
</li>
<li>
<p>resource: The name of the resource (<code>https://vault.azure.net</code>) to use in the authorization request.</p>
</li>
</ul>
<h2 id="implement-managed-identities">Implement managed identities</h2>
<h3 id="explore-managed-identities">Explore managed identities</h3>
<p>A common challenge for developers is the management of secrets and credentials used to secure communication between different components making up a solution. Managed identities eliminate the need for developers to manage credentials.</p>
<p>Managed identities provide an identity for applications to use when connecting to resources that support Azure Active Directory (Azure AD) authentication. Applications may use the managed identity to obtain Azure AD tokens. For example, an application may use a managed identity to access resources like Azure Key Vault where developers can store credentials in a secure manner or to access storage accounts.</p>
<h4 id="types-of-managed-identities">Types of managed identities</h4>
<p>There are two types of managed identities:</p>
<ul>
<li>
<p><strong>system-assigned managed identity</strong></p>
</li>
<li>
<p><strong>user-assigned managed identity</strong></p>
</li>
</ul>
<h4 id="characteristics-of-managed-identities">Characteristics of managed identities</h4>

<table>
<thead>
<tr>
<th>Characteristic</th>
<th>System-assigned managed identity</th>
<th>User-assigned managed identity</th>
</tr>
</thead>
<tbody>
<tr>
<td>Creation</td>
<td>Created as part of an Azure resource (for example, an Azure virtual machine or Azure App Service)</td>
<td>Created as a stand-alone Azure resource</td>
</tr>
<tr>
<td>Lifecycle</td>
<td>Shared lifecycle with the Azure resource that the managed identity is created with. When the parent resource is deleted, the managed identity is deleted as well.</td>
<td>Independent life-cycle. Must be explicitly deleted.</td>
</tr>
<tr>
<td>Sharing across Azure resources</td>
<td>Cannot be shared, it can only be associated with a single Azure resource.</td>
<td>Can be shared, the same user-assigned managed identity can be associated with more than one Azure resource.</td>
</tr>
</tbody>
</table><h4 id="when-to-use-managed-identities">When to use managed identities</h4>
<p>The image below gives an overview of the scenarios that support using managed identities. For example, you can use managed identities if you want to build an app using Azure App Services that accesses Azure Storage without having to manage any credentials.</p>
<p><img src="https://docs.microsoft.com/en-us/training/wwl-azure/implement-managed-identities/media/managed-identities-use-case.png" alt="Image showing a list of sources that gain access to targets through Azure Active Directory."></p>
<h4 id="what-azure-services-support-managed-identities">What Azure services support managed identities?</h4>
<p>Managed identities for Azure resources can be used to authenticate to services that support Azure Active Directory authentication.</p>
<h3 id="discover-the-managed-identities-authentication-flow">Discover the managed identities authentication flow</h3>
<h4 id="how-a-system-assigned-managed-identity-works-with-an-azure-virtual-machine">How a system-assigned managed identity works with an Azure virtual machine</h4>
<ol>
<li>
<p>Azure Resource Manager receives a request to enable the system-assigned managed identity on a virtual machine.</p>
</li>
<li>
<p>Azure Resource Manager creates a service principal in Azure Active Directory for the identity of the virtual machine. The service principal is created in the Azure Active Directory tenant that’s trusted by the subscription.</p>
</li>
<li>
<p>Azure Resource Manager configures the identity on the virtual machine by updating the Azure Instance Metadata Service identity endpoint with the service principal client ID and certificate.</p>
</li>
<li>
<p>After the virtual machine has an identity, use the service principal information to grant the virtual machine access to Azure resources. To call Azure Resource Manager, use role-based access control in Azure Active Directory to assign the appropriate role to the virtual machine service principal. To call Key Vault, grant your code access to the specific secret or key in Key Vault.</p>
</li>
<li>
<p>Your code that’s running on the virtual machine can request a token from the Azure Instance Metadata service endpoint, accessible only from within the virtual machine: <code>http://169.254.169.254/metadata/identity/oauth2/token</code></p>
</li>
<li>
<p>A call is made to Azure Active Directory to request an access token (as specified in step 5) by using the client ID and certificate configured in step 3. Azure Active Directory returns a JSON Web Token (JWT) access token.</p>
</li>
<li>
<p>Your code sends the access token on a call to a service that supports Azure Active Directory authentication.</p>
</li>
</ol>
<h4 id="how-a-user-assigned-managed-identity-works-with-an-azure-virtual-machine">How a user-assigned managed identity works with an Azure virtual machine</h4>
<ol>
<li>
<p>Azure Resource Manager receives a request to create a user-assigned managed identity.</p>
</li>
<li>
<p>Azure Resource Manager creates a service principal in Azure Active Directory for the user-assigned managed identity. The service principal is created in the Azure Active Directory tenant that’s trusted by the subscription.</p>
</li>
<li>
<p>Azure Resource Manager receives a request to configure the user-assigned managed identity on a virtual machine and updates the Azure Instance Metadata Service identity endpoint with the user-assigned managed identity service principal client ID and certificate.</p>
</li>
<li>
<p>After the user-assigned managed identity is created, use the service principal information to grant the identity access to Azure resources. To call Azure Resource Manager, use role-based access control in Azure Active Directory to assign the appropriate role to the service principal of the user-assigned identity. To call Key Vault, grant your code access to the specific secret or key in Key Vault.</p>
</li>
<li>
<p>Your code that’s running on the virtual machine can request a token from the Azure Instance Metadata Service identity endpoint, accessible only from within the virtual machine: <code>http://169.254.169.254/metadata/identity/oauth2/token</code></p>
</li>
<li>
<p>A call is made to Azure Active Directory to request an access token (as specified in step 5) by using the client ID and certificate configured in step 3. Azure Active Directory returns a JSON Web Token (JWT) access token.</p>
</li>
<li>
<p>Your code sends the access token on a call to a service that supports Azure Active Directory authentication.</p>
</li>
</ol>
<h3 id="configure-managed-identities">Configure managed identities</h3>
<p>You can configure an Azure virtual machine with a managed identity during, or after, the creation of the virtual machine.</p>
<h4 id="system-assigned-managed-identity">System-assigned managed identity</h4>
<p>To create, or enable, an Azure virtual machine with the system-assigned managed identity your account needs the <strong>Virtual Machine Contributor</strong> role assignment. No additional Azure AD directory role assignments are required.</p>
<h5 id="enable-system-assigned-managed-identity-during-creation-of-an-azure-virtual-machine">Enable system-assigned managed identity during creation of an Azure virtual machine</h5>
<p>The following example creates a virtual machine named <em>myVM</em> with a system-assigned managed identity, as requested by the <code>--assign-identity</code> parameter, with the specified <code>--role</code> and <code>--scope</code>. The <code>--admin-username</code> and <code>--admin-password</code> parameters specify the administrative user name and password account for virtual machine sign-in. Update these values as appropriate for your environment:</p>
<pre><code>az vm create --resource-group myResourceGroup \ 
    --name myVM --image win2016datacenter \ 
    --generate-ssh-keys \ 
    --assign-identity \ 
    --role contributor \
    --scope mySubscription \
    --admin-username azureuser \ 
    --admin-password myPassword12
</code></pre>
<h5 id="enable-system-assigned-managed-identity-on-an-existing-azure-virtual-machine">Enable system-assigned managed identity on an existing Azure virtual machine</h5>
<pre><code>az vm identity assign -g myResourceGroup -n myVm
</code></pre>
<h4 id="user-assigned-managed-identity">User-assigned managed identity</h4>
<p>To assign a user-assigned identity to a virtual machine during its creation, your account needs the <strong>Virtual Machine Contributor</strong> and <strong>Managed Identity Operator</strong> role assignments. No additional Azure AD directory role assignments are required.</p>
<p>Enabling user-assigned managed identities is a two-step process:</p>
<ol>
<li>Create the user-assigned identity</li>
<li>Assign the identity to a virtual machine</li>
</ol>
<h5 id="create-a-user-assigned-identity">Create a user-assigned identity</h5>
<pre><code>az identity create -g myResourceGroup -n myUserAssignedIdentity
</code></pre>
<h5 id="assign-a-user-assigned-managed-identity-during-the-creation-of-an-azure-virtual-machine">Assign a user-assigned managed identity during the creation of an Azure virtual machine</h5>
<pre><code>az vm create \
--resource-group &lt;RESOURCE GROUP&gt; \
--name &lt;VM NAME&gt; \
--image UbuntuLTS \
--admin-username &lt;USER NAME&gt; \
--admin-password &lt;PASSWORD&gt; \
--assign-identity &lt;USER ASSIGNED IDENTITY NAME&gt; \
--role &lt;ROLE&gt; \
--scope &lt;SUBSCRIPTION&gt;
</code></pre>
<h5 id="assign-a-user-assigned-managed-identity-to-an-existing-azure-virtual-machine">Assign a user-assigned managed identity to an existing Azure virtual machine</h5>
<pre><code>az vm identity assign \
    -g &lt;RESOURCE GROUP&gt; \
    -n &lt;VM NAME&gt; \
    --identities &lt;USER ASSIGNED IDENTITY&gt;
</code></pre>
<h3 id="acquire-an-access-token">Acquire an access token</h3>
<p>A client application can request managed identities for Azure resources app-only access token for accessing a given resource. The token is based on the managed identities for Azure resources service principal. As such, there is no need for the client to register itself to obtain an access token under its own service principal. The token is suitable for use as a bearer token in service-to-service calls requiring client credentials.</p>
<p>This unit provides various code and script examples for token acquisition, as well as guidance on important topics such as handling token expiration and HTTP errors.</p>
<h4 id="acquire-a-token">Acquire a token</h4>
<p>The fundamental interface for acquiring an access token is based on REST, making it accessible to any client application running on the VM that can make HTTP REST calls. This is similar to the Azure AD programming model, except the client uses an endpoint on the virtual machine versus an Azure AD endpoint.</p>
<p>Sample request using the Azure Instance Metadata Service (IMDS) endpoint:</p>
<pre><code>GET 'http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&amp;resource=https://management.azure.com/' HTTP/1.1 Metadata: true
</code></pre>

<table>
<thead>
<tr>
<th>Element</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>GET</code></td>
<td>The HTTP verb, indicating you want to retrieve data from the endpoint. In this case, an OAuth access token.</td>
</tr>
<tr>
<td><code>http://169.254.169.254/metadata/identity/oauth2/token</code></td>
<td>The managed identities for Azure resources endpoint for the Instance Metadata Service.</td>
</tr>
<tr>
<td><code>api-version</code></td>
<td>A query string parameter, indicating the API version for the IMDS endpoint. Please use API version <code>2018-02-01</code> or greater.</td>
</tr>
<tr>
<td><code>resource</code></td>
<td>A query string parameter, indicating the App ID URI of the target resource. It also appears in the <code>aud</code> (audience) claim of the issued token. This example requests a token to access Azure Resource Manager, which has an App ID URI of <code>https://management.azure.com/</code>.</td>
</tr>
<tr>
<td><code>Metadata</code></td>
<td>An HTTP request header field, required by managed identities for Azure resources as a mitigation against Server Side Request Forgery (SSRF) attack. This value must be set to “true”, in all lower case.</td>
</tr>
</tbody>
</table><p>Sample response:</p>
<pre><code>HTTP/1.1 200 OK
Content-Type: application/json
{
  "access_token": "eyJ0eXAi...",
  "refresh_token": "",
  "expires_in": "3599",
  "expires_on": "1506484173",
  "not_before": "1506480273",
  "resource": "https://management.azure.com/",
  "token_type": "Bearer"
}
</code></pre>
<h5 id="get-a-token-by-using-c">Get a token by using C#</h5>
<p>The code sample below builds the request to acquire a token, calls the endpoint, and then extracts the token from the response.</p>
<pre><code>using System;
using System.Collections.Generic;
using System.IO;
using System.Net;
using System.Web.Script.Serialization; 

// Build request to acquire managed identities for Azure resources token
HttpWebRequest request = (HttpWebRequest)WebRequest.Create("http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&amp;resource=https://management.azure.com/");
request.Headers["Metadata"] = "true";
request.Method = "GET";

try
{
    // Call /token endpoint
    HttpWebResponse response = (HttpWebResponse)request.GetResponse();

    // Pipe response Stream to a StreamReader, and extract access token
    StreamReader streamResponse = new StreamReader(response.GetResponseStream()); 
    string stringResponse = streamResponse.ReadToEnd();
    JavaScriptSerializer j = new JavaScriptSerializer();
    Dictionary&lt;string, string&gt; list = (Dictionary&lt;string, string&gt;) j.Deserialize(stringResponse, typeof(Dictionary&lt;string, string&gt;));
    string accessToken = list["access_token"];
}
catch (Exception e)
{
    string errorText = String.Format("{0} \n\n{1}", e.Message, e.InnerException != null ? e.InnerException.Message : "Acquire token failed");
}
</code></pre>
<h4 id="token-caching">Token caching</h4>
<p>While the managed identities for Azure resources subsystem does cache tokens, we also recommend to implement token caching in your code. As a result, you should prepare for scenarios where the resource indicates that the token is expired.</p>
<p>On-the-wire calls to Azure Active Directory result only when:</p>
<ul>
<li>Cache miss occurs due to no token in the managed identities for Azure resources subsystem cache.</li>
<li>The cached token is expired.</li>
</ul>
<h4 id="retry-guidance">Retry guidance</h4>
<p>It is recommended to retry if you receive a 404, 429, or 5xx error code. Throttling limits apply to the number of calls made to the IMDS endpoint. When the throttling threshold is exceeded, IMDS endpoint limits any further requests while the throttle is in effect. During this period, the IMDS endpoint will return the HTTP status code 429 (“Too many requests”), and the requests fail.</p>
<h2 id="implement-azure-app-configuration">Implement Azure App Configuration</h2>
<h3 id="explore-the-azure-app-configuration-service">Explore the Azure App Configuration service</h3>
<p>Azure App Configuration provides a service to centrally manage application settings and feature flags.</p>
<p>Modern programs, especially programs running in a cloud, generally have many components that are distributed in nature. Spreading configuration settings across these components can lead to hard-to-troubleshoot errors during an application deployment. Use App Configuration to store all the settings for your application and secure their access in one place.</p>
<p>App Configuration offers the following benefits:</p>
<ul>
<li>A fully managed service that can be set up in minutes</li>
<li>Flexible key representations and mappings</li>
<li>Tagging with labels</li>
<li>Point-in-time replay of settings</li>
<li>Dedicated UI for feature flag management</li>
<li>Comparison of two sets of configurations on custom-defined dimensions</li>
<li>Enhanced security through Azure-managed identities</li>
<li>Complete data encryptions, at rest or in transit</li>
<li>Native integration with popular frameworks</li>
</ul>
<p>App Configuration complements Azure Key Vault, which is used to store application secrets. App Configuration makes it easier to implement the following scenarios:</p>
<ul>
<li>Centralize management and distribution of hierarchical configuration data for different environments and geographies</li>
<li>Dynamically change application settings without the need to redeploy or restart an application</li>
<li>Control feature availability in real-time</li>
</ul>
<h4 id="use-app-configuration">Use App Configuration</h4>
<p>The easiest way to add an App Configuration store to your application is through a client library that Microsoft provides.</p>
<ul>
<li>App Configuration provider for .NET Core</li>
<li>App Configuration builder for .NET</li>
<li>App Configuration client for Spring Cloud</li>
<li>App Configuration REST API</li>
</ul>
<h3 id="create-paired-keys-and-values">Create paired keys and values</h3>
<h4 id="keys">Keys</h4>
<p>Keys serve as the name for key-value pairs and are used to store and retrieve corresponding values. It’s a common practice to organize keys into a hierarchical namespace by using a character delimiter, such as <code>/</code> or <code>:</code>.</p>
<p>Keys stored in App Configuration are <strong>case-sensitive</strong>, unicode-based strings.</p>
<p>You can use any unicode character in key names entered into App Configuration except for <code>*</code>, <code>,</code>, and <code>\</code>. These characters are reserved. There’s a combined size limit of 10,000 characters on a key-value pair.</p>
<h4 id="design-key-namespaces">Design key namespaces</h4>
<p>There are two general approaches to naming keys used for configuration data: flat or hierarchical. These methods are similar from an application usage standpoint, but hierarchical naming offers a number of advantages:</p>
<ul>
<li>Easier to read. Instead of one long sequence of characters, delimiters in a hierarchical key name function as spaces in a sentence.</li>
<li>Easier to manage. A key name hierarchy represents logical groups of configuration data.</li>
<li>Easier to use. It’s simpler to write a query that pattern-matches keys in a hierarchical structure and retrieves only a portion of configuration data.</li>
</ul>
<p>Below are some examples of how you can structure your key names into a hierarchy:</p>
<ul>
<li>
<p>Based on component services</p>
<pre><code>AppName:Service1:ApiEndpoint
AppName:Service2:ApiEndpoint
</code></pre>
</li>
<li>
<p>Based on deployment regions</p>
<pre><code>AppName:Region1:DbEndpoint
AppName:Region2:DbEndpoint
</code></pre>
</li>
</ul>
<h5 id="label-keys">Label keys</h5>
<p>Label provides a convenient way to create variants of a key. A common use of labels is to specify multiple environments for the same key:</p>
<pre><code>Key = AppName:DbEndpoint &amp; Label = Test
Key = AppName:DbEndpoint &amp; Label = Staging
Key = AppName:DbEndpoint &amp; Label = Production
</code></pre>
<h5 id="version-key-values">Version key values</h5>
<p>App Configuration doesn’t version key values automatically as they’re modified. Use labels as a way to create multiple versions of a key value. For example, you can input an application version number or a Git commit ID in labels to identify key values associated with a particular software build.</p>
<h5 id="query-key-values">Query key values</h5>
<p>Each key value is uniquely identified by its key plus a label that can be <code>null</code>. You query an App Configuration store for key values by specifying a pattern. The App Configuration store returns all key values that match the pattern and their corresponding values and attributes.</p>
<h4 id="values">Values</h4>
<p>Values assigned to keys are also unicode strings. You can use all unicode characters for values. There’s an optional user-defined content type associated with each value. Use this attribute to store information, for example an encoding scheme, about a value that helps your application to process it properly.</p>
<p>Configuration data stored in an App Configuration store, which includes all keys and values, is encrypted at rest and in transit. App Configuration isn’t a replacement solution for Azure Key Vault. Don’t store application secrets in it.</p>
<h3 id="manage-application-features">Manage application features</h3>
<h4 id="basic-concepts-1">Basic concepts</h4>
<p>Feature management is a modern software-development practice that decouples feature release from code deployment and enables quick changes to feature availability on demand. It uses a technique called feature flags (also known as feature toggles, feature switches, and so on) to dynamically administer a feature’s lifecycle.</p>
<ul>
<li><strong>Feature flag</strong>: A feature flag is a variable with a binary state of <em>on</em> or <em>off</em>. The feature flag also has an associated code block.</li>
<li><strong>Feature manager</strong>: A feature manager is an application package that handles the lifecycle of all the feature flags in an application.</li>
<li><strong>Filter</strong>: A filter is a rule for evaluating the state of a feature flag. A user group, a device or browser type, a geographic location, and a time window are all examples of what a filter can represent.</li>
</ul>
<p>An effective implementation of feature management consists of at least two components working in concert:</p>
<ul>
<li>An application that makes use of feature flags.</li>
<li>A separate repository that stores the feature flags and their current states.</li>
</ul>
<h4 id="feature-flag-usage-in-code-1">Feature flag usage in code</h4>
<pre><code>if (featureFlag) {
    // Run the following code
}
</code></pre>
<p>You can set the value of <code>featureFlag</code> statically, as in the following code example:</p>
<pre><code>bool featureFlag = true;
</code></pre>
<p>You can also evaluate the flag’s state based on certain rules:</p>
<pre><code>bool featureFlag = isBetaUser();
</code></pre>
<p>A slightly more complicated feature flag pattern includes an <code>else</code> statement as well:</p>
<pre><code>if (featureFlag) {
    // This following code will run if the featureFlag value is true
} else {
    // This following code will run if the featureFlag value is false
}
</code></pre>
<h4 id="feature-flag-declaration-1">Feature flag declaration</h4>
<p>Each feature flag has two parts: a name and a list of one or more filters that are used to evaluate if a feature’s state is <em>on</em>. A filter defines a use case for when a feature should be turned on.</p>
<p>When a feature flag has multiple filters, the filter list is traversed in order until one of the filters determines the feature should be enabled. At that point, the feature flag is <em>on</em>, and any remaining filter results are skipped. If no filter indicates the feature should be enabled, the feature flag is <em>off</em>.</p>
<p>The feature manager supports <em>appsettings.json</em> as a configuration source for feature flags. The following example shows how to set up feature flags in a JSON file:</p>
<pre><code>"FeatureManagement": {
    "FeatureA": true, // Feature flag set to on
    "FeatureB": false, // Feature flag set to off
    "FeatureC": {
        "EnabledFor": [
            {
                "Name": "Percentage",
                "Parameters": {
                    "Value": 50
                }
            }
        ]
    }
}
</code></pre>
<h4 id="feature-flag-repository-1">Feature flag repository</h4>
<p>To use feature flags effectively, you need to externalize all the feature flags used in an application. This approach allows you to change feature flag states without modifying and redeploying the application itself.</p>
<p>Azure App Configuration is designed to be a centralized repository for feature flags. You can use it to define different kinds of feature flags and manipulate their states quickly and confidently. You can then use the App Configuration libraries for various programming language frameworks to easily access these feature flags from your application.</p>
<h3 id="secure-app-configuration-data">Secure app configuration data</h3>
<h4 id="encrypt-configuration-data-by-using-customer-managed-keys">Encrypt configuration data by using customer-managed keys</h4>
<p>Azure App Configuration encrypts sensitive information at rest using a 256-bit AES encryption key provided by Microsoft. Every App Configuration instance has its own encryption key managed by the service and used to encrypt sensitive information. Sensitive information includes the values found in key-value pairs. When customer-managed key capability is enabled, App Configuration uses a managed identity assigned to the App Configuration instance to authenticate with Azure Active Directory. The managed identity then calls Azure Key Vault and wraps the App Configuration instance’s encryption key. The wrapped encryption key is then stored and the unwrapped encryption key is cached within App Configuration for one hour. App Configuration refreshes the unwrapped version of the App Configuration instance’s encryption key hourly. This ensures availability under normal operating conditions.</p>
<h5 id="enable-customer-managed-key-capability">Enable customer-managed key capability</h5>
<p>The following components are required to successfully enable the customer-managed key capability for Azure App Configuration:</p>
<ul>
<li>Standard tier Azure App Configuration instance</li>
<li>Azure Key Vault with soft-delete and purge-protection features enabled</li>
<li>An RSA or RSA-HSM key within the Key Vault: The key must not be expired, it must be enabled, and it must have both wrap and unwrap capabilities enabled</li>
</ul>
<p>Once these resources are configured, two steps remain to allow Azure App Configuration to use the Key Vault key:</p>
<ol>
<li>Assign a managed identity to the Azure App Configuration instance</li>
<li>Grant the identity <code>GET</code>, <code>WRAP</code>, and <code>UNWRAP</code> permissions in the target Key Vault’s access policy.</li>
</ol>
<h4 id="use-private-endpoints-for-azure-app-configuration">Use private endpoints for Azure App Configuration</h4>
<p>You can use private endpoints for Azure App Configuration to allow clients on a virtual network (VNet) to securely access data over a private link. Network traffic between the clients on the VNet and the App Configuration store traverses over the VNet using a private link on the Microsoft backbone network, eliminating exposure to the public internet.</p>
<p>Using private endpoints for your App Configuration store enables you to:</p>
<ul>
<li>Secure your application configuration details by configuring the firewall to block all connections to App Configuration on the public endpoint.</li>
<li>Increase security for the virtual network (VNet) ensuring data doesn’t escape from the VNet.</li>
<li>Securely connect to the App Configuration store from on-premises networks that connect to the VNet using VPN or ExpressRoutes with private-peering.</li>
</ul>
<h5 id="private-endpoints-for-app-configuration">Private endpoints for App Configuration</h5>
<p>When creating a private endpoint, you must specify the App Configuration store to which it connects. If you have multiple App Configuration stores, you need a separate private endpoint for each store.</p>
<h5 id="dns-changes-for-private-endpoints">DNS changes for private endpoints</h5>
<p>When you create a private endpoint, the DNS CNAME resource record for the configuration store is updated to an alias in a subdomain with the prefix <code>privatelink</code>. Azure also creates a private DNS zone corresponding to the <code>privatelink</code> subdomain, with the DNS A resource records for the private endpoints.</p>
<p>When you resolve the endpoint URL from within the VNet hosting the private endpoint, it resolves to the private endpoint of the store. When resolved from outside the VNet, the endpoint URL resolves to the public endpoint. When you create a private endpoint, the public endpoint is disabled.</p>
<h4 id="managed-identities">Managed identities</h4>
<p>A managed identity from Azure Active Directory (AAD) allows Azure App Configuration to easily access other AAD-protected resources, such as Azure Key Vault.</p>
<p>Your application can be granted two types of identities:</p>
<ul>
<li>A <strong>system-assigned identity</strong> is tied to your configuration store. It’s deleted if your configuration store is deleted. A configuration store can only have one system-assigned identity.</li>
<li>A <strong>user-assigned identity</strong> is a standalone Azure resource that can be assigned to your configuration store. A configuration store can have multiple user-assigned identities.</li>
</ul>
<h5 id="add-a-system-assigned-identity">Add a system-assigned identity</h5>
<p>To set up a managed identity using the Azure CLI, use the <code>az appconfig identity assign</code> command against an existing configuration store.</p>
<pre><code>az appconfig identity assign \ 
    --name myTestAppConfigStore \ 
    --resource-group myResourceGroup
</code></pre>
<h5 id="add-a-user-assigned-identity">Add a user-assigned identity</h5>
<p>Creating an App Configuration store with a user-assigned identity requires that you create the identity and then assign its resource identifier to your store.</p>
<pre><code>az identity create --resource-group myResourceGroup --name myUserAssignedIdentity
</code></pre>
<pre><code>az appconfig identity assign --name myTestAppConfigStore \ 
    --resource-group myResourceGroup \ 
    --identities /subscriptions/[subscription id]/resourcegroups/myResourceGroup/providers/Microsoft.ManagedIdentity/userAssignedIdentities/myUserAssignedIdentity
</code></pre>
<h1 id="az-204-implement-api-management">AZ-204 Implement API Management</h1>
<h2 id="explore-api-management">Explore API Management</h2>
<h3 id="discover-the-api-management-service">Discover the API Management service</h3>
<p>API Management provides the core functionality to ensure a successful API program through developer engagement, business insights, analytics, security, and protection. Each API consists of one or more operations, and each API can be added to one or more products. To use an API, developers subscribe to a product that contains that API, and then they can call the API’s operation, subject to any usage policies that may be in effect.</p>
<p>The system is made up of the following components:</p>
<ul>
<li>The <strong>API gateway</strong> is the endpoint that:
<ul>
<li>Accepts API calls and routes them to your backend(s).</li>
<li>Verifies API keys, JWT tokens, certificates, and other credentials.</li>
<li>Enforces usage quotas and rate limits.</li>
<li>Transforms your API on the fly without code modifications.</li>
<li>Caches backend responses where set up.</li>
<li>Logs call metadata for analytics purposes.</li>
</ul>
</li>
<li>The <strong>Azure portal</strong> is the administrative interface where you set up your API program. Use it to:
<ul>
<li>Define or import API schema.</li>
<li>Package APIs into products.</li>
<li>Set up policies like quotas or transformations on the APIs.</li>
<li>Get insights from analytics.</li>
<li>Manage users.</li>
</ul>
</li>
<li>The <strong>Developer portal</strong> serves as the main web presence for developers, where they can:
<ul>
<li>Read API documentation.</li>
<li>Try out an API via the interactive console.</li>
<li>Create an account and subscribe to get API keys.</li>
<li>Access analytics on their own usage.</li>
</ul>
</li>
</ul>
<h4 id="products">Products</h4>
<p>Products are how APIs are surfaced to developers. Products in API Management have one or more APIs, and are configured with a title, description, and terms of use. Products can be <strong>Open</strong> or <strong>Protected</strong>. Protected products must be subscribed to before they can be used, while open products can be used without a subscription.</p>
<h4 id="groups">Groups</h4>
<ul>
<li><strong>Administrators</strong> - Azure subscription administrators are members of this group. Administrators manage API Management service instances, creating the APIs, operations, and products that are used by developers.</li>
<li><strong>Developers</strong> - Authenticated developer portal users fall into this group. Developers are the customers that build applications using your APIs. Developers are granted access to the developer portal and build applications that call the operations of an API.</li>
<li><strong>Guests</strong> - Unauthenticated developer portal users, such as prospective customers visiting the developer portal of an API Management instance fall into this group. They can be granted certain read-only access, such as the ability to view APIs but not call them.</li>
</ul>
<p>In addition to these system groups, administrators can create custom groups.</p>
<h4 id="developers">Developers</h4>
<p>Developers represent the user accounts in an API Management service instance.</p>
<h4 id="policies">Policies</h4>
<p>Policies are a powerful capability of API Management that allow the Azure portal to change the behavior of the API through configuration. Policies are a collection of statements that are executed sequentially on the request or response of an API (e.g. JSON to XML conversion).</p>
<h4 id="developer-portal">Developer portal</h4>
<p>The developer portal is where developers can learn about your APIs, view and call operations, and subscribe to products.</p>
<h4 id="explore-api-gateways">Explore API gateways</h4>
<p>Your solution may contain several front- and back-end services. In this scenario, how does a client know what endpoints to call? What happens when new services are introduced, or existing services are refactored? How do services handle SSL termination, authentication, and other concerns? An <em>API gateway</em> can help to address these challenges.</p>
<p><img src="https://docs.microsoft.com/en-us/training/wwl-azure/explore-api-management/media/api-gateway.png" alt="Diagram of an API gateway."></p>
<p>An API gateway sits between clients and services. It acts as a reverse proxy, routing requests from clients to services. It may also perform various cross-cutting tasks such as authentication, SSL termination, and rate limiting. If you don’t deploy a gateway, clients must send requests directly to front-end services. However, there are some potential problems with exposing services directly to clients:</p>
<ul>
<li>It can result in complex client code. The client must keep track of multiple endpoints, and handle failures in a resilient way.</li>
<li>It creates coupling between the client and the backend.</li>
<li>A single operation might require calls to multiple services. That can result in multiple network round trips between the client and the server, adding significant latency.</li>
<li>Each public-facing service must handle concerns such as authentication, SSL, and client rate limiting.</li>
<li>Services must expose a client-friendly protocol such as HTTP or WebSocket. This limits the choice of communication protocols.</li>
<li>Services with public endpoints are a potential attack surface, and must be hardened.</li>
</ul>
<p>A gateway helps to address these issues by decoupling clients from services.</p>
<ul>
<li>
<p><strong>Gateway routing</strong>: Use the gateway as a reverse proxy to route requests to one or more backend services.</p>
</li>
<li>
<p><strong>Gateway aggregation</strong>: Use the gateway to aggregate multiple individual requests into a single request.</p>
</li>
<li>
<p><strong>Gateway Offloading</strong>: Use the gateway to offload functionality from individual services to the gateway, particularly cross-cutting concerns.</p>
</li>
</ul>
<p>Here are some examples of functionality that could be offloaded to a gateway:</p>
<ul>
<li>SSL termination</li>
<li>Authentication</li>
<li>Client rate limiting (throttling)</li>
<li>Logging and monitoring</li>
<li>Response caching</li>
</ul>
<p><strong>hier weiter</strong></p>
<h3 id="explore-api-management-policies">Explore API Management policies</h3>
<p>In Azure API Management, policies are a powerful capability of the system that allow the publisher to change the behavior of the API through configuration. Policies are a collection of Statements that are executed sequentially on the request or response of an API.</p>
<p>Policies are applied inside the gateway which sits between the API consumer and the managed API. The gateway receives all requests and usually forwards them unaltered to the underlying API. However a policy can apply changes to both the inbound request and outbound response. Policy expressions can be used as attribute values or text values in any of the API Management policies, unless the policy specifies otherwise.</p>
<h2 id="understanding-policy-configuration">Understanding policy configuration</h2>
<p>The policy definition is a simple XML document that describes a sequence of inbound and outbound statements. The XML can be edited directly in the definition window.</p>
<p>The configuration is divided into <code>inbound</code>, <code>backend</code>, <code>outbound</code>, and <code>on-error</code>. The series of specified policy statements is executed in order for a request and a response.</p>
<pre><code>&lt;policies&gt;
  &lt;inbound&gt;
    &lt;!-- statements to be applied to the request go here --&gt;
  &lt;/inbound&gt;
  &lt;backend&gt;
    &lt;!-- statements to be applied before the request is forwarded to 
         the backend service go here --&gt;
  &lt;/backend&gt;
  &lt;outbound&gt;
    &lt;!-- statements to be applied to the response go here --&gt;
  &lt;/outbound&gt;
  &lt;on-error&gt;
    &lt;!-- statements to be applied if there is an error condition go here --&gt;
  &lt;/on-error&gt;
&lt;/policies&gt;
</code></pre>
<p>If there is an error during the processing of a request, any remaining steps in the <code>inbound</code>, <code>backend</code>, or <code>outbound</code> sections are skipped and execution jumps to the statements in the <code>on-error</code> section. By placing policy statements in the <code>on-error</code> section you can review the error by using the <code>context.LastError</code> property, inspect and customize the error response using the <code>set-body</code> policy, and configure what happens if an error occurs.</p>
<h2 id="examples">Examples</h2>
<h3 id="apply-policies-specified-at-different-scopes">Apply policies specified at different scopes</h3>
<p>If you have a policy at the global level and a policy configured for an API, then whenever that particular API is used both policies will be applied. API Management allows for deterministic ordering of combined policy statements via the base element.</p>
<pre><code>&lt;policies&gt;
    &lt;inbound&gt;
        &lt;cross-domain /&gt;
        &lt;base /&gt;
        &lt;find-and-replace from="xyz" to="abc" /&gt;
    &lt;/inbound&gt;
&lt;/policies&gt;
</code></pre>
<p>In the example policy definition above, the <code>cross-domain</code> statement would execute before any higher policies which would in turn, be followed by the <code>find-and-replace</code> policy.</p>
<h3 id="filter-response-content">Filter response content</h3>
<p>The policy defined in example below demonstrates how to filter data elements from the response payload based on the product associated with the request.</p>
<p>The snippet assumes that response content is formatted as JSON and contains root-level properties named “minutely”, “hourly”, “daily”, “flags”.</p>
<pre><code>&lt;policies&gt;
  &lt;inbound&gt;
    &lt;base /&gt;
  &lt;/inbound&gt;
  &lt;backend&gt;
    &lt;base /&gt;
  &lt;/backend&gt;
  &lt;outbound&gt;
    &lt;base /&gt;
    &lt;choose&gt;
      &lt;when condition="@(context.Response.StatusCode == 200 &amp;&amp; context.Product.Name.Equals("Starter"))"&gt;
        &lt;!-- NOTE that we are not using preserveContent=true when deserializing response body stream into a JSON object since we don't intend to access it again. See details on https://docs.microsoft.com/azure/api-management/api-management-transformation-policies#SetBody --&gt;
        &lt;set-body&gt;
          @{
            var response = context.Response.Body.As&lt;JObject&gt;();
            foreach (var key in new [] {"minutely", "hourly", "daily", "flags"}) {
            response.Property (key).Remove ();
           }
          return response.ToString();
          }
    &lt;/set-body&gt;
      &lt;/when&gt;
    &lt;/choose&gt;    
  &lt;/outbound&gt;
  &lt;on-error&gt;
    &lt;base /&gt;
  &lt;/on-error&gt;
&lt;/policies&gt;
</code></pre>
<hr>
<h2 id="next-unit-create-advanced-policies">Next unit: Create advanced policies</h2>
<p><a href="https://docs.microsoft.com/en-us/training/modules/explore-api-management/5-create-advanced-policies/">Continue</a></p>
<p>Need help? See our <a href="https://docs.microsoft.com/en-us/training/support/troubleshooting?uid=learn.wwl.explore-api-management.policies&amp;documentId=a9bc8f57-963a-76fa-8de7-70fbb120c07e&amp;versionIndependentDocumentId=332f5e7c-01ed-adf0-fe28-2a8e0bd501b8&amp;contentPath=%2FMicrosoftDocs%2Flearn-pr%2Fblob%2Flive%2Flearn-pr%2Fwwl-azure%2Fexplore-api-management%2F4-api-management-policies.yml&amp;url=https%3A%2F%2Fdocs.microsoft.com%2Fen-us%2Ftraining%2Fmodules%2Fexplore-api-management%2F4-api-management-policies&amp;author=jeffko">troubleshooting guide</a> or provide specific feedback by <a href="https://docs.microsoft.com/en-us/training/support/troubleshooting?uid=learn.wwl.explore-api-management.policies&amp;documentId=a9bc8f57-963a-76fa-8de7-70fbb120c07e&amp;versionIndependentDocumentId=332f5e7c-01ed-adf0-fe28-2a8e0bd501b8&amp;contentPath=%2FMicrosoftDocs%2Flearn-pr%2Fblob%2Flive%2Flearn-pr%2Fwwl-azure%2Fexplore-api-management%2F4-api-management-policies.yml&amp;url=https%3A%2F%2Fdocs.microsoft.com%2Fen-us%2Ftraining%2Fmodules%2Fexplore-api-management%2F4-api-management-policies&amp;author=jeffko#report-feedback">reporting an issue</a>.</p>
<h2 id="create-advanced-policies---training--microsoft-docs.md">Create advanced policies - Training  Microsoft <a href="http://Docs.md">Docs.md</a></h2>
<ul>
<li>3 minutes</li>
</ul>
<p>This unit provides a reference for the following API Management policies:</p>
<ul>
<li>Control flow - Conditionally applies policy statements based on the results of the evaluation of Boolean expressions.</li>
<li>Forward request - Forwards the request to the backend service.</li>
<li>Limit concurrency - Prevents enclosed policies from executing by more than the specified number of requests at a time.</li>
<li>Log to Event Hub - Sends messages in the specified format to an Event Hub defined by a Logger entity.</li>
<li>Mock response - Aborts pipeline execution and returns a mocked response directly to the caller.</li>
<li>Retry - Retries execution of the enclosed policy state</li>
</ul>

    </div>
  </div>
</body>

</html>
